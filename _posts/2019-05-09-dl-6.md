---
layout: post
title: 【Method】深度学习（六）架构设计
categories: Analytics
---

## 原理

神经网络设计的另一个关键点是确定它的架构。架构（architecture）一词是指网络的整体结构：它应该具有多少单元，以及这些单元应该如何连接。

大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数。在这种结构中，第一层由下式给出：

$$h^{(1)} = g^{(1)} (W^{(1)T} x + b^{(1)})$$

第二层由

$$h^{(2)} = g^{(2)} (W^{(2)T} h^{(1)} + b^{(2)})$$

给出，以此类推。

在这些链式架构中，主要的架构考虑是选准网络的深度和每一层的宽度。我们将会看到，即使只有一个隐藏层的网络也足够适应训练集。更深层的网络通常能够对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但是通常也更难以优化。对于一个具体的任务，理性的网络架构必须通过实验，观测在验证集上的误差来找到。

### 万能近似性质和深度

线性模型，通过矩阵乘法将特征映射到输出，顾名思义。仅能表示线性函数。它具有易于训练的优点，因为当使用线性模型时，许多损失函数会导出凸优化问题。可惜的是，我们经常希望我们的系统学习非线性函数。

乍一看，我们可能认为学习非线性函数需要为我们想要学习的那种非线性专门设计一类模拟族。幸运的是，具有隐藏层的前馈网络提供了一种万能近似框架。具体来说，万能近似定理（universal approximation theorem）（Hornik et al., 1989； Cybenko, 1989）表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。前馈网络的导数也可以任意好地来近似函数的导数（Hornik et al., 1990）。Borel可测的概念超出了讨论的范畴；对于我们想要实现的目标，只需要知道定义在$$R^n$$的有界闭集上的任意连续函数是Borel可测的，因此可以用神经网络来近似。神经网络也可以近似从任何有限维离散空间映射到另一个的任意函数。虽然原始定理最初以具有特殊激活函数的单元的形式来描述，这个激活函数当变量取绝对值非常大的正值和负值时都会饱和，万能近似定理也已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的整流线性单元（Leshno et al., 1993）。

万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的MLP一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。即使MLP能够表示该函数，学习也可能因两个不同的原因而失败。首先，用于训练的优化算法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错误的函数。回忆“没有免费的午餐”定理，说明了没有普遍优越的机器学习算法。前馈网络提供了表述函数的万能系统，在这种意义上，给定一个函数，存在一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。

万能近似定理说明了，存在一个足够大的网络能够达到我们所希望的任意精度，但是定理并没有说这个网络有多大。Barron（1993）提供了单层网络近似一大类函数所需大小的一些界。不幸的是，在最坏的情况下，可能需要指数数量的隐藏单元（可能一个隐藏单元对应着一个需要区分的输入配置）。这在二进制值的情况下很容易看到：向量$$v \in {0,1}^n$$上的可能的二值型函数的数量是$$2^{2^n}$$，并且选择一个这样的函数需要$$2^n$$位，这通常需要$$O(2^n)$$的自由度。

总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。很多情况下，使用更深的模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误差。

