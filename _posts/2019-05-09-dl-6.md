---
layout: post
title: 【Method】深度学习（六）架构设计
categories: Analytics
---

## 原理

神经网络设计的另一个关键点是确定它的架构。架构（architecture）一词是指网络的整体结构：它应该具有多少单元，以及这些单元应该如何连接。

大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数。在这种结构中，第一层由下式给出：

$$h^{(1)} = g^{(1)} (W^{(1)T} x + b^{(1)})$$

第二层由

$$h^{(2)} = g^{(2)} (W^{(2)T} h^{(1)} + b^{(2)})$$

给出，以此类推。

在这些链式架构中，主要的架构考虑是选准网络的深度和每一层的宽度。我们将会看到，即使只有一个隐藏层的网络也足够适应训练集。更深层的网络通常能够对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但是通常也更难以优化。对于一个具体的任务，理性的网络架构必须通过实验，观测在验证集上的误差来找到。

### 万能近似性质和深度

线性模型，通过矩阵乘法将特征映射到输出，顾名思义。仅能表示线性函数。它具有易于训练的优点，因为当使用线性模型时，许多损失函数会导出凸优化问题。可惜的是，我们经常希望我们的系统学习非线性函数。

乍一看，我们可能认为学习非线性函数需要为我们想要学习的那种非线性专门设计一类模拟族。幸运的是，具有隐藏层的前馈网络提供了一种万能近似框架。具体来说，万能近似定理（universal approximation theorem）（Hornik et al., 1989； Cybenko, 1989）表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。