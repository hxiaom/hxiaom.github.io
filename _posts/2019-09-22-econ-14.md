---
layout: post
title: 【Method】时间序列回归（一）导论
categories: Analytics
---

时间序列数据（同一个体在多个时点的观测数据）能被用于回答那些横截面数据所不能回答的问题。其中，一类问题是，一个变量$$X_i$$的变化对另一个变量$$Y_i$$的因果效应将如何随着时间变化？换言之，X的变化对Y的动态因果效应是什么？例如，一项要求乘客使用安全带的法律在生效初期及当驾驶员适应了这项法律之后，对交通事故死亡率的因果效应是什么？另一类问题是你对未来某个变量的最佳预测是什么？例如，你对下个月通货膨胀率、利率或股票价格的最佳预测是什么？这两类问题（第一类问题有关动态因果效应，第二类问题有关经济预测）都能通过使用时间序列数据来解答。但是，时间序列数据也给我们带来了挑战，为了克服这些挑战，我们需要一些新的技术。

“未来将会与过去很相似”，这是时间序列回归中非常重要的假设，我们称之为“平稳性”。时间序列变量在某些情况下可能并不平稳，其中与经济时间序列分析特别相关的两种情况：第一，序列存在长期趋势；第二，随着时间的推移，总体回归可能并不稳定，即总体回归中可能存在突变。这些对平稳性假设的偏离会严重危害到基于时间序列回归的预测和推断。然而幸运的是，我们有检验是否存在趋势和突变的统计方法，一旦发现有趋势或突变的现象，我们就能及时调整模型的设定形式。

## 利用回归模型进行预测

即使一个模型中的回归系数不能被解读为因果效应，但这个模型可能对预测有帮助。从预测角度看，重要的问题是这个模型能否提供一个尽可能精确的预测。尽管完美的预测并不存在，回归模型依然能给出精确和可靠的预测结果。

如果能获得过去的测试成绩，则一个好的起点是利用现在和过去的测试成绩来预测将来的测试成绩。

## 时间序列数据和序列相关介绍

### 滞后、一阶差分、对数值及增长率

时间序列变量Y在时点t的观测值用$$Y_t$$表示，总观测数用T表示。观测之间的间隔，即时点t和时点t+1之间的时间间隔是一个单位（例如，一周、一个月、一个季度、或一年）。

我们用特殊的术语和记号来表示Y的未来值和过去值。Y的前一期数值被称作Y的一阶滞后值，或更简洁地称为一阶滞后，记作$$Y_{t-1}$$。它的j阶滞后值是j个时期前的数值，记作$$Y_{t-j}$$。相似地，$$Y_{t+1}$$表示Y未来一期的数值。

Y在时期t-1和时期t之间数值变化量是$$Y_t-Y_{t-1}$$，这个变化量是变量$$Y_t$$的一阶差分。在时间序列数据中，“$$\Delta$$”用来表示一阶差分，故$$\Delta Y_t = Y_t - Y_{t-1}$$。

经济时间序列数据经常在取数值或取对数值的差分之后进行分析，这样做的一个原因是许多经济时间序列呈现近似指数增长的态势，即平均而言，长期中的序列增长率每年保持在一个固定的百分比。这一点意味着，序列的对数值近似线性增长。这样做的另一个原因是，许多经济时间序列变量的标准差大约与它的水平值成正比。这也意味着，序列对数值的标准差近似于一个常数。无论是哪一种情况，将时间序列数据转化成对数形式都是有用的，因为对数值的变化相当于原始序列的百分比变化。

$$Y_t$$的一阶滞后记作$$Y_{t-1}$$；j阶滞后记作$$Y_{t-j}$$。

一个序列的一阶差分，$$\Delta Y_t$$，是这个序列从t-1期到t期的变化量，即$$\Delta Y_t = Y_t - Y_{t-1}$$。

$$Y_t$$对数的一阶差分是$$\Delta ln(Y_t) = ln(Y_t) - ln(Y_{t-1})$$

一个时间序列$$Y_t$$从t-1期到t期的百分比变化大约为$$100 \Delta ln(Y_t)$$。当百分比变化很小时，这种近似是很精确的。

### 自相关

在时间序列数据中，某一时期的Y值总是和前一期值相关。时间序列变化和自身滞后值的相关性被称为自相关（autocorrelation）或序列相关（serial correlation）。一阶自相关（或自相关系数）是$$Y_t$$和$$Y_{t-1}$$的相关系数，即两个相邻时间点数值的相关系数。二阶自相关是$$Y_t$$和$$Y_{t-2}$$的相关系数，j阶自相关是$$Y_t$$和$$Y_{t-j}$$的相关系数。类似地，j阶自协方差（$$j^{th}$$ atcto coveriance）是$$Y_t$$与$$Y_{t-j}$$的协方差。

序列$$Y_t$$的j阶自协方差是$$Y_t$$和它的j阶滞后值$$Y_{t-j}$$的协方差，j阶自相关系数是$$Y_t$$和$$Y_{t-j}$$的相关系数。即

$$j阶自协方差 = Cov(Y_t, Y_{t-j})$$

$$j阶自相关 = \rau_j = corr(Y_t, Y_{t-j}) = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$$

j阶自相关系数有时也被称为j阶序列相关系数

j阶总体自协方差及自相关系数能够用j阶样本自协方差和自相关系数，即，$$\hat{Cov(Y_t, Y_{t-j})}$$和$$\hat{\rau}_j$$，来估计：

$$\hat{Cov(Y_t, Y_{t-j})} = \frac{1}{T} \sum_{t=j+1}^T (Y_t - \bar{Y}_{j+1:T})(Y_{t-j}- \bar{Y}_{1:T-j})$$

$$\hat{\rau}_j = \frac{\hat{Cov(Y_t, Y_{t-j})}}{\hat{Var(Y_t)}}$$

其中，$$\bar{Y}_{j+1:T}$$表示$$Y_t$$在观测期$$t=j+1,..., T$$的样本均值，$$\hat{Var(Y_t)}$$为Y的样本方差。

## 自回归

自回归模型（autoregression）是一种将时间序列变量与自身的滞后值相联系的模型。

### 一阶自回归模型

如果你想要预测一个时间序列的未来值，则从上一期的滞后值考虑将会是一个不错的开始。

一阶自回归模型可缩写为AR(1)，其中1表示是一阶模型。序列$$Y_t$$的AR(1)总体模型是：

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t$$

式中，$$u_t$$为误差项。

预测和预测误差。假设你有Y的历史数据，你想预测它的未来值。如果$$Y_t$$服从上式所描述的AR(1)模型，$$\beta_0$$和$$\beta_1$$已知，则基于$$Y_T$$对$$Y_{T+1}$$的预测值为$$\beta_0 +\beta_1 Y_T$$。

在实践中，$$\beta_0$$和$$\beta_1$$未知，从而预测必须以$$\beta_0$$和$$\beta_1$$的估计值为基础。我们将使用$$\beta_0$$和$$\beta_1$$的OLS估计值，而它们的估计值可通过使用历史数据得到。总的来说，我们使用$$\hat{Y}_{T+1 \mid T}$$表示基于截止到时间T的数据对T+1期数据的预测。相应地，使用上式所示的AR(1)模型做出的预测为

$$\hat{Y}_{T+1 \mid T} = \hat{\beta_0} + \hat{\beta_1} Y_T$$

式中，$$\hat{\beta}_0$$和$$\hat{\beta}_1$$表示使用截止到T期的历史数据得到的估计值。

预测误差（forecast error）是预测的错误程度，即实际发生的$$Y_{T+1}$$值与预测值$$\hat{Y}_{T+1 \mid T}$$的差：

$$预测误差 = Y_{T+1} - \hat{Y}_{T+1 \mid T}$$

预测值与预测误差。预测值不是OLS预期值，预测误差不是OLS残差。OLS预期值是利用估计模型所用到的样本内观测值计算出来的。相反地，预测值是针对样本外的某个时间段而言的，因此预测的被解释变量对应的真实值并不在估计模型所用到的样本中。同理，OLS残差为Y的实际值与它的预期值之差，而预测误差为Y的未来值（这个值不在估计模型所用到的样本中）与其预测值之差。换言之，预测值和预测误差属于“样本外”的观测，而预期值和残差属于“样本内”的观测。

根均方预测误差。根均方预测误差（root mean squared forecast error，RMSFE）是预测误差大小的测度方式，即使用预测模型做出预测的典型错误大小。RMSFE是预测误差平方均值的平方根

$$RMSFE = \sqrt{E[(Y_{T+1} - \hat{Y}_{T+1 \mid T})^2]}$$

根均方预测误差有两种误差来源：由于$$u_i$$的未来值未知，以及$$\beta_0$$和$$\beta_1$$的估计偏差。如果第一种错误比第二种错误大得多（当样本容量足够大时便会如此），则RMSFE近似为$$\sqrt{Var(u_t)}$$，即总体自回归模型中误差项的标准差。而$$u_t$$的标准差可用回归标准误差（SER）来估计。因此，如果系数估计的误差小到可以被忽略，则RMSFE能通过回归标准误差来估计。

### P阶自回归模型

AR(1)模型使用$$Y_{t-1}$$来预测$$Y_t$$，但这样做会忽略高阶滞后项中包含的有用信息。若想利用这些信息，一个方法是在AR(1)模型中加入额外的滞后项，即得到p阶自回归模型，记为AR(p)。

P阶自回归模型(AR(p)模型)把$$Y_t$$作为其p阶滞后值的线性函数，即在AR(p)模型中，解释变量为$$Y_{t-1}, Y_{t-2}, ..., Y_{t-p}$$和一个截距项。AR(p)模型中滞后项的个数p，被称为自回归模型的滞后阶数，或滞后长度。

AR(p)模型的预测和误差项的性质。模型假设给定$$Y_t$$的所有滞后值，$$u_t$$的条件期望为零，即$$E(u_t \mid Y_{t-1}, Y_{t-2},...) = 0$$。这一假设意味着以下两点：

第一，以这个序列的所有历史值对$$Y_{T+1}$$的最优预测只依赖于最近的p个滞后值。特别地，令$$Y_{T+1 \mid T} = E(Y_{T+1} \mid Y_{T}, Y_{T-1}, ...)$$表示在给定$$Y_t$$所有历史值的条件下$$Y_{T+1}$$的条件均值。那么，基于所有历史值，$$Y_{T+1 \mid T}$$是所有预测中RMSFE最小的预测值。如果$$Y_t$$服从AR(p)过程，则基于$$Y_T, Y_{T-1}, ...$$对$$Y_{T+1}$$的最佳预测是

$$Y_{T+1 \mid T} = \beta_0 + \beta_1 Y_t + \beta_2 Y_{t-1} + ... + \beta_p Y_{t-p+1}$$

这个模型服从AR(p)过程，同时假设$$E(u_t \mid Y_{t-1}, Y_{t-2}, ...) =0$$。在实践中，系数$$\beta_0, \beta_1, ..., \beta_p$$未知，因此在使用上式进行预测时，将使用各个系数的估计值。

第二，误差项$$u_t$$是序列不相关的。

## 包含额外预测变量的时间序列模型和自回归分布滞后模型

经济理论通常会提出对预测有帮助的其他变量。这些其他变量（或预测变量）能被加入自回归模型中以得到一个包含多个预测变量的时间序列回归模型。当自回归模型包含其他变量及它们的滞后值时，得到的便是自回归分布滞后模型。

### 自回归分布滞后模型

包含被解释变量$$Y_t$$的p阶滞后值与额外预测变量$$X_t$$的q阶滞后值的自回归分布滞后模型，记作ADL(p,q)，模型形式为：

$$Y_t = \beta_0 +\beta_1 Y_{t-1} + \beta_2 Y_{t-2} + ... + \beta_p Y_{t-p} + \sigma_1 X_{t-1} + \sigma_2 X_{t-2} + ... + \sigma_q X_{t-q} + u_t$$

其中，$$\beta_0, \beta_1, ..., \beta_p, \sigma_1, ..., \sigma_q$$为未知系数，$$u_t$$为误差项，同时假设$$E(u_t \mid Y_{t-1}, Y_{t-2}, ..., X_{t-1}, X_{t-2}, ...) =0$$

ADL模型假设给定Y和X的所有历史值，误差项的条件均值为零，这意味着误差项不包含Y和X的任何历史信息。换言之，本模型的滞后阶数p和q是真实的滞后阶数，即除了上式所示的滞后项外，模型不再包含X和Y的任何其他滞后项。

ADL模型包含了被解释变量的滞后项（自回归部分），以及单个额外预测变量X的滞后项。总体而言，利用多个预测变量可以改进预测结果。但在讨论包含多个预测变量的时间序列回归模型之前，我们首先介绍平稳性的概念。

### 平稳性

时间序列数据的回归分析需要利用历史数据来量化变量间的历史关系。如果未来和过去相似，则这些历史关系可以被用来预测未来。但如果未来和过去有根本性不同，则这些历史关系可能不再是预测未来的可靠依据。

在时间序列分析中，“历史关系可推广到未来”这一思想可由平稳性（stationarity）概念来总结，即时间序列变量的概率分布不随时间而改变。

如果一个时间序列$$Y_t$$的概率分布不随时间而改变，即对任意T值而言，$$(Y_{s+1}, Y_{s+2}, ..., Y_{s+T})$$的联合分布不依赖于s，则这个时间序列是平稳的，否则是非平稳的。如果对任何T来说，一对时间序列$$X_t$$和$$Y_t$$的联合分布$$(X_{s+1}, Y_{s+1}, X_{s+2}, Y_{s+2}, ..., X_{s+T}, Y_{s+T})$$都不依赖于s，则这对时间序列被称为联合平稳的。平稳性要求未来和过去相似，至少在概率分布要如此。

### 包含多个预测变量的时间序列回归

包含多个预测变量的时间序列模型拓展了ADL模型，使得ADL模型包含多个预测变量和它们的滞后值。因为存在多个预测变量及其滞后值，从而我们采用双下标来表示回归系数和解释变量。

假设时间序列回归模型包含k个额外预测变量，其中，第一个预测变量有$$q_1$$个滞后项，第二个预测变量有$$q_2$$个滞后项，以此类推。则这样的模型可表示为

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + ... + \beta_p Y_{t-p} + \sigma_{11} X_{1t-1} + ... + \sigma_{1q_1} X_{1t-q_1} + ... + \sigma_{k1} X_{kt} + \sigma_{k2} X_{kt-2} + \sigma_{kq_k} X_{kt-q_k} + u_t$$

其中：(1) $$E(u_t \mid Y_{t-1}, Y_{t-2}, ..., X_{1t-1}, X_{1t-2}, ..., X_{kt-1}, X_{kt-2}, ...) = 0$$

(2) 随机变量$$(Y_t, X_{1t}, ..., X_{kt})$$具有平稳分布，并且随着j变大时，$$(Y_t, X_{1t}, ..., X_{kt})$$和￥￥（Y_{t-j}, X_{1t-j}, ..., X_{kt-j}$$相互独立。

(3) 不存在大的异常值，即$$X_{1t}, ..., X_{kt}$$和$$Y_t$$具有非零、有限四阶矩。

(4) 不存在完全多重共线性。

时间序列回归模型的假设。上述假设是对针对横截面数据的多元回归模型的最小二乘假设的修正。

第一个假设是：给定所有解释变量及解释变量的所有滞后项，误差项的条件均值为零。这个假设拓展了AR模型及ADL模型的假设，且意味着利用Y和X的所有历史值对$$Y_t$$的最佳预测可由上式给出。

横截面数据的第二个最小二乘假设为$$(X_{1i}, X_{2i}, ..., X_{ki}, Y_i)$$为独立同分布的，而时间序列回归模型的第二个假设则根据时间序列数据特点对其进行了修正，可用两部分来表述：

第一，数据是从平稳分布中抽取的，从而今天的数据分布与过去的分布是相同的。显而易见，时间序列数据的这个假设相当于横截面数据中独立同分布假设中的“同分布”部分：时间序列变量（包括其滞后项）的联合分布不随时间而变化。在实践中，许多时间序列看上去是非平稳的，这意味着这个假设在实践中往往不成立。若时间序列变量是非平稳的，则时间序列回归会出现问题：预测可能是有偏误的，预测也可能不是有效的（基于同样的样本能得到MSFE更小的预测值），或者基于OLS的常规统计推断可能存在误导性。究竟哪个问题会出现，以及有什么应对方法，这些需要根据非平稳的原因来进行判断。经济时间序列中存在的两种重要类型的非平稳：趋势及突变，包括非平稳可能的带来的问题、检验方法及应对方法。但现在我们暂时假设时间序列是联合平稳的，从而关注平稳时间序列变量的回归。

第二，时间序列回归的第二个假设是另一部分是：随着时间间隔变大，随机变量呈现出相互独立的性质。这个假设代替了独立分布假设。这个假设有时也被称为弱相依（weak dependence），它保证了在大样本中，数据存在足够的随机性，从而大数定律和中心极限定理得以成立。

第三个假设与最小二乘假设一样，即不存在大的异常值。用数学语言精确表达为：所有变量具有非零、有限的四阶矩。

第四个假设也和横截面数据对应的假设相同，即回归变量之间不存在完全多重共线性。

统计推断及Granger因果关系检验。时间序列回归模型中的统计推断过程与横截面数据中的情形一样。

在时间序列预测中，F统计量有一个重要作用：它可以用来检验某个解释变量的滞后值是否提供了其他解释变量所不包含的有用的预测信息。如果一个变量没有预测作用，则这个变量的所有滞后项的系数均为零。检验这一原假设的F统计量被称为Granger因果统计量（Granger causality statistic），相对应的检验被称为Granger因果关系检验。

Granger因果统计量是指检验原假设“包含多个预测变量的时间序列回归中某个变量所有滞后项的系数均为零”所构造的F统计量。例如，$$(X_{1t-1}, X_{1t-2}, ..., X_{1t-q})$$前的系数均为零的原假设意味着，这些解释变量没有包含其他解释变量所不包含的预测信息，对这个原假设进行的检验被称为Granger因果关系检验。

Granger因果关系与本书其他章节所提及的因果关系是不同的概念，二者之间没有任何关系。在第1章中，因果关系是利用理性化随机对照实验的术语来定义的，即在其他实验条件不变的情况下，改变X的值对Y所产生的影响。与之不同的是，Granger因果关系意味着，如果X Granger导致了Y，则给定回归方程中的其他变量的情况下，X是Y的有用预测变量。尽管”Granger预测关系“比”Granger因果关系“更精准，但后者已经成为计量经济学专用术语的一部分。

### 预测的不确定性和预测区间

在任何估计中，最好能给出关于这个估计的不确定性的度量，预测也不例外。一个度量预测不确定性的工具是根均方预测误差(RMSFE)。在加入“$$u_t$$服从正态分布”这个假设后，RMSFE能够被用来构建一个预测区间，即以一定概率包含变量未来真值的区间。

预测不确定性。预测误差有两个组成部分：回归方程的系数估计导致的不确定性及由$$u_t$$未来取值不确定所导致的预测不确定性。对于系数不多、观测样本足够大的回归而言，由后者导致的预测不确定性要比由前者导致的不确定性大得多。一般而言，两种不确定性来源都是重要的，从而我们现在给出一种能同时包含两种不确定性的RMSFE表达方式。

为了简化符号，考虑仅有一个预测变量的ADL(1,1)模型，即$$Y_t = \beta_0 + \beta_1 Y_{t-1} + \sigma_1 X_{t-1} + u_t$$。我们用该模型来预测$$Y_{T+1}$$，同时假设$$u_t$$是同方差的。预测值为$$\hat{Y}_{T+1 \mid T} = \hat{\beta}_0 + \hat{\beta}_1 Y_T + \hat{\sigma}_1 X_T$$，预测误差为

$$Y_{T+1} - \hat{Y}_{T+1 \mid T} = u_{T+1} - [(\hat{\beta}_0 - \beta_0) + (\hat{\beta}_1 - \beta_1) Y_T + (\hat{\sigma}_1 - \sigma_1) X_T]$$

因为$$u_{T+1}$$的条件均值为零，且为同方差，$$u_{T+1}$$的方差为$$\sigma_u^2$$，并且和上式方括号内的项不相关。因此，均方误差MSFE为

$$\begin{align}
MSFE &= E[(Y_{T+1} - \hat{Y}_{T+1 \mid T})^2] \\
&= \sigma_u^2 + Var[(\hat{\beta}_0 - \beta_0) + (\hat{\beta}_1 - \beta_1) Y_T + (\hat{\sigma}_1 - \sigma_1) X_T] \\
\end{align}$$

RMSFE是MSFE的平方根。

要估计MSFE，就需要估计上式中的两部分。其中，第一项$$\sigma_u^2$$可用回归标准误差的平方来估计，第二项需要估计回归系数加权平均的方差。

估计MSFE的另外一种方法是利用伪样本外预测的方差。

预测区间。预测区间在很多方面都和置信区间类似。例如，一个95%的预测区间（forecast interval）是以95%的概率包含序列未来真值的区间。

预测区间和置信区间的一大重要差别在于：95%的置信区间的常用公式（$$估计量 \pm 1.96 倍标准误差$$）是以中心极限定理为基础的，因此这一结果对很大范围内的误差项分布设定都成立。与之不同的是，因为预测误差包含了误差项$$u_{T+1}$$的取值，为了计算预测区间，需要估计误差项的分布，或对其分布做出一些假设。

在实践中，通常假设$$u_{T+1}$$服从正态分布。若如此，预测误差是两个独立的正态分布之和（在中心极限定理下，$$\hat{\beta}_0, \hat{\beta}_1, \hat{\sigma}_1$$服从正态分布），因此预测误差服从正态分布，方差和MSFE一样。因此，95%的置信区间为$$\hat{Y}_{T \mid T+1} \pm 1.96 SE(Y_{T+1} - \hat{Y}_{T+1 \mid T})$$，其中，$$SE(Y_{T+1} - \hat{Y}_{T+1 \mid T})$$为RMSFE的估计。

我们这里主要讨论误差项$$u_{T+1}$$为同方差的情况。如果$$u_{T+1}$$是异方差的，则可能需要开发一种异方差模型，以估计$$\sigma_u^2$$项。

由于存在与$$u_{T+1}$$相关的不确定性，95%的预测区间可能很大，会使得预测区间的决策参考价值很小。因此，专业的预测值通常会给出比95%更紧的预测区间，如一倍标准差预测区间（如果误差项服从正态分布，则为68%的预测区间）。除此之外，一些预测人员会报告多种预测区间。

## 运用信息准则选择滞后阶数

一般而言，采用一阶滞后确实有道理，也容易被理解。但是，为何使用二阶滞后而不是三阶或四阶滞后呢？更一般地，在一个时间序列回归中，我们应该使用多少阶的滞后项呢？本节将介绍一些选择滞后阶数的统计方法，首先介绍自回归方程中滞后阶数的选择，其次介绍包含多个预测变量的时间序列回归模型中滞后阶数的选择。

### 确定自回归的滞后阶数

在选择自回归模型的滞后阶数时，我们需要权衡加入更多滞后项的边际收益和增加估计不确定性的边际成本。一方面，如果自回归模型中的阶数太低，则可能会遗漏包含在较远滞后项中有价值的信息。另一方面，如果滞后阶数太高，你就需要估计更多的系数，从而给预测带来额外的估计误差。

F统计量方法。一个方法是：首先建立高阶的自回归模型，接着对滞后阶数进行假设检验。例如，你可能一开始估计的是AR(6)模型，接着可以通过假设检验，以检验第6个滞后项的系数在5%的显著性水平下是否显著。如果不显著，则去掉第6个滞后项，估计AR(5)模型，并接着检验第5项滞后系数的显著性，以此类推。这个方法的缺点在于，有时会得出一个过于庞大的模型，即使AR模型的滞后阶数为5，即第6个滞后项的系数为零，但在5%的显著性水平下，使用t统计量对第6个滞后项的稀疏进行假设检验时，可能会错误地拒绝原假设。因此，当真实滞后阶数为5时，这种方法会有5%的概率认为滞后阶数为6.

BIC。另外一种估计滞后阶数的方法是最小化”信息准则“。其中一种信息准则为贝叶斯信息准则（Bayes information criterion，BIC），也被称为Schwarz信息准则（SIC），其表达式为

$$BIC(p) = ln[\frac{SSR(p)}{T}] + (p+1) \frac{ln(T)}{T}$$

其中，$$SSR(p)$$为AR(p)模型的残差平方和。p的BIC估计值$$\hat{p}$$是$$$p=0,1,...,p_{max}$$中使得BIC(p)最小化的取值，其中$$p_{max}$$是我们所考虑的最大p值，p=0对应于只包含一个截距项的模型。

乍看起来，BIC公式似乎有一些神秘，但它是符合直觉的。现在考虑上式中的第一项。因为回归方程的系数是通过OLS方法估计，残差平方和会随着滞后阶数的增加而减少（至少非增）。第二项为被估计参数的个数（p个滞后项的系数及常数项）乘以$$ln(T)/ T$$。第二项随着滞后项的加入而增加。BIC准则权衡了这两股力量，使得p的BIC估计量为真实滞后阶数的一致估计。

AIC。BIC并不是唯一的信息准则，另一种信息准则为赤池信息准则（Akaike information criterion，AIC）：

$$AIC(p) = ln[\frac{SSR(p)}{T}] + (p+1) \frac{2}{T}$$

AIC和BIC的差别在于BIC中的ln(T)被AIC中的2所代替，因此AIC中的第二项会更小。因此，在AIC中SSR要降低更多才能说明加入更多的滞后项是正确的。从理论上说，AIC中的第二项没有足够大以保证选择正确的滞后阶数，即使大样本下也是如此。因此，p的AIC估计值是非一致的。在大样本下，AIC会以非零概率高估p的值。

尽管理论上存在这些不足，AIC在实践中仍然被大量使用。如果你担心BIC给出的滞后阶数太少，AIC准则会是一个合理的选择。

信息准则计算的注释。若要评价两个模型的拟合效果，最好是采用相同的数据来估计它们。因为BIC和AIC是进行这种比较的正规方法，纳入比较的自回归模型应该用相同的样本进行估计。

### 包含多个预测变量的时间序列回归模型的滞后阶数选择

包含多个预测变量的时间序列回归模型滞后阶数的选择问题和自回归模型类似：滞后项太少会丢失有价值的信息而降低预测准确性，而滞后项太多则会增加估计的不确定性。滞后项的选择必须在得到更多信息和估计更多系数之间达到平衡。

F统计量方法。与单变量自回归模型一样，在包含多个预测变量的时间序列回归模型中确定滞后阶数的一个方法是，使用F统计量对滞后项前的系数进行联合假设检验。如果待比较的模型数量较少，则F统计量方法简单易用。然而，一般来说，F统计量方法可能会高估滞后阶数。

信息准则。与自回归模型的情形一样，BIC和AIC准则能够用来估计包含多个预测变量的时间序列回归模型的滞后阶数。如果回归模型有K个系数（包含截距项），BIC为

$$BIC(p) = ln[\frac{SSR(K)}{T}] + K \frac{ln(T)}{T}$$

AIC准则的定义方式相似，只是上式中ln(T)被数值2所替换。在实际应用中，我们对每一个候选模型均使用BIC或AIC准则，将BIC或AIC准则取最小值的模型作为我们最终选择的模型。

在使用信息准则估计滞后阶数时，有两个重要的问题需要考虑：首先，正如在自回归模型中所讨论的，所有候选模型必须用同样的样本进行估计。上式中符号T表示用来估计模型的观测值数量，它必须对所有的模型都一样。第二，当存在多个预测变量时，这个方法对计算的要求很高，因为需要计算很多不同的模型（许多不同滞后项的组合）。在实践中，一个捷径是对所有解释变量使用相同的滞后阶数，即设定$$p=q_1 = ... = q_k$$，因此只需要比较$$p_{max+1}$$个模型。

## 非平稳性I：趋势

我们假设被解释变量和解释变量是平稳的。但是，如果解释变量和被解释变量是非平稳的，则常规的假设检验、置信区间和预测将变得不再可靠。非平稳会导致什么问题？我们用什么方法来解决？这些都取决于非平稳的特征或类型。

### 趋势是什么

趋势（trend）是一个变量随着时间变化而呈现出的一种持续性的、长期的移动。时间序列变量会围绕其趋势上下波动。

确定性和随机趋势。时间序列的趋势有两种，分别为确定性趋势和随机性趋势。确定性趋势（deterministic trend）是时间的一个非随机函数。相比较而言，随机性趋势（stochastic trend）是随机的且随时间而变化。像很多其他计量经济学家一样，我们认为在建模中使用随机性趋势要比确定性趋势更合适。出于这些原因的考虑，我们主要关注经济时间序列中的随机性趋势，而不是确定性趋势。并且，当我们提到时间序列数据的“趋势”这个术语时，我们指的是随机性趋势。本节将介绍关于随机性趋势的最简单的模型，即随机游走模型。

随机游走模型。具有随机趋势的单变量模型中，最简单的是随机游走模型。如果一个时间序列$$Y_t$$的变化量是独立同分布的，则称该时间序列变量$$Y_t$$服从随机游走（random walk），即

$$Y_t = Y_{t-1} + u_t$$

其中，$$u_t$$是独立同分布的。然而，更一般地，我们将服从上式且$$u_t$$的条件均值为零$$(E(u_t \mid Y_{t-1}, Y_{t-2}, ...)=0)$$的模型称为随机游走。

随机游走的基本思想是序列在明天的取值等于序列在今天的取值加上一个不可预测的变化：因为$$Y_t$$遵循的路径由随机变量$$u_t$$决定，从而路径是“随机游走”。因为$$E(u_t \mid Y_{t-1}, Y_{t-2}, ...)=0$$，所以$$E(Y_t \mid Y_{t-1}, Y_{t-2}, ...)=Y_{t-1}$$。换句话说，如果$$Y_t$$服从随机游走，则对序列明天取值的最优预测为今天的取值。

一些序列存在一个明显的上升趋势，在这种情况下，序列必须包含一个使序列有上升趋势的调整项。这个调整项或者“漂移”项拓展了随机游走模型。这个扩展后的模型被称为带漂移的随机游走（random walk with drift）：

$$Y_t = \beta_0 + Y_{t-1} + u_t$$

其中，$$E(u_t \mid Y_{t-1}, Y_{t-2}, ...)=0$$，$$\beta_0$$是随机游走的漂移项。如果$$\beta_0$$为正，则$$Y_t$$平均来看是上升的。在带漂移项的随机游走模型中，序列明天取值的最优估计是序列今天的取值加上漂移项$$\beta_0$$。

随机游走模型虽然简单，却很有用，它是本书中所使用的最基本的趋势模型。

随机游走模型是非平稳的。如果$$Y_t$$服从随机游走，则它一定是非平稳的：随机游走过程的方差随着时间而增加，因此$$Y_t$$的分布随时间而变化。因此$$u_t$$和$$Y_{t-1}$$不相关，因此$$Var(Y_t) = Var(Y_{t-1}) + Var(u_t)$$；想要使得$$Y_t$$为平稳的，$$Y_t$$的方差不能依赖于时间，即$$Y_t$$的方差必须和$$Y_{t-1}$$的方差相同，但这只有当$$u_t$$的方差为零时才成立。从另一个角度来思考，想象$$Y_t$$的初始值为零，即$$Y_0 = 0$$。因此，$$Y_1 = u_1, Y_2 = u_1 + u_2$$，以此类推，$$Y_t = u_1 + u_2 + ... + u_t$$。因为$$u_t$$是序列不相关，从而$$Var(Y_t) = Var(u_1 + u_2 + ... + u_t) = t \sigma_u^2$$。因此，$$Y_t$$的方差依赖于时间t，它随着t的增加而增加。这充分表明，$$Y_t$$是非平稳的。

因为随机游走的方差递增且无上界，它的总体自相关不能被定义（一阶协方差和方差是无界的，故无法定义二者的比率）。然而，随机游走的一个特定是，它的样本自相关系数非常接近1；事实上，随机游走的第j阶的样本自相关系数依概率收敛到1。

随机性趋势、自回归模型和单位根。随机游走模型是AR(1)模型是特殊形式，其中$$\beta_1=1$$。换言之，如果$$Y_t$$服从AR(1)且$$\beta_1=1$$，则$$Y_t$$包含一个随机性趋势且是非平稳的。但是，如果$$\beta_1$$的绝对值小于1，且$$u_t$$是平稳的，则$$Y_t$$和它的滞后项的联合分布不依赖于时间t，故$$Y_t$$是平稳的。

使AR(p)为平稳过程的条件比使AR(1)为平稳过程的条件更复杂。它的正式表述设计多项式$$1-\beta_1 z - \beta_2 z^2 - \beta_3 z^3 - ... - \beta_p z^p$$的根（满足$$1-\beta_1 z - \beta_2 z^2 - \beta_3 z^3 - ... - \beta_p z^p=0$$的根）。为了使AR(p)平稳，这个多项式根的绝对值必须大于1。以AR(1)模型为例，多项式的根为$$z=1/\beta_1$$。因此，根的绝对值大于1等价于$$\beta_1$$的绝对值小于1。

如果AR(p)有一个根为1，我们称这个序列有一个单位根。如果$$Y_t$$有一个单位根，则它包含一个随机性趋势。如果$$Y_t$$是平稳的（因此不包含单位根），则它不包含随机性趋势。出于这个原因，我们将不加区别地使用术语随机性趋势和单位根。

### 随机性趋势产生的问题

如果一个时间序列具有随机性趋势（有一个单位根），则这个模型系数的OLS估计量及相应t统计量的分布将是非标准的（非正态的），即使在大样本下依然如此。我们讨论这个问题的三个方面：(1) 如果AR(1)模型是单位根过程，则它的系数估计量将偏向于零；(2) 解释变量系数的t统计量分布是非正态的，即使在大样本下也是如此；(3) 随机趋势带来最极端的威胁是，如果两个序列均有随机性趋势，即使它们相互独立，我们也会有很大概率得出“二者是相关”的结论，这种情况被称为伪回归。

问题1：自回归的系数偏向于零。假设$$Y_t$$服从随机游走过程，但计量经济学家并不知道这一点，从而他们会使用AR(1)模型进行估计。因为$$Y_t$$是非平稳的，所以时间序列最小二乘回归的基本假设不成立。总的来说，在这种情况下，我们不能指望估计量及检验统计量仍服从大样本下的正态分布。事实上，在这个例子中，自回归系数的OLS估计量是一致的，但它的分布为非正态的，即使在大样本下也是如此：$$\hat{\beta}_1$$的渐近分布偏向于零。$$\hat{\beta}_1$$的期望值存在一个与样本数量T相关的偏差。

“系数偏向于零”这一性质意味着，如果$$Y_t$$是随机游走过程，则与基于随机游走模型的预测相比，AR(1)模型的预测表现更差。这个结论对高阶自回归模型同样适用。

问题2：t统计量的非正态分布。如果一个时间序列包含了一个随机性趋势，则它的最小二乘t统计量的分布在原假设下为非正态的，即使在大样本下依然如此。这意味着常规的置信区间不再有效，且假设检验也不能通过常规方法进行。一般来说，这个t统计量分布的临界值很难通过临界值表示出来，因为这个统计量的分布依赖于解释变量之间的关系。

含有单位根的自回归模型是一个特例，其t统计量分布的临界值能够通过列表给出。

问题3：伪回归。随机性趋势会导致两个原本没有相关性的时间序列呈现出相关性，这个问题被称为伪回归（spurious regression）。当序列包含随机性趋势时，OLS的回归结果可能具有误导性。在一种特殊情况下，回归结果是可靠的。这种特殊情况是指，两个序列的趋势成分是相同的，即这两个序列包含共同的随机趋势。在这种情况下，我们称这两个序列是协整的。

### 检测随机性趋势：单位AR根的检验

我们可以通过正式或非正式的方法来检验时间序列是否存在趋势。其中，非正式的方法包括观察数据图、计算自相关系数。因为，当时间序列包含随机性趋势时，一阶自相关系数将接近于1，这个性质至少在大样本下成立。较小的一阶自相关系数，连同没有明显趋势的数据图意味着序列不包含趋势。如果担心这种方法不可靠，我们就可以使用正式的检验方法，即检验的原假设为时间序列包含趋势；备择假设为时间序列不包含趋势。

我们使用Dickey-Fuller检验方法来检验时间序列的随机性趋势。尽管Dickey-Fuller检验不是唯一能够用来检验随机性趋势的检验方法，DF检验是实践中最常用且最可靠的检验。

AR(1)模型的Dickey-Fuller检验。我们从自回归模型入手来介绍Dickey-Fuller检验。正如之前所讨论的，随机游走是AR(1)模型的特例，其中，$$\beta_1=1$$。如果$$\beta_1=1, Y_t$$是非平稳的且包含一个随机性趋势。在AR(1)模型的框架下，$$Y_t$$有随机性趋势的假设检验可以通过以下检验进行：

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t, H_0: \beta_1=1, H_1: \beta_1 <1$$

如果$$\beta_1 = 1$$，AR(1)有一个单位根，因此上式中的原假设是AR(1)有一个单位根，备择假设为AR(1)模型是平稳的。

这个检验有一个简化形式。将上式两边同时减去$$Y_{t-1}$$，并令$$\sigma = \beta_1 -1$$，则转换为

$$\Delta Y_t = \beta_0 + \sigma Y_{t-1} + u_t, H_0: \sigma=1, H_1: \sigma <1$$

上式中，检验$$\sigma=0$$的t统计量被称为Dickey-Fuller统计量。上式的结构简单，回归软件会自动给出检验$$\sigma=0$$的t统计量。值得注意的是，因为备择假设是单边的，因此Dickey-Fuller检验是单边检验。此外，Dickey-Fuller统计量使用“非稳健”标准误差计算的。

AR(p)模型的Dickey-Fuller检验。上式中的Dickey-Fuller检验只适用于AR(1)模型。对于一些时间序列来说，AR(1)模型无法描述其所有的序列相关性，因此在这种情况下，更高阶的自回归模型才是更合适的。

Dickey-Fuller检验在AR(p)模型中的推广：ADF检验是对自回归模型进行的假设检验，其中，$$H_0: \sigma=0, H_1: \sigma<0$$，回归模型如下：

$$\Delta Y_t = \beta_0 + \sigma Y_{t-1} + \gamma_1 \Delta Y_{t-1} + \gamma_2 \Delta Y_{t-2} + ... + \gamma_p \Delta Y_{t-p} + u_t$$

在原假设下，$$Y_t$$包含一个随机性趋势；在备择假设下，$$Y_t$$是平稳的。ADF统计量是检验上式中$$\sigma=0$$的最小二乘统计量。

如果备择假设为“$$Y_t$$为带确定性线性时间趋势的平稳过程”，则时间趋势t必须作为一个额外的解释变量加入回归方程中。在这种情况下，Dickey-Fuller检验回归方程变为：

$$\Delta Y_t = \beta_0 + at + \sigma Y_{t-1} + \gamma_1 \Delta Y_{t-1} + \gamma_2 \Delta Y_{t-2} + ... + \gamma_p \Delta Y_{t-p} + u_t$$

其中，a是未知系数，ADF统计量是检验上式中$$\sigma=0$$的最小二乘t统计量。

滞后阶数p可以通过BIC或AIC准则来估计。当p为零时，说明$$\Delta Y_t$$的滞后项不能作为解释变量加入，ADF检验简化成了AR(1)模型下的DF检验。ADF统计量的分布是非正态的，即使在大样本下依然如此。单边ADF检验的临界值依赖于回归方程的形式。

研究显示，在ADF检验中，使用较多的滞后阶数比使用较少的滞后阶数要好，因此在进行ADF检验时，建议使用AIC准则而不是BIC准则来估计滞后阶数。

### 避免由随机性趋势带来的问题

当时间序列存在趋势时，可靠的方法是通过序列变换使它不再包含趋势。如果序列具有随机性趋势，即序列有单位根，则序列的一阶差分序列将不含趋势。例如，若$$Y_t$$服从随机游走，则$$Y_t = \beta_0 + Y_{t-1} + u_t$$，于是$$\Delta Y_t = \beta_0 + u_t$$是平稳的。因此，利用一阶差分能够去除序列中的随机性趋势。

然而，在实践中，你很难百分之百确定一个序列是否包含随机性趋势。一般来说，无法拒绝原假设并不意味着原假设为真，这只能表示没有足够证据证明它是错误的。因此，利用ADF检验得出无法拒绝单位根的原假设并不意味着序列真的有一个单位根。例如，假设AR(1)模型中$$\beta_1$$的真实值非常接近于1，比如说为0.98，在这种情况下ADF检验的势很低，即正确拒绝原假设的概率较低。然而，虽然无法拒绝单位根的原假设并不意味着序列有一个单位根，但我们依然可以认为自回归模型的真实系数近似为1，并使用该序列的差分序列而不是其水平量。

## 非平稳性II：突变

第二种非平稳性是指，总体回归函数随样本的变化。在经济学中，突变会由于很多不同的原因而发生，例如，经济政策的变化、经济结构的变化或由于创新引起的产业变化。如果这些变化或者“突变”（breaks）发生了，则基于忽略这些变化的回归模型而做出的统计推断及预测将是有偏误的。

本节将介绍两种时间序列回归方程的突变检验方法：第一种方法从假设检验的角度出发，通过F统计量检验回归方程系数的变化。第二种方法从预测的角度出发来检验方程的突变：首先利用部分样本（时间较早的案部分样本）来估计模型，然后利用估计结果来预测另一部分样本的值，最后判断预测效果。如果预测效果比预想的差很多，则有理由认为存在突变。

### 什么是突变

突变是指总体回归方程的系数在某一个特定时间发生了离散变化，或在一段较长时间内发生了缓慢变化。

宏观经济数据发生离散变化的原因之一是宏观经济政策发生了较大变化。

突变也可能是缓慢变化的，在这种情况下，总体回归方程是随着时间逐渐变化的。例如，总体回归方程的渐变现象可能是因为经济政策的缓慢转变或经济结构的持续性变化而发生。本节介绍的突变检验方法对以上两种突变情形都适用，即离散变化和缓慢变化。

突变导致的问题。在样本区间内，如果总体回归函数发生了改变，则对全样本进行回归得到的OLS估计量度量的是变量之间的“平均”关系，即估计结果融合了两段不同时期内的变量关系。根据突变发生的时间及突变的大小，“平均”回归函数可能和真实的回归函数有很大不同，这将导致预测效果很差。

### 突变的检验

发现突变的一种方法是检验回归系数的离散变化。如何进行检验呢？这将依赖于突变发生时点（突变点）是否已知。

突变点已知的突变检验。在某些应用中，你可能会想知道在一个给定时间点是否发生了突变。例如，如果你正在利用20世纪70年代以来的数据学习国际贸易关系，你可能会假设利率的总体回归函数在1972年（当固定汇率的布雷顿森林体系解体时）发生了一次突变。

如果突变发生的时点是已知的，则可以使用二元变量交互回归来检验不存在系数突变的原假设。为了使问题变得简单，我们考虑ADL(1,1)模型，因此方程中存在一个截距项、$$Y_t$$的一个滞后项及$$X_t$$的一个滞后项。令$$\tau$$表示假设的突变时点，令$$D_t(\tau)$$表示一个二元变量，在突变发生前，其取值为0，而在突变发生后则取值为1。因此，加入二元变量后的回归方程为

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + \sigma_1 X_{t-1} + \gamma_0 D_t(\tau) + \gamma_1 [D_t (\tau) \times Y_{t-1}] + \gamma_2 [D_t (\tau) \times X_{t-1}] + u_t$$

如果不存在突变，则总体回归函数在两部分样本下是相同的，因此含有二元变量$$D_t (\tau)$$的项不会出现在上式中。换言之，在不存在突变的原假设下，$$\gamma_0 = \gamma_1 = \gamma_2 =0$$。在存在突变的备择假设下，总体回归函数在突变时点$$\tau$$的前后存在不同，在这种情况下，至少有一个系数$$\gamma$$不为零。这个检验经常被称为突变点已知的邹氏检验（Chow test），即以它的发明者（Gregory Chow）的姓氏命名。

如果方程中包含多个预测变量或更多的滞后项，则可构造所有解释变量的二元变量交互项，并检验所有含有$$D_t (\tau)$$的项对应的系数皆为零的假设。

通过一些修正，我们可以仅仅检验某一部分系数是否发生突变，即只对某些解释变量设置二元交互项。

突变点未知的突变检验。突变发生的具体时点通常是未知的，或者我们只知道突变点的范围。例如，你怀疑一个突变点可能在两个时点（$$\tau_0$$和$$\tau_1$$）之间发生。通过修正后的邹氏检验可以用来检验这类已知突变点范围的突变。我们首先计算从$$\tau_0$$到$$\tau_1$$所有时点的邹氏检验F统计量，然后用计算出的最大的F统计量来检验这类未知时点的突变。这种修正后的邹氏检验也被称为Quandt似然比检验（QLR）统计量，或称为sup-Wald统计量。

因为QLR统计量是很多F统计量中最大的一个，从而它的分布和单个F统计量是不同的。QLR统计量的临界值必须从一个特殊的分布中得到。类似常规的F统计量，这个分布依赖于待检验的约束个数q，即备择假设下允许发生突变或变化的系数个数（包括截距项）。此外，QLR统计量的分布也依赖于$$\tau_0 / T$$和$$\tau_1 / T$$，即将用于计算F统计量的子样本端点$$\tau_0$$和$$\tau_1$$与总样本容量的比例。

为了使大样本下的QLR统计量渐近分布具有良好的性质，子样本的端点$$\tau_0$$和$$\tau_1$$不能太接近于样本的起点和终点。出于这个原因，在实践中，我们通常是在“修剪”的样本范围或子集内计算QLR统计量。通常使用15%的剔除量，即令$$\tau_0=0.15T, \tau_1 = 0.85T$$（四舍五入后的整数）。经过15%的剔除后，F统计量是使用样本中间位置的70%数据来计算的。

类似于邹氏检验，QLR检验也可以用来检验回归方程中仅有部分回归系数发生突变的情形。我们首先在不同的突变点进行邹氏检验，并且只对那些我们怀疑发生突变的解释变量设置二元变量交互项。其次，计算从$$\tau_0$$到$$\tau_1$$所有时点的邹氏检验统计量的最大值。这种情况下的QLR检验临界值的限制条件q的数量是进行F检验时设定的限制条件的个数。

如果在被检验区间存在一个离散突变点，则QLR检验在大样本条件下将会以很大概率拒绝原假设。此外，F统计量达到最大值的时点$$\hat{\tau}$$是突变点$$\tau$$的估计值。在满足一定条件下有$$\hat{\tau} /T \to^p \tau / T$$；换言之，突变点的估计量与样本长度T的比值是真实突变点$$\tau$$与T比值的一致估计量。

当回归方程存在多个突变，或者当突变是以一种缓慢变化的形式发生时，QLR统计量在大样本下会以很大的概率拒绝“不存在突变”的原假设。这意味着，QLR统计量检测到的是回归系数的不稳定性，而不仅仅是单个离散突变。因此，如果QLR统计量拒绝了原假设，这意味着回归模型存在一次离散突变，或存在多次离散突变，或存在缓慢变化。

警告：你很可能不知道突变时点，即使你认为你知道。有时候，专家可能会相信他/她知道突变发生的时点，从而使用邹氏检验而不是QLR检验。但是，如果专家的这个判断是基于他/她对于这个有待分析序列的了解所作出的，实际上也是利用数据估计出来的，只不过他/她采用的是一种非正式的方法而已。对突变时点的初始值估计意味着常规用的F临界值不能用于该突变时点的邹氏检验。因此，在这种情况下使用QLR统计量仍然是合适的。

### 伪样本外预测

对模型预测效果的最终检验是看其在样本外预测中的表现，即利用估计得到的模型进行“现实”预测，并评估其预测表现。伪样本外预测（pseudo out-of-sample forecasting）是一种模拟预测模型的现实预测表现方法。伪样本外预测的思想很简单：挑选一个接近于样本期终点的时点，使用直到这一时点的样本数据来估计预测模型，然后使用预测模型做出一个预测（预测下一时点的值）。对样本期终点附近的多个时点重复进行上述步骤，可以得到一系列预测值，并由此计算出一系列伪样本外预测误差。通过检验这些预测误差，可以判读这些预测关系式是否是平稳的。

我们称这种方法为伪样本外预测，是因为它不是真正的样本外预测。真正的样本外预测是发生在现实中，即你在不知道序列未来真实值的情况下所作出的预测。在伪样本外预测中，你使用你的模型模拟现实中的真实预测，但你知道序列的“未来”真实值，从而可以用这些“未来”真实值来评估预测表现。伪样本外预测模拟了现实中的预测，但却不需要等待序列未来真实值的发布便可检验其表现。

伪样本外预测给了预测者一种用来评估其模型在样本终点附近预测效果的方法。这个方法提供了有价值的信息，它或者能提高我们对模型预测效果的信心，或者能提醒我们模型在最近一段时间可能存在突变。

伪样本外预测的计算方法分为以下几步：

(1) 选择观测的数量P，即将被用来进行伪样本外预测的样本量；例如，P可以是样本长度的10%或者20%，令s=T-P。

(2) 利用裁剪后的数据集（即t=1, ..., s）来估计预测模型。

(3) 计算s+1期的预测值，表示为$$\tilde{Y}_{s+1 \mid s}$$。

(4) 计算预测误差，$$\tilde{u}_{s+1} = Y_{s+1} - \tilde{Y}_{s+1 \mid s}$$。

(5) 对剩下的数据，即s=T-P+1到T=1，重复步骤(2)~步骤(4)（在每个时期重新估计模型）。伪样本外预测值是$${\tilde{Y}_{s+1 \mid s}, s= T-P,...,T-1}$$，伪样本外预测误差为$${\tilde{u}_{s+1 \mid s}, s=T-P,..., T-1}$$。

伪样本外预测的其他用途。伪样本外预测的第二个用途是估计RMSFE。因为伪样本外预测仅仅使用了预测时点之前的数据，从而伪样本外预测误差可以同时反映与误差项未来值相关的不确定性和由回归系数估计带来的不确定性；换言之，伪样本外预测误差包含了两种误差来源。因此，伪样本外预测误差的样本标准差可以作为RMSFE的估计值。RMSFE的这种估计量可以用来量化预测不确定性和构造预测区间。

伪样本外预测的第三个用途是比较两个或更多的预测模型。两个在拟合数据方面表现相当的模型，在伪样本外预测方面的表现却可能差异很大。当模型存在差异时（如使用了不同的预测变量），伪样本外预测提供了一种比较不同模型的简便方法，且这种方法侧重于比较模型在预测方面的表现。

### 避免由突变带来的问题

根据突变特点的不同，调整总体回归模型的最佳方法也不同。如果离散突变发生在某一特定时点，则QLR统计量将以很高的概率识别出突变，同时突变发生的时点也能被估计出来。因此，回归方程可以通过引入二元变量来进行调整。如果所有系数都存在突变，则这个方程将采用如下形式。其中的$$\tau$$可以用突变发生时点的估计量$$\hat{\tau}$$替代；如果只是某些系数发生了突变，则只有相关的交互项会出现在回归模型中。如果确实存在一个离散突变，则回归系数的统计推断和以往一样，例如，可对t统计量使用正态临界值。此外，也可以将回归模型用于预测。

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + \sigma_1 X_{t-1} + \gamma_0 D_t(\tau) + \gamma_1 [D_t (\tau) \times Y_{t-1}] + \gamma_2 [D_t (\tau) \times X_{t-1}] + u_t$$

如果突变不是离散度，而是缓慢且持续变化的，则补救方法将更复杂且超出了我们讨论的范围。

