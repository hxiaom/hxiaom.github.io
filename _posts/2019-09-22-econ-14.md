---
layout: post
title: 【Method】时间序列回归（一）导论
categories: Analytics
---

时间序列数据（同一个体在多个时点的观测数据）能被用于回答那些横截面数据所不能回答的问题。其中，一类问题是，一个变量$$X_i$$的变化对另一个变量$$Y_i$$的因果效应将如何随着时间变化？换言之，X的变化对Y的动态因果效应是什么？例如，一项要求乘客使用安全带的法律在生效初期及当驾驶员适应了这项法律之后，对交通事故死亡率的因果效应是什么？另一类问题是你对未来某个变量的最佳预测是什么？例如，你对下个月通货膨胀率、利率或股票价格的最佳预测是什么？这两类问题（第一类问题有关动态因果效应，第二类问题有关经济预测）都能通过使用时间序列数据来解答。但是，时间序列数据也给我们带来了挑战，为了克服这些挑战，我们需要一些新的技术。

“未来将会与过去很相似”，这是时间序列回归中非常重要的假设，我们称之为“平稳性”。时间序列变量在某些情况下可能并不平稳，其中与经济时间序列分析特别相关的两种情况：第一，序列存在长期趋势；第二，随着时间的推移，总体回归可能并不稳定，即总体回归中可能存在突变。这些对平稳性假设的偏离会严重危害到基于时间序列回归的预测和推断。然而幸运的是，我们有检验是否存在趋势和突变的统计方法，一旦发现有趋势或突变的现象，我们就能及时调整模型的设定形式。

## 利用回归模型进行预测

即使一个模型中的回归系数不能被解读为因果效应，但这个模型可能对预测有帮助。从预测角度看，重要的问题是这个模型能否提供一个尽可能精确的预测。尽管完美的预测并不存在，回归模型依然能给出精确和可靠的预测结果。

如果能获得过去的测试成绩，则一个好的起点是利用现在和过去的测试成绩来预测将来的测试成绩。

## 时间序列数据和序列相关介绍

### 滞后、一阶差分、对数值及增长率

时间序列变量Y在时点t的观测值用$$Y_t$$表示，总观测数用T表示。观测之间的间隔，即时点t和时点t+1之间的时间间隔是一个单位（例如，一周、一个月、一个季度、或一年）。

我们用特殊的术语和记号来表示Y的未来值和过去值。Y的前一期数值被称作Y的一阶滞后值，或更简洁地称为一阶滞后，记作$$Y_{t-1}$$。它的j阶滞后值是j个时期前的数值，记作$$Y_{t-j}$$。相似地，$$Y_{t+1}$$表示Y未来一期的数值。

Y在时期t-1和时期t之间数值变化量是$$Y_t-Y_{t-1}$$，这个变化量是变量$$Y_t$$的一阶差分。在时间序列数据中，“$$\Delta$$”用来表示一阶差分，故$$\Delta Y_t = Y_t - Y_{t-1}$$。

经济时间序列数据经常在取数值或取对数值的差分之后进行分析，这样做的一个原因是许多经济时间序列呈现近似指数增长的态势，即平均而言，长期中的序列增长率每年保持在一个固定的百分比。这一点意味着，序列的对数值近似线性增长。这样做的另一个原因是，许多经济时间序列变量的标准差大约与它的水平值成正比。这也意味着，序列对数值的标准差近似于一个常数。无论是哪一种情况，将时间序列数据转化成对数形式都是有用的，因为对数值的变化相当于原始序列的百分比变化。

$$Y_t$$的一阶滞后记作$$Y_{t-1}$$；j阶滞后记作$$Y_{t-j}$$。

一个序列的一阶差分，$$\Delta Y_t$$，是这个序列从t-1期到t期的变化量，即$$\Delta Y_t = Y_t - Y_{t-1}$$。

$$Y_t$$对数的一阶差分是$$\Delta ln(Y_t) = ln(Y_t) - ln(Y_{t-1})$$

一个时间序列$$Y_t$$从t-1期到t期的百分比变化大约为$$100 \Delta ln(Y_t)$$。当百分比变化很小时，这种近似是很精确的。

### 自相关

在时间序列数据中，某一时期的Y值总是和前一期值相关。时间序列变化和自身滞后值的相关性被称为自相关（autocorrelation）或序列相关（serial correlation）。一阶自相关（或自相关系数）是$$Y_t$$和$$Y_{t-1}$$的相关系数，即两个相邻时间点数值的相关系数。二阶自相关是$$Y_t$$和$$Y_{t-2}$$的相关系数，j阶自相关是$$Y_t$$和$$Y_{t-j}$$的相关系数。类似地，j阶自协方差（$$j^{th}$$ atcto coveriance）是$$Y_t$$与$$Y_{t-j}$$的协方差。

序列$$Y_t$$的j阶自协方差是$$Y_t$$和它的j阶滞后值$$Y_{t-j}$$的协方差，j阶自相关系数是$$Y_t$$和$$Y_{t-j}$$的相关系数。即

$$j阶自协方差 = Cov(Y_t, Y_{t-j})$$

$$j阶自相关 = \rau_j = corr(Y_t, Y_{t-j}) = \frac{Cov(Y_t, Y_{t-j})}{\sqrt{Var(Y_t)Var(Y_{t-j})}}$$

j阶自相关系数有时也被称为j阶序列相关系数

j阶总体自协方差及自相关系数能够用j阶样本自协方差和自相关系数，即，$$\hat{Cov(Y_t, Y_{t-j})}$$和$$\hat{\rau}_j$$，来估计：

$$\hat{Cov(Y_t, Y_{t-j})} = \frac{1}{T} \sum_{t=j+1}^T (Y_t - \bar{Y}_{j+1:T})(Y_{t-j}- \bar{Y}_{1:T-j})$$

$$\hat{\rau}_j = \frac{\hat{Cov(Y_t, Y_{t-j})}}{\hat{Var(Y_t)}}$$

其中，$$\bar{Y}_{j+1:T}$$表示$$Y_t$$在观测期$$t=j+1,..., T$$的样本均值，$$\hat{Var(Y_t)}$$为Y的样本方差。

## 自回归

自回归模型（autoregression）是一种将时间序列变量与自身的滞后值相联系的模型。

### 一阶自回归模型

如果你想要预测一个时间序列的未来值，则从上一期的滞后值考虑将会是一个不错的开始。

一阶自回归模型可缩写为AR(1)，其中1表示是一阶模型。序列$$Y_t$$的AR(1)总体模型是：

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + u_t$$

式中，$$u_t$$为误差项。

预测和预测误差。假设你有Y的历史数据，你想预测它的未来值。如果$$Y_t$$服从上式所描述的AR(1)模型，$$\beta_0$$和$$\beta_1$$已知，则基于$$Y_T$$对$$Y_{T+1}$$的预测值为$$\beta_0 +\beta_1 Y_T$$。

在实践中，$$\beta_0$$和$$\beta_1$$未知，从而预测必须以$$\beta_0$$和$$\beta_1$$的估计值为基础。我们将使用$$\beta_0$$和$$\beta_1$$的OLS估计值，而它们的估计值可通过使用历史数据得到。总的来说，我们使用$$\hat{Y}_{T+1 \mid T}$$表示基于截止到时间T的数据对T+1期数据的预测。相应地，使用上式所示的AR(1)模型做出的预测为

$$\hat{Y}_{T+1 \mid T} = \hat{\beta_0} + \hat{\beta_1} Y_T$$

式中，$$\hat{\beta}_0$$和$$\hat{\beta}_1$$表示使用截止到T期的历史数据得到的估计值。

预测误差（forecast error）是预测的错误程度，即实际发生的$$Y_{T+1}$$值与预测值$$\hat{Y}_{T+1 \mid T}$$的差：

$$预测误差 = Y_{T+1} - \hat{Y}_{T+1 \mid T}$$

预测值与预测误差。预测值不是OLS预期值，预测误差不是OLS残差。OLS预期值是利用估计模型所用到的样本内观测值计算出来的。相反地，预测值是针对样本外的某个时间段而言的，因此预测的被解释变量对应的真实值并不在估计模型所用到的样本中。同理，OLS残差为Y的实际值与它的预期值之差，而预测误差为Y的未来值（这个值不在估计模型所用到的样本中）与其预测值之差。换言之，预测值和预测误差属于“样本外”的观测，而预期值和残差属于“样本内”的观测。

根均方预测误差。根均方预测误差（root mean squared forecast error，RMSFE）是预测误差大小的测度方式，即使用预测模型做出预测的典型错误大小。RMSFE是预测误差平方均值的平方根

$$RMSFE = \sqrt{E[(Y_{T+1} - \hat{Y}_{T+1 \mid T})^2]}$$

根均方预测误差有两种误差来源：由于$$u_i$$的未来值未知，以及$$\beta_0$$和$$\beta_1$$的估计偏差。如果第一种错误比第二种错误大得多（当样本容量足够大时便会如此），则RMSFE近似为$$\sqrt{Var(u_t)}$$，即总体自回归模型中误差项的标准差。而$$u_t$$的标准差可用回归标准误差（SER）来估计。因此，如果系数估计的误差小到可以被忽略，则RMSFE能通过回归标准误差来估计。

### P阶自回归模型

AR(1)模型使用$$Y_{t-1}$$来预测$$Y_t$$，但这样做会忽略高阶滞后项中包含的有用信息。若想利用这些信息，一个方法是在AR(1)模型中加入额外的滞后项，即得到p阶自回归模型，记为AR(p)。

P阶自回归模型(AR(p)模型)把$$Y_t$$作为其p阶滞后值的线性函数，即在AR(p)模型中，解释变量为$$Y_{t-1}, Y_{t-2}, ..., Y_{t-p}$$和一个截距项。AR(p)模型中滞后项的个数p，被称为自回归模型的滞后阶数，或滞后长度。

AR(p)模型的预测和误差项的性质。模型假设给定$$Y_t$$的所有滞后值，$$u_t$$的条件期望为零，即$$E(u_t \mid Y_{t-1}, Y_{t-2},...) = 0$$。这一假设意味着以下两点：

第一，以这个序列的所有历史值对$$Y_{T+1}$$的最优预测只依赖于最近的p个滞后值。特别地，令$$Y_{T+1 \mid T} = E(Y_{T+1} \mid Y_{T}, Y_{T-1}, ...)$$表示在给定$$Y_t$$所有历史值的条件下$$Y_{T+1}$$的条件均值。那么，基于所有历史值，$$Y_{T+1 \mid T}$$是所有预测中RMSFE最小的预测值。如果$$Y_t$$服从AR(p)过程，则基于$$Y_T, Y_{T-1}, ...$$对$$Y_{T+1}$$的最佳预测是

$$Y_{T+1 \mid T} = \beta_0 + \beta_1 Y_t + \beta_2 Y_{t-1} + ... + \beta_p Y_{t-p+1}$$

这个模型服从AR(p)过程，同时假设$$E(u_t \mid Y_{t-1}, Y_{t-2}, ...) =0$$。在实践中，系数$$\beta_0, \beta_1, ..., \beta_p$$未知，因此在使用上式进行预测时，将使用各个系数的估计值。

第二，误差项$$u_t$$是序列不相关的。

## 包含额外预测变量的时间序列模型和自回归分布滞后模型

经济理论通常会提出对预测有帮助的其他变量。这些其他变量（或预测变量）能被加入自回归模型中以得到一个包含多个预测变量的时间序列回归模型。当自回归模型包含其他变量及它们的滞后值时，得到的便是自回归分布滞后模型。

### 自回归分布滞后模型

包含被解释变量$$Y_t$$的p阶滞后值与额外预测变量$$X_t$$的q阶滞后值的自回归分布滞后模型，记作ADL(p,q)，模型形式为：

$$Y_t = \beta_0 +\beta_1 Y_{t-1} + \beta_2 Y_{t-2} + ... + \beta_p Y_{t-p} + \sigma_1 X_{t-1} + \sigma_2 X_{t-2} + ... + \sigma_q X_{t-q} + u_t$$

其中，$$\beta_0, \beta_1, ..., \beta_p, \sigma_1, ..., \sigma_q$$为未知系数，$$u_t$$为误差项，同时假设$$E(u_t \mid Y_{t-1}, Y_{t-2}, ..., X_{t-1}, X_{t-2}, ...) =0$$

ADL模型假设给定Y和X的所有历史值，误差项的条件均值为零，这意味着误差项不包含Y和X的任何历史信息。换言之，本模型的滞后阶数p和q是真实的滞后阶数，即除了上式所示的滞后项外，模型不再包含X和Y的任何其他滞后项。

ADL模型包含了被解释变量的滞后项（自回归部分），以及单个额外预测变量X的滞后项。总体而言，利用多个预测变量可以改进预测结果。但在讨论包含多个预测变量的时间序列回归模型之前，我们首先介绍平稳性的概念。

### 平稳性

时间序列数据的回归分析需要利用历史数据来量化变量间的历史关系。如果未来和过去相似，则这些历史关系可以被用来预测未来。但如果未来和过去有根本性不同，则这些历史关系可能不再是预测未来的可靠依据。

在时间序列分析中，“历史关系可推广到未来”这一思想可由平稳性（stationarity）概念来总结，即时间序列变量的概率分布不随时间而改变。

如果一个时间序列$$Y_t$$的概率分布不随时间而改变，即对任意T值而言，$$(Y_{s+1}, Y_{s+2}, ..., Y_{s+T})$$的联合分布不依赖于s，则这个时间序列是平稳的，否则是非平稳的。如果对任何T来说，一对时间序列$$X_t$$和$$Y_t$$的联合分布$$(X_{s+1}, Y_{s+1}, X_{s+2}, Y_{s+2}, ..., X_{s+T}, Y_{s+T})$$都不依赖于s，则这对时间序列被称为联合平稳的。平稳性要求未来和过去相似，至少在概率分布要如此。

### 包含多个预测变量的时间序列回归

包含多个预测变量的时间序列模型拓展了ADL模型，使得ADL模型包含多个预测变量和它们的滞后值。因为存在多个预测变量及其滞后值，从而我们采用双下标来表示回归系数和解释变量。

假设时间序列回归模型包含k个额外预测变量，其中，第一个预测变量有$$q_1$$个滞后项，第二个预测变量有$$q_2$$个滞后项，以此类推。则这样的模型可表示为

$$Y_t = \beta_0 + \beta_1 Y_{t-1} + \beta_2 Y_{t-2} + ... + \beta_p Y_{t-p} + \sigma_{11} X_{1t-1} + ... + \sigma_{1q_1} X_{1t-q_1} + ... + \sigma_{k1} X_{kt} + \sigma_{k2} X_{kt-2} + \sigma_{kq_k} X_{kt-q_k} + u_t$$

其中：(1) $$E(u_t \mid Y_{t-1}, Y_{t-2}, ..., X_{1t-1}, X_{1t-2}, ..., X_{kt-1}, X_{kt-2}, ...) = 0$$

(2) 随机变量$$(Y_t, X_{1t}, ..., X_{kt})$$具有平稳分布，并且随着j变大时，$$(Y_t, X_{1t}, ..., X_{kt})$$和￥￥（Y_{t-j}, X_{1t-j}, ..., X_{kt-j}$$相互独立。

(3) 不存在大的异常值，即$$X_{1t}, ..., X_{kt}$$和$$Y_t$$具有非零、有限四阶矩。

(4) 不存在完全多重共线性。

时间序列回归模型的假设。上述假设是对针对横截面数据的多元回归模型的最小二乘假设的修正。

第一个假设是：给定所有解释变量及解释变量的所有滞后项，误差项的条件均值为零。这个假设拓展了AR模型及ADL模型的假设，且意味着利用Y和X的所有历史值对$$Y_t$$的最佳预测可由上式给出。

横截面数据的第二个最小二乘假设为$$(X_{1i}, X_{2i}, ..., X_{ki}, Y_i)$$为独立同分布的，而时间序列回归模型的第二个假设则根据时间序列数据特点对其进行了修正，可用两部分来表述：

第一，数据是从平稳分布中抽取的，从而今天的数据分布与过去的分布是相同的。显而易见，时间序列数据的这个假设相当于横截面数据中独立同分布假设中的“同分布”部分：时间序列变量（包括其滞后项）的联合分布不随时间而变化。在实践中，许多时间序列看上去是非平稳的，这意味着这个假设在实践中往往不成立。若时间序列变量是非平稳的，则时间序列回归会出现问题：预测可能是有偏误的，预测也可能不是有效的（基于同样的样本能得到MSFE更小的预测值），或者基于OLS的常规统计推断可能存在误导性。究竟哪个问题会出现，以及有什么应对方法，这些需要根据非平稳的原因来进行判断。经济时间序列中存在的两种重要类型的非平稳：趋势及突变，包括非平稳可能的带来的问题、检验方法及应对方法。但现在我们暂时假设时间序列是联合平稳的，从而关注平稳时间序列变量的回归。

第二，时间序列回归的第二个假设是另一部分是：随着时间间隔变大，随机变量呈现出相互独立的性质。这个假设代替了独立分布假设。这个假设有时也被称为弱相依（weak dependence），它保证了在大样本中，数据存在足够的随机性，从而大数定律和中心极限定理得以成立。

第三个假设与最小二乘假设一样，即不存在大的异常值。用数学语言精确表达为：所有变量具有非零、有限的四阶矩。

第四个假设也和横截面数据对应的假设相同，即回归变量之间不存在完全多重共线性。

统计推断及Granger因果关系检验。时间序列回归模型中的统计推断过程与横截面数据中的情形一样。

在时间序列预测中，F统计量有一个重要作用：它可以用来检验某个解释变量的滞后值是否提供了其他解释变量所不包含的有用的预测信息。如果一个变量没有预测作用，则这个变量的所有滞后项的系数均为零。检验这一原假设的F统计量被称为Granger因果统计量（Granger causality statistic），相对应的检验被称为Granger因果关系检验。