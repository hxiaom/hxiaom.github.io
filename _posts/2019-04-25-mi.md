---
layout: post
title: 【Method】互信息
categories: Analytics
---

## 原理

根据熵的连锁规则，有

$$H(X,Y) = H(X) + H(Y \mid X) = H(Y) + H(X \mid Y)$$

因此，

$$H(X) -H(X \mid Y) = H(Y) - H(Y \mid X)$$

这个差叫做X和Y的互信息（mutual information，MI）记作$$I(X;Y)$$。或者定义为：如果$$(X,Y) \sim p(x,y)$$，则X，Y之间的互信息$$I(X;Y) = H(X) -H(X \mid Y)$$。

$$I(X;Y)$$反映的是在知道了Y的值以后X的不确定性的减少量。可以理解为Y的值透露了多少关于X的信息量。

如果将定义中的H(X)和$$H(X \mid Y)$$展开，可得

$$\begin{align}
I(X;Y) &= H(X) -H(X \mid Y) \\
&= H(X) + H(Y) - H(X, Y) \\
&= \sum_x p(x) log \frac{1}{p(x)} + \sum_y p(y) log \frac{1}{p(y)} + \sum_{x,y} p(x,y) log p(x,y) \\
&= \sum_{x,y} p(x,y) log \frac{p(x,y)}{p(x)p(y)} \\
\end{align}$$ 

## 参考文献

- 统计自然语言处理 宗成庆 2.2.3