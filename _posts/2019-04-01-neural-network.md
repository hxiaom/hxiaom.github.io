---
layout: post
title: 【Method】深度学习（二）深度前馈网络
categories: Analytics
---

## 原理

深度前馈网络（deep feedforward network），也叫作前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron, MLP），是典型的深度学习模型。前馈网络的目标是近似某个函数$$f^*$$。例如，对于分类器，$$y=f^*(x)$$将输入x映射到一个类别y。前馈网络定义了一个映射$$y=f(x;\theta)$$，并且学习参数$$\theta$$的值，使它能够得到最佳的函数近似。

这种模型被称为前向（feedforward）的，是因为信息流过x的函数，流经用于定义f的中间计算过程，最终到达输出y。在模型的输出和模型本身之间没有反馈（feedback）连接。当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络（recurrent neural network）。

前馈神经网络被称作网络（network）是因为它们通常用许多不同函数复合在一起来表示。该模型与一个有向无环图相关联，而图描述了函数是如何复合在一起的。例如，我们有三个函数$$f^{(1)},f^{(2)},f^{(3)}$$连接在一个链上以形成$$f(x)=f^{(3)}(f^{(2)}(f^{(1)}(x)))$$。这些链式结构是神经网络中最常用的结构。在这种情况下，$$f^{(1)}$$被称为网络的第一层（first layer），$$f^{(2)}$$被称为第二层（second layer），以此类推。链的全长称为模型的深度（depth）。正是因为这个术语才出现了“深度学习”这个名字。前馈网络的最后一层被称为输出层（output layer）。在神经网络训练的过程中，我们让$$f(x)$$去匹配$$f^*(x)$$的值。训练数据为我们提供了在不同训练点上取值的、含有噪声的$$f^*(x)$$的近似实例。每个样本x都伴随着一个标签$$y \approx f^*(x)$$。训练样本直接指明了输出层在每一点x上必须做什么；它必须产生一个接近y的值。但是训练数据并没有直接指明其他层应该怎么做。学习算法必须决定如何使用这些层来产生想要的输出，但是训练数据并没有说每个单独的层应该做什么。相反，学习算法必须决定如何使用这些层来最好地实现$$f^*$$的近似。因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层（hidden layer）。

