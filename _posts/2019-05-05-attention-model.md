---
layout: post
title: 【Method】深度学习（三）Attention Model
categories: Analytics
---

## 原理

当人在看一样东西的时候，我们当前时刻关注的一定是我们当前正在看的这样东西的某一个地方，换句话说，当我们目光移向别处时，注意力随着目光的移动也在转移，这意味着，当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。

从Attention的作用角度出发，我们就可以从两个角度来分类Attention种类：Spatial Attention空间注意力（图片）和Temporal Attention时间注意力（序列）。更具实际的应用，也可以将Attention分为Soft Attention和Hard Attention。Soft Attention是所有的数据都会注意，都会计算出相应的注意力权值，不会设置筛选条件。Hard Attention会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。

### Encoder-Decoder框架

目前绝大多数文献中出行的AM（Attention Model）模式想附着在Encoder-Decoder框架下的，当然，其实AM模型可以看做一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，应用场景异常广泛。

Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<X,Y>。（思考：<X,Y>对很通用，X是一个问句，Y是答案；X是一个句子，Y是抽取的关系三元组；X是汉语句子，Y是汉语句子的英文翻译等），我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是统一语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成

$$X=<x_1, x_2, ...,x_m>$$

$$Y = <y_1, y_2,...,y_n>$$

Encoder顾名思义就是对句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C（C为一个张量或者其他表示）：

$$C=F(x_1, x_2, ...,x_m)$$

对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息$$y_1,y_2, ..., y_{i-1}$$来生成i时刻要生成的单词$$y_i$$：

$$y_i = G(C,y_1, y_2,...,y_{i-1})$$

每个$$y_i$$都依次这么产生，那么看起来就是真个系统根据输入句子X生成了目标句子Y。（其实这里的Encoder-Decoder是一个序列到序列的seq2seq模型，这个模型是对顺序有依赖的。）

Encoder-Decoder是个非常通用的计算框架。至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的如CNN/RNN/BiRNN/GRU/SLTM/Deep LSTM等，这里的变化组合非常多。

### Attention Model

以上介绍的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：

$$y_1 = f(C)$$

$$y_2 = f(C, y_1)$$

$$y_3 = f(C, y_1, y_2)$$

其中f是decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，是$$y_1, y_2$$也好，还是$$y_3$$也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder编码产生的，这意味着不论是生成哪个单词，$$y_1,y_2$$还是$$y_3$$，其实句子X中任意单词对生成某个目标单词$$y_i$$来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。

引入AM模型，以翻译一个英语句子举例：输入X：Tom chase Jerry。理性输出：汤姆追逐杰瑞。

应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值（权值）：

(Tom, 0.3), (Chase, 0.2), (Jerry, 0.5)

每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小（即原始单词X对当前词的翻译的权重大小）。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子的每个单词都应该学会其对应的源语句中单词的注意力分配概率信息。这意味着在生成单词$$y_i$$的时候，原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的$$C_i$$。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的$$C_i$$。

即生成目标句子单词的过程成了下面的形式：

$$y_1 = f(C_1)$$

$$y_2 = f(C_2, y_1)$$

$$y_3 = f(C_3, y_1, y_2)$$

每个$$C_i$$可能对应着不同的源语句单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：

$$C_{汤姆} = g(0.6 * f_2("Tom"), 0.2 * f_2("Chase"), 0.2 * f_2("Jerry"))$$

$$C_{追逐} = g(0.2 * f_2("Tom"), 0.7 * f_2("Chase"), 0.1 * f_2("Jerry"))$$

$$C_{杰瑞} = g(0.3 * f_2("Tom"), 0.2 * f_2("Chase"), 0.5 * f_2("Jerry"))$$

其中，$$f_2$$函数代表Encoder对输入英文单词的某种变换函数，比如如果Encodcer是用的RNN模型的话，这个$$f_2$$函数的结果往往是某个时刻输入$$x_i$$后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，也就是常常在论文里看到的下列公式：

$$c_i = \sum_{j=1}^{T_x} \alpha_{ij}h_j$$

$$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x} exp(e_{ik})}$$

$$e_{ij} = a(s_{i-1}, h_j)$$

$$a_{ij}$$是一个softmax模型输出，概率值的和为1。$$e_{ij}$$表示一个对齐模型，用于衡量encoder端的位置j个词，对于decoder端的位置i个词的对齐程度（影响程度）。

## 参考文献

- [Attention Model（注意力模型）](https://zhuanlan.zhihu.com/p/61816483)