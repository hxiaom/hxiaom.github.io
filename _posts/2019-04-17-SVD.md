---
layout: post
title: 【Method】奇异值分解(SVD)
categories: Analytics
---

## 原理

奇异值分解（Singular Value Decomposition，SVD）是机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。

### 特征值与特征向量回顾

我们首先回顾下特征值和特征向量的定义如下：

$$Ax = \lambda x$$

其中A是一个$$n \times n$$的实对称矩阵，x是一个n维向量，则我们说λ是矩阵A的一个特征值，而x是矩阵A的特征值λ所对应的特征向量。

求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值$$\lambda_1 \leq \lambda_2 \leq ... \leq lambda_n$$，以及这n个特征值所对应的特征向量$${w_1, w_2,...,w_n}$$，如果这n个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：

$$ A=W\sumW^{-1}$$

其中W就是这n个特征向量所张成的$$n \times n$$维矩阵，而$$\sum$$为这n个特征值为对角线的$$n \times n$$维矩阵。

一般我们会把W的这n个特征向量标准化，即满足$$\| w_i \|_2 = 1$$，或者说$$w_i^Tw_i=1$$，此时W的n个特征向量为标准正交基，满足$$WTW=I$$，即$$W^T=W^{-1}$$，也就是说W为酉矩阵。

这样我们的特征分解表达式可以写成

$$A=W\sum W^T$$

注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。


## 参考文献

- [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)