---
layout: post
title: 【Method】奇异值分解(SVD)
categories: Analytics
---

## 原理

奇异值分解（Singular Value Decomposition，SVD）是机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域。是很多机器学习算法的基石。

### 特征值与特征向量回顾

我们首先回顾下特征值和特征向量的定义如下：

$$Ax = \lambda x$$

其中A是一个$$n \times n$$的实对称矩阵，x是一个n维向量，则我们说λ是矩阵A的一个特征值，而x是矩阵A的特征值λ所对应的特征向量。

求出特征值和特征向量有什么好处呢？就是我们可以将矩阵A特征分解。如果矩阵A可对角化，并且我们求出了矩阵A的n个特征值$$\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$$，以及这n个特征值所对应的特征向量$${w_1, w_2,...,w_n}$$，如果这n个特征向量线性无关，那么矩阵A就可以用下式的特征分解表示：

$$ A=W\Sigma W^{-1}$$

其中W就是这n个特征向量所张成的$$n \times n$$维矩阵，而$$\Sigma$$为这n个特征值为对角线的$$n \times n$$维矩阵。

一般我们会把W的这n个特征向量标准化，即满足$$\| w_i \|_2 = 1$$，或者说$$w_i^Tw_i=1$$，此时W的n个特征向量为标准正交基，满足$$W^T W=I$$，即$$W^T=W^{-1}$$，也就是说W为酉矩阵。

这样我们的特征分解表达式可以写成

$$A=W\Sigma W^T$$

矩阵可以理解为对向量的拉伸，旋转，投影。而对于方阵而言，不存在维度升降的投影，因此：
- 特征值就是拉伸的大小
- 特征向量指明了拉伸的方向

给定任意的一个向量x，我们如何求Ax呢？ 很简单，把x沿着$$w_1, w_2,...$$分解，然后分别按照各自的比例$$\lambda_1, \lambda_2,...$$伸缩 最后再求和即可。

一定要这样分解吗？不能直接拉伸，旋转吗？当你运算$$A^n x$$的时候，我们就发现这里的这样分解的好处了。沿着各个P的伸缩量正好是$$\lambda^n$$。

所以，特征值在动态系统分析中是描述系统稳定性的非常重要的量，它决定了离散系统在空间内某个方向上的变化趋势（是无限扩张($$\lambda>1$$)？还是收缩($$\lambda<1$$)？还是保持不变($$\lambda=1$$)？），这是判断离散线性系统的重要特征。

特征值分解也就很好定义。 一个可对角化的方阵A：
分解为：$$A=W\Sigma W^{-1}$$ ，P的列向量为特征向量（$$W=[w_1,w_2,...]$$）。
理解为：以W为基的坐标分解变换+伸缩变换+以P为基坐标还原变换。

注意到要进行特征分解，矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。

### SVD 

而特征值分解其实是对旋转缩放两种效应的归并。（有投影效应的矩阵不是方阵，没有特征值）特征值，特征向量由Ax=x得到，它表示如果一个向量v处于A的特征向量方向，那么Av对v的线性变换作用只是一个缩放。也就是说，求特征向量和特征值的过程，我们找到了这样一组基，在这组基下，矩阵的作用效果仅仅是存粹的缩放。对于实对称矩阵，特征向量正交，我们可以将特征向量式子写成，这样就和奇异值分解类似了，就是A矩阵将一个向量从x这组基的空间旋转到x这组基的空间，并在每个方向进行了缩放，由于前后都是x，就是没有旋转或者理解为旋转了0度。


## 参考文献

- [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)
- [矩阵的奇异值与特征值有什么相似之处与区别之处？](https://www.zhihu.com/question/19666954/answer/54788626)
- [如何理解矩阵特征值和特征向量？](https://www.matongxue.com/madocs/228.html)
- [如何直观理解矩阵和线性代数？](https://www.zhihu.com/question/21082351/answer/19055262)