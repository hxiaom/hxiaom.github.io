---
layout: post
title: 【Method】卷积网络（四）基本卷积函数的变体
categories: Analytics
---

## 原理

当在神经网络的上下文中讨论卷积时，我们通常不是指数学文献中使用的那种标准的离散卷积运算。实际应用中的函数略有不同。这里我们详细讨论一下这些差异，并且对神经网络中用到的函数的一些重要性质进行重点说明。

首先，当提到神经网络中的卷积时，我们通常是指由多个并行卷积组成的运算。这是因为具有单个核的卷积只能提取一种类型的特征，尽管它作用在多个空间位置上。我们通常希望网络的每一层能够在多个位置提取多种类型的特征。

另外，输入通常也不仅仅是实值的网络，而是由一系列观测数据的向量构成的网络。例如，一幅彩色图像在每一个像素点都会有红、绿、蓝三种颜色的亮度。在多层的卷积网络中，第二层的输入是第一层的输出，通常在每个位置包含多个不同卷积的输出。当处理图像时，我们通常把卷积的输入输出都看作3维的张量，其中一个索引用于标明不同的通道（例如红、绿、蓝），另外两个索引标明在每个通道上的空间坐标。软件实现通常使用批处理模式，所以实际上会使用4维的张量，第4维索引用于标明批处理中不同的实例，但我们为简明起见这里忽略批处理索引。

因为卷积网络通常使用多通道的卷积，所以即使使用了核翻转，也不一定保证网络的线性运算是可交换的。只有当其中每个运算的输入和输出具有相同的通道数时，这些多通道运算才是可交换的。

假定我们有一个4维的核张量K，它的每一个元素是$$K_{i,j,k,l}$$，表示输出中处于通道i的一个单元和输入中处于通道j中的一个单元的连接强度，并且在输出单元和输入单元之间有k行l列的偏置。假定我们的输入又观测数据V组成，它的每一个元素是$$V_{i,j,k}$$，表示处在通道i中第j行第k列的值。假定我们的输出Z和输入V具有相同的形式，如果输出Z是通过对K和V进行卷积而不涉及翻转K得到的，那么

$$Z_{i,j,k} = \sum_{l,m,n} V_{l, j+m-1, k+n-1}K_{i,l,m,n}$$

这里对所有的l、m和n进行求和是对所有（在求和式中）有效的张量索引的值进行求和。在线性代数中，向量的索引通常从1开始，这就是上式中-1的由来。但是像C或Python这类编程语言索引通常从0开始，这使得上述公式可以更加简洁。

我们有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是提取特征没有先前那么好了）。我们可以把这一过程看作对全卷积函数输出的下采样（downsampling）。如果只想在输出的每个方向上每间隔s个像素进行采样，那么我们可以定义一个下采样卷积函数c使得
