---
layout: post
title: 【Method】回归分析基础（一）一元线性回归
categories: Analytics
---

本章将引入线性回归模型，用于建立一个变量X与另一个变量Y之间的某种联系。该模型假定X与Y之间具有线性关系，模型中的斜率表示X的一个单位变化对Y所产生的影响。正如Y的均值是Y总体分布的一个未知特征参数，联系X和Y的直线斜率也是X和Y联合分布的一个未知特征参数。计量经济学正是告诉我们如何利用这两个变量的样本数据来估计此斜率，即估计X的单位变化对Y产生的影响。

## 线性回归模型

假设现在有一个包含n个学区的样本，令$$Y_i$$表示第i个学区学生平均测试成绩，$$X_i$$表示第i个学区的平均班级规模，$$u_i$$表示影响第i个学区学生测试成绩的其他因素，则对每一学区（i=1,2,...,n），

$$Y_i = \beta_0 + \beta_1 X_i + \mu_i$$

上式便是一元线性回归模型（linear regression model with a single regressor），其中Y被称为被解释变量（dependent variable），X被称为解释变量（independent variable）或回归元（regressor）。

右侧第一部分$$\beta_0 + \beta_1 X_i$$被称为总体回归线（population regression line）或总体回归函数（population regression function），描述的是X和Y在总体平均水平上的关系。因此，如果已知X的值，根据总体回归线就可以预测被解释变量Y的值为$$\beta_0 + \beta_1 X$$。

截距（intercept）$$\beta_0$$和斜率（slope）$$\beta_1$$被称为总体回归线的系数（coefficients），也被称作回归线的参数（parameters）。斜率$$\beta_1$$表示X变化一个单位所引起的Y的变化，截距表示当X=0时总体回归线的取值，是总体回归线与Y轴的交点。在有些计量经济学应用中，截距具有明确的经济学含义；而在其他应用中，截距没有实际含义。

$$\mu_i$$被称作误差项（error term），误差包含了造成第i个学区学生平均测试成绩与总体回归线预测值之间差异的所有其他因素；对于某一特定的观测i，误差项包含了除X之外影响被解释变量Y取值的所有其他因素。在班级规模的例子中，这些其他因素包括第i个学区所特有的那些影响学生测试成绩的因素，如师资力量、学生家庭经济状况、运气，甚至包括评分过程中的任何失误。

## 线性回归模型的系数估计

在实际应用中，总体回归线的截距$$\beta_0$$和斜率$$\beta_1$$都是未知的，需要借助已知的样本数据来估计。

这一估计问题类似于你在统计学中遇到的其他估计问题。虽然我们不清楚关于X（班级规模）和Y（测试成绩）的总体回归的斜率$$\beta_{ClassSize}$$的取值，但正如可以通过从总体中抽取的样本数据来估计总体均值一样，我们在这里也可以利用样本数据来估计总体斜率$$\beta_{ClassSize}$$。

我们可以先通过散点图和相关系数来判断两个随机变量之间的关系。计算出相关系数后，我们希望能够画出穿过这些数据的一条直线，该直线的斜率就是基于这些数据得到的$$\beta_{ClassSize}$$的估计值。

### 普通最小二乘法

OLS估计量是选择能够使估计的回归线与观测数据尽可能接近的回归系数，这一接近程度是采用给定X时Y的预测误差平方和来度量。

正如之前所讨论的，样本均值$$\bar{Y}$$是总体均值E(Y)的最小二乘估计量，即在所有可能的估计量m中，$$\bar{Y}$$是使预测误差平方和$$\sum_{i=1}^n (Y_i -m)^2$$达到最小的估计量。

OLS估计量将这种思想推广到线性回归模型中。令$$b_0$$和$$b_1$$分别表示$$\beta_0$$和$$\beta_1$$的某个估计量，基于这些估计量的回归线为$$b_0 + b_1 X_i$$，则由这条直线得到$$Y_i$$的预测值为$$b_0 + b_1 X_i$$。从而第i个观测的预测误差为$$Y_i - (b_0 + b_1 X_i) = Y_i - b_0 - b_1 X_i$$。因此，所有n个观测的预测误差平方和为

$$\sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2$$

上式中线性回归模型的预测误差平方和是均值估计问题中的误差平方和的推广。事实上，如果没有解释变量$$X_i$$，则上式中便不会出现$$b_1$$，那么除了符号不同之外（上式中是$$b_0$$，均值估计中是$$m$$），这两个问题是完全相同的。正如存在唯一的估计量使均值估计问题的误差平方和达到最小，也应存在$$\beta_0$$和$$\beta_1$$的唯一一对估计量使上式达到最小。

上式中使预测误差平方和达到最小的截距和斜率的估计量被称为$$\beta_0$$和$$\beta_1$$的普通最小二乘（OLS）估计量（ordinary least squares (OLS) estimators）。

OLS有其特定的符号和术语，$$\beta_0$$的OLS估计量记作$$\hat{\beta}_0$$，$$\beta_1$$的OLS估计量记作$$\hat{\beta}_1$$。OLS回归线（OLS regression line）是由OLS估计量构造的直线$$\hat{\beta}_0 + \hat{\beta}_1 X$$，也被称为样本回归线（sample regression line）或样本回归方程（sample regression function）。给定$$X_i$$时，基于OLS回归线的$$Y_i$$预测值（predict value）为$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$。而第i个观测值对应的残差（residual）为$$Y_i$$与其预测值之差：$$\hat{\mu}_i = Y_i - \hat{Y}_i$$。

OLS估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$是总体系数$$\beta_0$$和$$\beta_1$$的样本对应，类似地，OLS回归线$$\hat{\beta}_0 + \hat{\beta}_1 X$$是总体回归线$$\beta_0 + \beta_1 X$$的样本对应，OLS残差$$\hat{\mu}_i$$是总体残差$$\mu_i$$的样本对应。

你可以通过反复尝试不同的$$b_0$$和$$b_1$$，直到你找到使上式最小的$$b_0$$和$$b_1$$，它们便是最小二乘估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$。但这种方法相当繁琐。幸运的是，我们可以利用求解上式最小化问题的微积分公式，从而简化OLS估计量的计算。

斜率$$\beta_1$$和截距$$\beta_0$$的OLS估计量分别为

$$\hat{\beta_1} = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = \frac{S_{XY}}{S_X^2}$$

$$\hat{\beta_0} = \bar{Y} - \hat{\beta}_1 \bar{X}$$

OLS预测值$$\hat{Y}_i$$和残差$$\hat{u}_i$$分别为

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i, i=1,2,...,n$$

$$\hat{u}_1 = Y_i - \hat{Y}_i, i =1,2,...,n$$

估计的截距($$\hat{\beta}_0$$)、斜率($$\hat{\beta}_1$$)及残差($$\hat{u}_i$$)是利用$$X_i$$和$$Y_i(i=1,2,...,n)$$的n组样本观测数据计算得到的，它们分别是未知总体截距($$\beta_0$$)、斜率($$\beta_1$$)和误差($$u_i$$)真值的估计结果。

### 为什么使用OLS估计量

为什么使用OLS估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$呢？这里存在两方面的原因：

第一，OLS是实践中最常用的方法，它已经成为经济、金融及社会科学回归分析中的“通用语言”。利用OLS（或它的变形）得出结论意味着你与其他经济学家和统计学家们使用了“同种语言”。

第二，OLS估计量具有理性的理论性质。这些性质与之前研究$$\bar{Y}$$作为总体均值估计量的理想性质类似。在满足前提假设的条件下，OLS估计量是无偏且一致的。另外，在一系列无偏估计量中，OLS估计量也是有效的，但这种有效性的结论仅在一些附加的特殊条件下才成立。

## 拟合优度

在估计完线性回归模型之后，你可能想知道该回归线拟合数据的效果如何。解释变量解释了大部分还是仅仅解释了一小部分被解释变量的变化呢？观测值是紧密地聚集在回归线周围还是很分散呢？

$$R^2$$和回归标准误差度量了OLS回归线拟合数据的效果。$$R^2$$的取值范围为0~1，它度量了能被$$X_i$$解释的$$Y_i$$方差的比例；回归标准误差度量了$$Y_i$$与其预测值偏差的大小。

### $$R^2$$

回归$$R^2$$（regression $$R^2$$）是指可由$$X_i$$解释（或预测）的$$Y_i$$样本方差的比例。由预测值和残差的定义，我们可以将被解释变量$$Y_i$$写成预测值$$\hat{Y}_i$$和残差$$\hat{u}_i$$之和：

$$Y_i = \hat{Y}_i + \hat{u}_i$$

于是，$$R^2$$即为$$\hat{Y}_i$$的样本方差与$$Y_i$$的样本方差之比。

$$R^2$$在数学上可表示为被解释平方和与总平方和之比，其中被解释平方和（explained sum of squares, ESS）为$$Y_i$$的预测值$$\hat{Y}_i$$与其均值离差的平方和，而总平方和（total sum of squares, TSS）为$$Y_i$$与其均值离差的平方和：

$$ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$$

$$TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2$$

注意，OLS预测值的样本均值$$\bar{\hat{Y}}$$等于$$\bar{Y}$$。

$$R^2$$为被解释平方和与总平方和之比：

$$R^2 = \frac{ESS}{TSS}$$

另一方面，$$R^2$$也可以用不能由$$X_i$$解释的$$Y_i$$方差的比例来表示。残差平方和（sum of squared redsidual, SSR）为OLS残差的平方和：

$$SSR = \sum_{i=1}^n \hat{u}_i^2$$

$$TSS=ESS+SSR$$。所以$$R^2$$也可以被表示为1减去残差平方和与总平方和之比：

$$R^2 = 1- \frac{SSR}{TSS}$$

最后，Y关于一元变量X回归的$$R^2$$也是Y和X相关系数的平方。

$$R^2$$的取值范围为0~1。如果$$\hat{\beta}_1=0$$，则表明$$X_i$$不能解释$$Y_i$$的任何变化，且$$Y_i$$的预测值为$$\hat{Y}_i = \hat{\beta}_0 = \bar{Y}$$。在这种情况下，被解释平方和为零，从而残差平方和等于总平方和，此时$$R^2$$为零。相反地，如果$$X_i$$解释了$$Y_i$$的所有变化，则对所有的i，有$$Y_i = \hat{Y}_i$$，且所有残差都为零（即$$\hat(u)_i=0$$），于是ESS=TSS且$$R^2=1$$。一般来说，$$R^2$$不取0或1这样的极端值，而是取两者之间的某个值。$$R^2$$接近1表明解释变量能够很好地预测$$Y_i$$，而$$R^2$$接近零表明解释变量不能较好地预测$$Y_i$$。

### 回归标准误差

回归标准误差（standard error of the regression，SER）是回归误差项$$u_i$$的标准差估计量。$$u_i$$与$$Y_i$$具有相同单位，所以SER采用被解释变量的单位来度量观测值在回归线附近的离散程度。

由于回归误差项$$u_1,...,u_n$$是不可观测的，所以我们利用样本中相应的OLS残差$$\hat{u}_1, ...,\hat{u}_n$$来计算SER。故SER的表达式为

$$SER = s_{\hat{u}} = \sqrt{s_{\hat{u}}^2}，其中s_{\hat{u}}^2 = \frac{1}{n-2} \sum_{i=1}^n \hat{u}_i^2 = \frac{SSR}{n-2}$$

其中，$$s_{\hat{u}}^2$$公式中用到了OLS残差的样本均值为零的结论。

这里使用除数n-2的原因：它修正了估计两个回归系数时产生的微小向下偏差。这种做法被称作”自由度修正“，因为我们要估计两个系数($$\beta_0和\beta_1$$)，数据损失了2个自由度，所以该式中的除数为n-2。但当n很大时，是除以n还是n-1或者除以n-2几乎没有什么差别。

回归$$R^2$$低（且SER大）的事实本身并不意味着该回归是”好“还是“坏”，但低的$$R^2$$确实能够告诉我们还存在其他更重要因素在影响着因变量。

## 最小二乘假设

本节给出了线性回归模型和抽样方案的三个假设，基于这些假设可以利用OLS方法得到未知回归系数$$\beta_0$$和$$\beta_1$$的合适估计量。这些假设乍听起来可能显得比较抽象，但它们有非常具体的含义，理解这些假设对于理解OLS方法在何时能够给出回归系数的有效估计是十分关键的。

### 假设1：给定$$X_i$$时，$$u_i$$的条件均值为零

第一个最小二乘假设（least squares assumptions）是给定$$X_i$$时，$$u_i$$的条件均值为零。这一假设是对$$u_i$$所包含的“其他因素”的规范数学表述。该假设意味着，在给定$$X_i$$取值时，其他因素的分布均值为零，表明这些因素与$$X_i$$不相关。

$$E(u_i \mid X_i)=0$$的假设等同于假定总体回归线是给定$$X_i$$时$$Y_i$$的条件均值。

随机对照实验中u的条件均值。在一个随机对照实验中，实验对象被随机分配到处理组（X=1）或者对照组（X=0）。我们通常利用与实验对象无关的计算机程序来实现随机分配，以确保X的分布对立于实验对象的所有个体特征。随机分配使X与u相互独立，这就意味着，在给定X时，u的条件均值为零。

在观测数据中，X不是随机分配的。但仍然希望X就像是随机分配的，准确点说，即$$E(u_i \mid X_i)=0$$。在实证应用中，这一假设是否成立还需要我们仔细考虑和判断，以后我们会反复重申这个问题。

相关系数和条件均值。如果给定一个随机变量时，另一个随机变量的条件均值为零，则这两个随机变量的协方差为零，即两个变量不相关。因此，条件均值假设$$E(u_i \mid X_i) = 0$$表明$$X_i$$与$$u_i$$不相关或$$corr(X_i, u_i)=0$$。因为相关系数度量的只是线性关系，所以上述结论反过来不成立；即使$$X_i$$和$$u_i$$不相关，给定$$X_i$$时$$u_i$$的条件均值也可能不为零。然而，如果$$X_i$$和$$u_i$$是相关的，则$$E(u_i \mid X_i)$$必定不为零。因此，利用$$X_i$$和$$u_i$$之间可能存在的相关关系来讨论条件均值假设往往更为方便。

### 假设2：对于$$i=1,...,n, (X_i, Y_i)$$独立同分布

第二个最小二乘假设是$$(X_i, Y_i), i=1,...,n$$的观测独立同分布。