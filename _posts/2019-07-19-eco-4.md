---
layout: post
title: 【Method】回归分析基础（一）一元线性回归
categories: Analytics
---

本章将引入线性回归模型，用于建立一个变量X与另一个变量Y之间的某种联系。该模型假定X与Y之间具有线性关系，模型中的斜率表示X的一个单位变化对Y所产生的影响。正如Y的均值是Y总体分布的一个未知特征参数，联系X和Y的直线斜率也是X和Y联合分布的一个未知特征参数。计量经济学正是告诉我们如何利用这两个变量的样本数据来估计此斜率，即估计X的单位变化对Y产生的影响。

## 线性回归模型

假设现在有一个包含n个学区的样本，令$$Y_i$$表示第i个学区学生平均测试成绩，$$X_i$$表示第i个学区的平均班级规模，$$u_i$$表示影响第i个学区学生测试成绩的其他因素，则对每一学区（i=1,2,...,n），

$$Y_i = \beta_0 + \beta_1 X_i + \mu_i$$

上式便是一元线性回归模型（linear regression model with a single regressor），其中Y被称为被解释变量（dependent variable），X被称为解释变量（independent variable）或回归元（regressor）。

右侧第一部分$$\beta_0 + \beta_1 X_i$$被称为总体回归线（population regression line）或总体回归函数（population regression function），描述的是X和Y在总体平均水平上的关系。因此，如果已知X的值，根据总体回归线就可以预测被解释变量Y的值为$$\beta_0 + \beta_1 X$$。

截距（intercept）$$\beta_0$$和斜率（slope）$$\beta_1$$被称为总体回归线的系数（coefficients），也被称作回归线的参数（parameters）。斜率$$\beta_1$$表示X变化一个单位所引起的Y的变化，截距表示当X=0时总体回归线的取值，是总体回归线与Y轴的交点。在有些计量经济学应用中，截距具有明确的经济学含义；而在其他应用中，截距没有实际含义。

$$\mu_i$$被称作误差项（error term），误差包含了造成第i个学区学生平均测试成绩与总体回归线预测值之间差异的所有其他因素；对于某一特定的观测i，误差项包含了除X之外影响被解释变量Y取值的所有其他因素。在班级规模的例子中，这些其他因素包括第i个学区所特有的那些影响学生测试成绩的因素，如师资力量、学生家庭经济状况、运气，甚至包括评分过程中的任何失误。

## 线性回归模型的系数估计

在实际应用中，总体回归线的截距$$\beta_0$$和斜率$$\beta_1$$都是未知的，需要借助已知的样本数据来估计。

这一估计问题类似于你在统计学中遇到的其他估计问题。虽然我们不清楚关于X（班级规模）和Y（测试成绩）的总体回归的斜率$$\beta_{ClassSize}$$的取值，但正如可以通过从总体中抽取的样本数据来估计总体均值一样，我们在这里也可以利用样本数据来估计总体斜率$$\beta_{ClassSize}$$。

我们可以先通过散点图和相关系数来判断两个随机变量之间的关系。计算出相关系数后，我们希望能够画出穿过这些数据的一条直线，该直线的斜率就是基于这些数据得到的$$\beta_{ClassSize}$$的估计值。

### 普通最小二乘法

OLS估计量是选择能够使估计的回归线与观测数据尽可能接近的回归系数，这一接近程度是采用给定X时Y的预测误差平方和来度量。

正如之前所讨论的，样本均值$$\bar{Y}$$是总体均值E(Y)的最小二乘估计量，即在所有可能的估计量m中，$$\bar{Y}$$是使预测误差平方和$$\sum_{i=1}^n (Y_i -m)^2$$达到最小的估计量。

OLS估计量将这种思想推广到线性回归模型中。令$$b_0$$和$$b_1$$分别表示$$\beta_0$$和$$\beta_1$$的某个估计量，基于这些估计量的回归线为$$b_0 + b_1 X_i$$，则由这条直线得到$$Y_i$$的预测值为$$b_0 + b_1 X_i$$。从而第i个观测的预测误差为$$Y_i - (b_0 + b_1 X_i) = Y_i - b_0 - b_1 X_i$$。因此，所有n个观测的预测误差平方和为

$$\sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2$$

上式中线性回归模型的预测误差平方和是均值估计问题中的误差平方和的推广。事实上，如果没有解释变量$$X_i$$，则上式中便不会出现$$b_1$$，那么除了符号不同之外（上式中是$$b_0$$，均值估计中是$$m$$），这两个问题是完全相同的。正如存在唯一的估计量使均值估计问题的误差平方和达到最小，也应存在$$\beta_0$$和$$\beta_1$$的唯一一对估计量使上式达到最小。

上式中使预测误差平方和达到最小的截距和斜率的估计量被称为$$\beta_0$$和$$\beta_1$$的普通最小二乘（OLS）估计量（ordinary least squares (OLS) estimators）。

OLS有其特定的符号和术语，$$\beta_0$$的OLS估计量记作$$\hat{\beta}_0$$，$$\beta_1$$的OLS估计量记作$$\hat{\beta}_1$$。OLS回归线（OLS regression line）是由OLS估计量构造的直线$$\hat{\beta}_0 + \hat{\beta}_1 X$$，也被称为样本回归线（sample regression line）或样本回归方程（sample regression function）。给定$$X_i$$时，基于OLS回归线的$$Y_i$$预测值（predict value）为$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$$。而第i个观测值对应的残差（residual）为$$Y_i$$与其预测值之差：$$\hat{\mu}_i = Y_i - \hat{Y}_i$$。

OLS估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$是总体系数$$\beta_0$$和$$\beta_1$$的样本对应，类似地，OLS回归线$$\hat{\beta}_0 + \hat{\beta}_1 X$$是总体回归线$$\beta_0 + \beta_1 X$$的样本对应，OLS残差$$\hat{\mu}_i$$是总体残差$$\mu_i$$的样本对应。

你可以通过反复尝试不同的$$b_0$$和$$b_1$$，直到你找到使上式最小的$$b_0$$和$$b_1$$，它们便是最小二乘估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$。但这种方法相当繁琐。幸运的是，我们可以利用求解上式最小化问题的微积分公式，从而简化OLS估计量的计算。

斜率$$\beta_1$$和截距$$\beta_0$$的OLS估计量分别为

$$\hat{\beta_1} = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2} = \frac{S_{XY}}{S_X^2}$$

$$\hat{\beta_0} = \bar{Y} - \hat{\beta}_1 \bar{X}$$

OLS预测值$$\hat{Y}_i$$和残差$$\hat{u}_i$$分别为

$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i, i=1,2,...,n$$

$$\hat{u}_1 = Y_i - \hat{Y}_i, i =1,2,...,n$$

估计的截距($$\hat{\beta}_0$$)、斜率($$\hat{\beta}_1$$)及残差($$\hat{u}_i$$)是利用$$X_i$$和$$Y_i(i=1,2,...,n)$$的n组样本观测数据计算得到的，它们分别是未知总体截距($$\beta_0$$)、斜率($$\beta_1$$)和误差($$u_i$$)真值的估计结果。

### 为什么使用OLS估计量

为什么使用OLS估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$呢？这里存在两方面的原因：

第一，OLS是实践中最常用的方法，它已经成为经济、金融及社会科学回归分析中的“通用语言”。利用OLS（或它的变形）得出结论意味着你与其他经济学家和统计学家们使用了“同种语言”。

第二，OLS估计量具有理性的理论性质。这些性质与之前研究$$\bar{Y}$$作为总体均值估计量的理想性质类似。在满足前提假设的条件下，OLS估计量是无偏且一致的。另外，在一系列无偏估计量中，OLS估计量也是有效的，但这种有效性的结论仅在一些附加的特殊条件下才成立。
