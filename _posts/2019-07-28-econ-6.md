---
layout: post
title: 【Method】回归分析基础（三）多元线性回归
categories: Analytics
---

本章将解释遗漏变量偏差并引入多元回归模型，以消除遗漏变量偏差。多元回归的关键思想是，如果我们能得到遗漏变量的数据，就可以将这些变量作为解释变量加入模型中，以此来保证在其他变量不变的条件下，估计某个解释变量的影响。

## 遗漏变量偏差

### 遗漏变量偏差的定义

在回归中，如果某个被遗漏的变量（如英语学习者比例）是被解释变量（测试成绩）的决定因素之一，并与解释变量（学生-教师比）相关，则此时OLS估计会产生遗漏变量偏差（omitted variable bias）。

以下两个条件同时成立时，会产生遗漏变量偏差：(1) 遗漏变量与已包含的解释变量相关；(2) 遗漏变量是被解释变量的决定因素之一。

遗漏变量偏差和第一个最小二乘假设。遗漏变量偏差意味着第一个最小二乘假设-$$E(u_i \mid X_i) = 0$$出现错误。我们已经知悉，在一元线性回归模型中误差项$$u_i$$代表了除$$X_i$$外$$Y_i$$的所有其他决定因素。如果其中一个因素与$$X_i$$相关，这意味着误差项（包含这个因素）和$$X_i$$相关。换言之，如果遗漏变量是$$Y_i$$的决定因素，它被包含在误差项中，如果它又与$$X_i$$相关，则误差项也与$$X_i$$相关。因为$$u_i$$与$$X_i$$相关，在给定$$X_i$$时，$$u_i$$的条件均值不再为零。因此，这种相关性违背了第一条最小二乘假设，而且后果严重：OLS估计量是有偏的。即使在大样本情形下，这种偏差也不会消失，即OLS估计量是不一致的。

### 遗漏变量偏差的公式

上一部分关于遗漏变量偏差的讨论客运用数学公式来表达，即$$u_i$$和$$X_i$$的相关系数为$$corr(X_i, u_i) = \rho_{X_u}$$。假设第二和第三条最小二乘假设成立，而由于$$\rho_{X_u}$$不为零，于是第一条假设不成立。那么OLS估计量的极限为

$$\hat{\beta}_1 \to \beta_1 + \rho_{X_u} \frac{\sigma_u}{\sigma_X}$$

即随着样本量增大，$$\hat{\beta}_1$$接近$$\beta_1 + \rho_{X_u} \frac{\sigma_u}{\sigma_X}$$的概率越来越大。

上式概括了上文中关于遗漏变量偏差讨论的一些思想:

(1) 无论样本量大小，遗漏变量偏差都是一个问题。由于$$\hat{\beta}_1$$不依赖概率收敛到其真实值$$\beta_1$$，$$\hat{\beta}_1$$是有偏的且不一致的；也就是说，当遗漏变量存在时，$$\hat{\beta}_1$$不是$$\beta_1$$的一致估计量。$$\rho_{X_u} (\sigma_u / \sigma_X)$$项是$$\hat{\beta}_1$$在大样本中仍然存在的偏差。

(2) 偏差的大小取决于解释变量和误差项的相关系数$$\rho_{X_u}$$。$$\mid \rho_{X_u}$$越大，偏差越大。

(3) $$\hat{\beta}_1$$的偏差方向取决于X和u是正相关还是负相关。例如，我们猜测仍然在学习英语的学生比例对学区

### 通过数据分组处理遗漏变量偏差

应该如何处理遗漏变量偏差？可以通过数据分组观察遗漏变量带来的偏差。

更有效地方法是，使用多元回归的方法可以提供这种估计。

## 多元回归模型

多元回归模型(multiple regression model)加入了其他变量作为解释变量，扩展了一元回归模型。该模型能够在其他解释变量$$(X_{2i}, X_{3i}, ...)$$不变的情况下，估计变量$$X_{1i}$$的变化对$$Y_i$$的影响。

### 总体回归线

假设现在只有两个解释变量$$X_{1i}$$和$$X_{2i}$$。在多元线性回归模型中，这两个解释变量和被解释变量Y之间的均值关系可用如下线性方程表示：

$$E(Y_i \mid X_{1i} = x_1, X_{2i} = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$

其中，$$E(Y_i \mid X_{1i} = x_1, X_{2i} = x_2)$$是给定$$X_{1i} = x_1$$和$$X_{2i} = x_2$$时，$$Y_i$$的条件期望。

上式是多元回归模型的总体回归线（population regression line）或总体回归方程（population regression function）。系数$$\beta_0$$是截距（intercept）；系数$$\beta_1$$是$$X_{1i}$$的斜率系数，或者简称$$X_{1i}$$的稀疏；系数$$\beta_2$$是$$X_{2i}$$的斜率系数，或者简称$$X_{2i}$$的稀疏；有时也称多元回归模型中的一个或多个解释变量为控制变量。

上式中系数$$\beta_1$$的解释不同于当$$X_{1i}$$是唯一解释变量时的情况。在上式中，$$\beta_1$$是在保持$$X_2$$不变（或控制$$X_2$$）的条件下，$$X_1$$的1个单位的变动对Y的影响。

在$$X_2$$保持不变的情况下，$$X_1$$的变化($$\delta X_1$$)对Y的期望效益，是解释变量为$$X_1 + \delta X_1$$和$$X_2$$时Y的期望剪去解释变量为$$X_1$$和$$X_2$$时Y的期望之差。相应地，把上式中的总体回归方程写成$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$，假设$$X_2$$不变（即控制$$X_2$$为常数），$$X_1$$改变$$\delta X_1$$。由于$$X_1$$发生了变动，$$Y$$也会变动一定数量，设为$$\delta Y$$。变化之后的新Y值$$(Y+\delta Y)$$为

$$Y + \delta Y = \beta_0 + \beta_1 (X_1 + \delta X_1) + \beta_2 X_2$$

上式减去等式$$Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2$$，便得到用$$\delta X_1$$表示的$$\delta Y$$表达式，即$$\delta Y = \beta_1 \delta X_1$$，对该方程变形得

$$\beta_1 = \frac{\delta Y}{\delta X_1}, 保持X_2不变$$

系数$$\beta_1$$是保持$$X_2$$固定不变时，1个单位$$X_1$$的变化对Y的影响（Y的期望的变化）。对$$\beta_1$$的刻画，也可以用另一种表述，即在$$X_2$$固定不变时，$$X_1$$对Y的偏效应（partial effect）。

对多元回归模型的截距$$\beta_0$$的解释与一元回归模型中类似：它是当$$X_{1i}$$和$$X_{2i}$$都为零时$$Y_i$$的期望值。简而言之，截距$$\beta_0$$决定了总体回归线与Y轴交点距离远点有多远。

### 总体回归模型

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,...,n$$

上式表示的是有两个解释变量的总体多元回归模型（population multiple regression model）。

在双变量回归中，可以认为$$\beta_0$$是恒等于1的解释变量的系数，即考虑$$\beta_0$$是$$X_{oi}$$的系数，对于$$i=1,...,n, X_{0i}=1$$。据此，上式的总体多元回归模型也可以写成

$$当$$X_{0i}=1, Y_i = \beta_0 X_{0i} + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,...,n$$

由于$$X_{0i}$$对于所有的观测值皆取值为1，它有时被称为常数回归变量（constant regressor）。同样地，$$\beta_0$$有时被称为回归中的常数项（constant term）。

由此，我们可以拓展至更一般的k变量模型。

$$Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + u_i, i=1,...,n$$

多元回归模型中的同方差和异方差定义是其在一元模型中的拓展形式。多元回归模型中，给定$$X_{1i}, ..., X_{ki}$$时，如果对于$$i=1,...,n$$，误差项$$u_i$$的条件分布方差$$Var(u_i \mid X_{1i}, ..., X_{ki})$$是常数，即不依赖$$X_{1i}, ..., X_{ki}$$的取值，则$$u_i$$是同方差的。否则，误差项是异方差的。

## 多元回归的OLS估计量

### OLS估计量

我们同样可以使用OLS方法来估计多元回归模型中的系数$$\beta_0, \beta_1, ..., \beta_k$$。设$$\beta_0, \beta_1, ..., \beta_K$$的估计量分别为$$b_0, b_1, ..., b_k$$，我们利用这些估计量计算得到$$Y_i$$的预测值为$$b_0 + b_1 X_{1i} + ... + b_k X_{ki}$$，同时$$Y_i$$的预测误差为$$Y_i - (b_0 + b_1 X_{1i} + ... + b_k X_{ki}) = Y_i - b_0 - b_1 X_{1i} - ... - b_k X_{ki}$$。从而我们得到n组观测的预测误差平方和为

$$\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - ... - b_k X_{ki})^2$$

使上式预测误差平方和达到最小的系数$$\beta_0, \beta_1, ..., \beta_k$$的估计量，被称为$$\beta_0, \beta_1, ..., \beta_k$$的普通最小二乘（OLS）估计量，分别记为$$\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_k$$。

多元线性回归模型中的OLS术语与一元线性回归模型基本一致。利用OLS估计量构造的直线$$\hat{\beta}_0 + \hat{\beta}_1 X_1 + ... + \hat{\beta}_k X_k$$，被称为OLS回归线。当$$X_{1i}, ..., X_{ki}$$给定时，基于OLS回归线便可以得到$$Y_i$$的预测值$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + ... + \hat{\beta}_k X_{ki}$$。对于第i组观测值，其OLS残差$$\hat{u}_i = Y_i - \hat{Y}_i$$。

我们可以利用试错法来计算OLS估计量，即通过尝试不同的$$b_0, ..., b_k$$取值直至残差平方和达到最小为止。除此之外，还可以利用微积分来推导OLS估计量，而且这种方法会简单很多。

在多元回归模型中，我们最好利用矩阵来表示和讨论这些公式。


## 多元回归的拟合优度

多元回归模型中常用的三个概括性统计量分别为回归标准误、回归$$R^2$$以及调整$$R^2$$（或记作$$\bar{R}^2$$）。这三个统计量皆度量了OLS估计得到的多元回归线对数据的描述或“拟合”效果。

### 回归标准误

回归标准误（SER）是对误差项$$u_i$$标准差的估计，故SER可用来度量Y的分布在回归线周围的离散程度。在多元回归中，SER表示为

$$SER = S_{\hat{u}} = \sqrt{S_{\hat{u}}^2}，其中S_{\hat{u}}^2= \frac{1}{n-k-1}\sum_{i=1}^n \hat{u}_i^2 = \frac{SSR}{n-k-1}$$

式中，SSR为残差平方和，即$$SSR= \sum_{i=1}^n \hat{u}_i^2$$

上式与一元回归模型SER定义的唯一区别在于，这里的除数是n-k-1而不是n-2。我们使用除数n-1（而不是n）调整了由估计两个系数（回归线的斜率和截距）所引起的向下偏差；这里，除数n-k-1调整了由估计k+1个系数（k个斜率系数和截距）所引起的向下偏差。在这里使用n-k-1而不是n的做法，我们称之为自由度调整。如果仅有一个解释变量，则k=1，因此与一元回归模型中的公式一样。当n较大时，自由度调整的效果则可忽略不计。

### $$R^2$$

回归$$R^2$$是由解释变量解释（或预测）的$$Y_i$$样本方差的比例。同等地，$$R^2$$也可表示为1减去不能由解释变量解释的$$Y_i$$方差的比例。

$$R^2$$的数学定义同一回归模型一样，即

$$R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}$$

其中，能够被解释的平方和为$$ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$$，总平方和为$$TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2$$。

在多元回归中，除非增加的解释变量的系数估计值恰好为零，否则只要增加解释变量，$$R^2$$就会增大。为了便于理解这一点，考虑两个解释变量的情形。利用OLS去估计包含两个解释变量的模型时，我们会得到使残差平方和达到最小的系数取值，如果第二个解释变量系数的OLS估计值恰好为零，则不论该变量是否加入回归模型中，SSR都不变；但如果该变量系数的OLS估计值不为零，则相对于排除该变量的模型，其SSR必定会降低。在实际中，系数估计值恰好为零的情形非常少，所以一般情况下，当模型中加入一个新的解释变量时，其SSR将降低。这就意味着增加新的解释变量后的$$R^2$$通常会增大（不可能会减小）。

### 调整$$R^2$$

由于增加新变量后的$$R^2$$会增大，所以$$R^2$$增大并不意味着加入新变量就可以提高模型的解释力或拟合效果。从这个意义上说，$$R^2$$会高估回归数据的拟合效果。一种解决办法是利用某个因子来缩小或降低$$R^2$$，而这正是调整$$R^2$$或$$\bar{R}^2$$可以做到的。

调整$$R^2$$或$$\bar{R}^2$$是$$R^2$$的一种修正形式，增加新的解释变量后$$\bar{R}^2$$不一定增大，其表达式为

$$\bar{R}^2 = 1- \frac{n-1}{n-k-1}\frac{SSR}{TSS} = 1 - \frac{S_{\hat{u}}^2}{S_Y^2}$$

与$$R^2$$相比，上式的区别在于：残差平方和与总平方和的比值与因子$$(n-1) / (n-k-1)$$相乘。

关于$$\bar{R}^2$$，需要注意以下三点：

第一，$$(n-1) / (n-k-1)$$必定大于1，故$$\bar{R}^2$$必定小于$$R^2$$。

第二，增加一个解释变量对$$\bar{R}^2$$会产生两种反向作用。一方面，SSR降低会使$$\bar{R}^2$$增大；另一方面，由于k的增加，因子$$(n-1)/(n-k-1)$$也会增大，所以$$\bar{R}^2$$究竟是增大还是减小取决于这两种作用的大小。

第三，$$\bar{R}^2$$可能为负。当所有解释变量减少的残差平方和太小，以至于无法抵消因子$$(n-1)/(n-k-1)$$的增加时，这种情况便会发生。

$$R^2$$和调整$$R^2$$的应用。由于$$\bar{R}^2$$定量描述了被解释变量的变化能够由解释变量解释的程度，故其应用很广泛。但过分依赖$$\bar{R}^2$$(或者$$R^2$$)会让我们掉入陷阱里。在实际应用中，“最大化$$\bar{R}^2$$”几乎没有任何经济或统计上的意义。相反地，在多元回归中是否应该加入一个变量，取决于加入该变量后我们是否可以更好地估计感兴趣的因果关系。我们之后将讨论如何判断哪些变量应该被加入或哪些变量应该被排除的问题。

## 多元回归模型的最小二乘假设

多元回归模型有四个最小二乘假设，其中，前三个是一元回归模型的最小二乘假设推广到多元回归模型的情形；第四个假设是新引入的，我们将详细介绍。

### 假设1：给定$$X_{1i}, X_{2i}, ..., X_{ki}$$时，$$u_i$$的条件均值为零

第一个假设是：给定$$X_{1i}, X_{2i}, ..., X_{ki}$$时，$$u_i$$的条件均值为零。这是将一元回归模型的第一个最小二乘假设推广到多元回归模型的情形。该假设表明，虽然$$Y_i$$有时位于总体回归线的上方，有时位于总体回归线的下方，但就总体平均水平而言，$$Y_i$$落在总体回归线上。因此，对于解释变量的任意取值，$$u_i$$的期望值都为零。如同一元回归模型的情形，这是使OLS估计量无偏的关键假设。

### 假设2：$$(X_{1i}, X_{2i}, ..., X_{ki}, Y_i), i=1,...,n$$独立同分布

第二个假设是：$$(X_{1i}, X_{2i}, ..., X_{ki}, Y_i), i=1,...,n$$为独立同分布(i.i.d.)的随机变量。如果数据来自简单随机抽样，则该假设自然成立。

### 假设3：不存在大的异常值

第三个最小二乘假设是：不存在大的异常值。所谓的大异常值，是指那些远超数据通常取值范围的观测值。该假设提醒我们，如同一元回归模型的情形，多元回归模型的OLS估计量对大的异常值非常敏感。

这一假设可以用数学语言精确表述为$$X_{1i}, X_{2i}, ..., X_{ki}$$和$$Y_i$$的四阶矩非零且有限：$$0<E(X_{1i}^4)<\infty,...,0<E(X_{ki}^4)<\infty 且0<E(X_{Y}^4)<\infty$$。另一种表达方式是，被解释变量和解释变量具有有限的峰度。该假设可以用于推导OLS估计量的大样本性质。

### 假设4：不存在完全多重共线性

第四个假设是多元回归模型新引入的假设，它排除了完全多重共线性的情形。若存在完全多重共线性，我们将无法计算OLS估计量。如果一个解释变量是其他解释变量的完全线性函数，则称这些解释变量之间存在完全多重共线性（Perfect Multicollinearity）。

直觉上看，我们试图用回归模型去解决这个不合逻辑的问题，才造成了完全多重共线性。在多元回归中，某个解释变量的系数是在控制其他解释变量不变时，这一解释变量的变化所产生的影响。当控制其他变量中，包括这个解释变量时，这显然是没有意义的，OLS不能估计这种无意义的偏效应。

完全多重共线性的产生往往反映了选取变量时的逻辑错误，或者原先没有察觉到的某些数据集特征。一般通过修正解释变量的方法来消除完全多重共线性问题。

## 多元回归模型中OLS估计量的分布

在最小二乘假设下，多元线性回归模型中OLS估计量$$\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_k$$是$$\beta_0, \beta_1, ..., \beta_k$$的无偏且一致估计量。在大样本情况下，$$\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_k$$的联合抽样分布近似于多维正态分布，且每个$$\hat{\beta}_j ~ N(\beta_j, \sigma_{\hat{\beta}_j}^2), j=0,...,k$$。

一般而言，解释变量之间存在相关性，因而OLS估计量也是相关的。

## 多重共线性

当其中某个解释变量是其他解释变量的完全线性组合时，模型存在完全多重共线性。本节将列举几个完全多重共线性的例子，并讨论当回归模型中包含多个二元变量时完全多重共线性如何产生及如何避免等问题。当某个解释变量与其他解释变量高度相关但不完全相关时，模型存在不完全多重共线性。与完全多重共线性不同，不完全多重共线性既不会妨碍对回归模型进行估计，也不意味着变量选取中存在逻辑问题，但却会导致无法得到一个或多个回归系数的精确估计。

### 完全多重共线性实例

当回归模型包含截距时，常数解释变量$$X_{oi}$$可能是导致完全多重共线性的原因；另外，完全多重共线性是针对现有的数据而言的。

虚拟变量陷阱。产生完全多重线性的另一个可能的原因是解释变量中包括多个二元变量（或虚拟变量）。

一般而言，如果有G个二元变量，每个观测值只属于其中一类，若回归模型有截距项且包括全部的G个二元变量，则回归模型将由于存在完全多重共线性而无法估计。这种情形称为虚拟变量陷阱（dummy variable trap）。避免虚拟变量陷阱的通常做法是，去掉其中一个二元变量，即只将G个二元变量中的G-1个作为解释变量。此时，某个包含的二元变量的系数表示在控制其他解释变量不变的条件下，该类个体相对于作为基准情形而被去掉的那类个体的增量效应。此外，若去掉截距项，则回归模型可包括所有的G个二元变量。

完全多重共线性的解决方法。当这种错误发生时，计算机软件会提示错误，需要修正回归模型以消除该错误。

### 不完全多重共线性

不完全多重共线性是指两个或两个以上的解释变量高度相关，即某些解释变量的线性组合与另一个解释变量高度相关。不完全多重线性对OLS估计量的理论不构成任何问题；事实上，OLS的目的之一是当解释变量之间存在潜在的相关性时，分离出不同解释变量的独立影响。

若解释变量之间存在不完全多重共线性，那么至少有一个解释变量的系数的估计量是不准确的。例如，考虑TestScore对STR和PctEL的回归，假设我们想加入第三个解释变量——本学区居民中第一代移民的比例。第一代移民通常将英语作为第二语言，所以变量PctEL与第一代移民比例高度相关。因为这两个变量高度相关，所以利用现有数据，很难在控制第一代移民比例不变的条件下，估计PctEL增加对测试成绩的偏效应。换言之，该数据集不能提供英语学习者比例较低而第一代移民比例较高时测试成绩的信息，反之亦然。如果最小二乘假设成立，那么该回归中PctEL系数的OLS估计是无偏的；然而，与不存在相关性的情形相比，此时PctEL系数估计量的方差会变大。

当多元回归模型中解释变量之间存在不完全多重共线性时，由于样本方差很大，其中一个或多个解释变量系数的估计结果是不准确的。

回归模型中存在完全多重共线性通常意味着存在逻辑错误，但不完全多重共线性未必是错误的，而只是反映了OLS、你的数据和试图解决的问题的某种特征。如果你选取某些变量加入回归模型中以处理可能存在的遗漏变量偏差，那么不完全多重共线性的存在意味着利用现有数据难以精确估计其中一个或多个变量的偏效应。