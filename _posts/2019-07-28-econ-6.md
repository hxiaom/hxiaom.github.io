---
layout: post
title: 【Method】回归分析基础（三）多元线性回归
categories: Analytics
---

本章将解释遗漏变量偏差并引入多元回归模型，以消除遗漏变量偏差。多元回归的关键思想是，如果我们能得到遗漏变量的数据，就可以将这些变量作为解释变量加入模型中，以此来保证在其他变量不变的条件下，估计某个解释变量的影响。

## 遗漏变量偏差

### 遗漏变量偏差的定义

在回归中，如果某个被遗漏的变量（如英语学习者比例）是被解释变量（测试成绩）的决定因素之一，并与解释变量（学生-教师比）相关，则此时OLS估计会产生遗漏变量偏差（omitted variable bias）。

以下两个条件同时成立时，会产生遗漏变量偏差：(1) 遗漏变量与已包含的解释变量相关；(2) 遗漏变量是被解释变量的决定因素之一。

遗漏变量偏差和第一个最小二乘假设。遗漏变量偏差意味着第一个最小二乘假设-$$E(u_i \mid X_i) = 0$$出现错误。我们已经知悉，在一元线性回归模型中误差项$$u_i$$代表了除$$X_i$$外$$Y_i$$的所有其他决定因素。如果其中一个因素与$$X_i$$相关，这意味着误差项（包含这个因素）和$$X_i$$相关。换言之，如果遗漏变量是$$Y_i$$的决定因素，它被包含在误差项中，如果它又与$$X_i$$相关，则误差项也与$$X_i$$相关。因为$$u_i$$与$$X_i$$相关，在给定$$X_i$$时，$$u_i$$的条件均值不再为零。因此，这种相关性违背了第一条最小二乘假设，而且后果严重：OLS估计量是有偏的。即使在大样本情形下，这种偏差也不会消失，即OLS估计量是不一致的。

### 遗漏变量偏差的公式

上一部分关于遗漏变量偏差的讨论客运用数学公式来表达，即$$u_i$$和$$X_i$$的相关系数为$$corr(X_i, u_i) = \rho_{X_u}$$。假设第二和第三条最小二乘假设成立，而由于$$\rho_{X_u}$$不为零，于是第一条假设不成立。那么OLS估计量的极限为

$$\hat{\beta}_1 \to \beta_1 + \rho_{X_u} \frac{\sigma_u}{\sigma_X}$$

即随着样本量增大，$$\hat{\beta}_1$$接近$$\beta_1 + \rho_{X_u} \frac{\sigma_u}{\sigma_X}$$的概率越来越大。

上式概括了上文中关于遗漏变量偏差讨论的一些思想:

(1) 无论样本量大小，遗漏变量偏差都是一个问题。由于$$\hat{\beta}_1$$不依赖概率收敛到其真实值$$\beta_1$$，$$\hat{\beta}_1$$是有偏的且不一致的；也就是说，当遗漏变量存在时，$$\hat{\beta}_1$$不是$$\beta_1$$的一致估计量。$$\rho_{X_u} (\sigma_u / \sigma_X)$$项是$$\hat{\beta}_1$$在大样本中仍然存在的偏差。

(2) 偏差的大小取决于解释变量和误差项的相关系数$$\rho_{X_u}$$。$$\mid \rho_{X_u}$$越大，偏差越大。

(3) $$\hat{\beta}_1$$的偏差方向取决于X和u是正相关还是负相关。例如，我们猜测仍然在学习英语的学生比例对学区

### 通过数据分组处理遗漏变量偏差

应该如何处理遗漏变量偏差？可以通过数据分组观察遗漏变量带来的偏差。

更有效地方法是，使用多元回归的方法可以提供这种估计。

## 多元回归模型

多元回归模型(multiple regression model)加入了其他变量作为解释变量，扩展了一元回归模型。该模型能够在其他解释变量$$(X_{2i}, X_{3i}, ...)$$不变的情况下，估计变量$$X_{1i}$$的变化对$$Y_i$$的影响。

### 总体回归线

假设现在只有两个解释变量$$X_{1i}$$和$$X_{2i}$$。在多元线性回归模型中，这两个解释变量和被解释变量Y之间的均值关系可用如下线性方程表示：

$$E(Y_i \mid X_{1i} = x_1, X_{2i} = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$

其中，$$E(Y_i \mid X_{1i} = x_1, X_{2i} = x_2)$$是给定$$X_{1i} = x_1$$和$$X_{2i} = x_2$$时，$$Y_i$$的条件期望。

上式是多元回归模型的总体回归线（population regression line）或总体回归方程（population regression function）。系数$$\beta_0$$是截距（intercept）；系数$$\beta_1$$是$$X_{1i}$$的斜率系数，或者简称$$X_{1i}$$的稀疏；系数$$\beta_2$$是$$X_{2i}$$的斜率系数，或者简称$$X_{2i}$$的稀疏；有时也称多元回归模型中的一个或多个解释变量为控制变量。

上式中系数$$\beta_1$$的解释不同于当$$X_{1i}$$是唯一解释变量时的情况。在上式中，$$\beta_1$$是在保持$$X_2$$不变（或控制$$X_2$$）的条件下，$$X_1$$的1个单位的变动对Y的影响。

在$$X_2$$保持不变的情况下，$$X_1$$的变化($$\delta X_1$$)对Y的期望效益，是解释变量为$$X_1 + \delta X_1$$和$$X_2$$时Y的期望剪去解释变量为$$X_1$$和$$X_2$$时Y的期望之差。相应地，把上式中的总体回归方程写成$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$，假设$$X_2$$不变（即控制$$X_2$$为常数），$$X_1$$改变$$\delta X_1$$。由于$$X_1$$发生了变动，$$Y$$也会变动一定数量，设为$$\delta Y$$。变化之后的新Y值$$(Y+\delta Y)$$为

$$Y + \delta Y = \beta_0 + \beta_1 (X_1 + \delta X_1) + \beta_2 X_2$$

上式减去等式$$Y=\beta_0 + \beta_1 X_1 + \beta_2 X_2$$，便得到用$$\delta X_1$$表示的$$\delta Y$$表达式，即$$\delta Y = \beta_1 \delta X_1$$，对该方程变形得

$$\beta_1 = \frac{\delta Y}{\delta X_1}, 保持X_2不变$$

系数$$\beta_1$$是保持$$X_2$$固定不变时，1个单位$$X_1$$的变化对Y的影响（Y的期望的变化）。对$$\beta_1$$的刻画，也可以用另一种表述，即在$$X_2$$固定不变时，$$X_1$$对Y的偏效应（partial effect）。

对多元回归模型的截距$$\beta_0$$的解释与一元回归模型中类似：它是当$$X_{1i}$$和$$X_{2i}$$都为零时$$Y_i$$的期望值。简而言之，截距$$\beta_0$$决定了总体回归线与Y轴交点距离远点有多远。

### 总体回归模型

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,...,n$$

上式表示的是有两个解释变量的总体多元回归模型（population multiple regression model）。

在双变量回归中，可以认为$$\beta_0$$是恒等于1的解释变量的系数，即考虑$$\beta_0$$是$$X_{oi}$$的系数，对于$$i=1,...,n, X_{0i}=1$$。据此，上式的总体多元回归模型也可以写成

$$当$$X_{0i}=1, Y_i = \beta_0 X_{0i} + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i, i=1,...,n$$

由于$$X_{0i}$$对于所有的观测值皆取值为1，它有时被称为常数回归变量（constant regressor）。同样地，$$\beta_0$$有时被称为回归中的常数项（constant term）。

由此，我们可以拓展至更一般的k变量模型。

$$Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k + u_i, i=1,...,n$$

多元回归模型中的同方差和异方差定义是其在一元模型中的拓展形式。多元回归模型中，给定$$X_{1i}, ..., X_{ki}$$时，如果对于$$i=1,...,n$$，误差项$$u_i$$的条件分布方差$$Var(u_i \mid X_{1i}, ..., X_{ki})$$是常数，即不依赖$$X_{1i}, ..., X_{ki}$$的取值，则$$u_i$$是同方差的。否则，误差项是异方差的。

## 多元回归的OLS估计量

### OLS估计量

我们同样可以使用OLS方法来估计多元回归模型中的系数$$\beta_0, \beta_1, ..., \beta_k$$。设$$\beta_0, \beta_1, ..., \beta_K$$的估计量分别为$$b_0, b_1, ..., b_k$$，我们利用这些估计量计算得到$$Y_i$$的预测值为$$b_0 + b_1 X_{1i} + ... + b_k X_{ki}$$，同时$$Y_i$$的预测误差为$$Y_i - (b_0 + b_1 X_{1i} + ... + b_k X_{ki}) = Y_i - b_0 - b_1 X_{1i} - ... - b_k X_{ki}$$。从而我们得到n组观测的预测误差平方和为

$$\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - ... - b_k X_{ki})^2$$

使上式预测误差平方和达到最小的系数$$\beta_0, \beta_1, ..., \beta_k$$的估计量，被称为$$\beta_0, \beta_1, ..., \beta_k$$的普通最小二乘（OLS）估计量，分别记为$$\hat{\beta}_0, \hat{\beta}_1, ..., \hat{\beta}_k$$。

多元线性回归模型中的OLS术语与一元线性回归模型基本一致。利用OLS估计量构造的直线$$\hat{\beta}_0 + \hat{\beta}_1 X_1 + ... + \hat{\beta}_k X_k$$，被称为OLS回归线。当$$X_{1i}, ..., X_{ki}$$给定时，基于OLS回归线便可以得到$$Y_i$$的预测值$$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_{1i} + ... + \hat{\beta}_k X_{ki}$$。对于第i组观测值，其OLS残差$$\hat{u}_i = Y_i - \hat{Y}_i$$。

我们可以利用试错法来计算OLS估计量，即通过尝试不同的$$b_0, ..., b_k$$取值直至残差平方和达到最小为止。除此之外，还可以利用微积分来推导OLS估计量，而且这种方法会简单很多。

在多元回归模型中，我们最好利用矩阵来表示和讨论这些公式。