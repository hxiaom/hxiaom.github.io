---
layout: post
title: "CS5487 Machine Learning (2)"
categories: Analytics
---

## 2. Probability Distributions

For parameter estimation, in a **frequentist** treatment, we choose specific values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a **Bayesian** treatment we introduce prior distributions over the parameters and then use Byes' theorem to compute the corresponding posterior distribution given the observed data.

One limitation of the parametric approach is that it **assumes a specific functional form for the distribution**, which may turn out to be inappropriate for a particular application. An alternative approach is given by **nonparametric** density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these **control the model complexity rather than the form of the distribution**. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels.

### 2.1 Binary Variables

It is worth noting that the log likelihood function depends on the N observations x_n only through their sum. This sum provides an example of a **sufficient statistic** for the data under this distribution.

#### 2.1.1 The beta distribution

As we have already noted, this can give severely **over-fitted** results for small data sets. In order to develop a **Bayesian** treatment for this problem, we need to introduce a **prior distribution p(μ)** over the parameter μ.