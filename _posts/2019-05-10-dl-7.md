---
layout: post
title: 【Method】深度学习（七）反向传播与随机梯度下降
categories: Analytics
---

## 原理

当我们使用前馈神经网络接收输入x并产生输出$$\hat{y}$$时，信息通过网络向前流动。输入x提供初始信息，然后传播到每一层的隐藏单元，最终产生输出$$\hat{y}$$。这称之为前向传播（forward propagation）。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数$$J(\theta)$$。反向传播（back propagation）算法（Rumelhart et al., 1986），经常简称backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在计算上的代价可能很大。反向传播算法使用简单和廉价的程序来实现这个目标。

反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但原则上它可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。特别地，我们会描述如何计算一个任意函数f的梯度$$\nabla_x f(x,y)$$，其中x是一组变量，我们需要它们的导数，而y是函数的另外一组输入变量，但我们并不需要它们的导数。在学习算法中，我们最常需要的梯度是代价函数关于参数的梯度，即$$\nabla_\theta J(\theta)$$。许多机器学习任务需要计算其他导数，来作为学习过程的一部分，或者用来分析学得的模型。反向传播算法也适用于这些任务，不局限于计算代价函数关于参数的梯度。通过在网络中传播信息来计算导数的想法非常普遍，它还可以用于计算诸如多输入函数f的Jacobian的值。我们这里描述的是最常用的情况，其中f只有单个输出。

赫布理论（Hebbian theory）是一个神经科学理论，解释了在学习的过程中脑中的神经元所发生的变化。赫布理论描述了突触可塑性的基本原理，即突触前神经元向突触后神经元的持续重复的刺激，可以导致突触传递效能的增加。

### 计算图

为了更精确地描述反向传播算法，使用更精确的计算图（computational graph）语言是很有帮助的。

将计算形式化为图形的方法有很多。

这里，我们使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩阵、张量、或者甚至是另一类型的变量。

为了形式化我们的图形，我们还需引入操作（operation）这一概念。操作是指一个或多个变量的简单函数。我们的图形语言伴随着一组被允许的操作。我们可以通过将多个操作复合在一起来描述更为复杂的函数。

不失一般性，我们定义一个操作仅返回单个输出变量。这并没有失去一般性，是因为输出变量可以有多个条目，例如向量。反向传播的软件实现通常支持具有多个输出的操作，但是我们在描述中避免这种情况，因为它引入了对概念理解不重要的许多额外细节。

如果变量y是变量x通过一个操作计算得到的，那么我们画一条从x到y的有向边。我们有时用操作的名称来注释输出的节点，当上下文很明确时，有时也会省略这个标注。

### 微积分中的链式法则

微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。

设x是实数，f和g是从实数映射到实数的函数。假设$$y = g(x)$$并且$$z = f(g(x)) = f(y)$$。那么链式法则是说

$$\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$$

我们可以将这种标量进行扩展。假设$$x \in R^m, y \in R^n$$，g是从$$R^m$$到$$R^n$$的映射，f是从$$R^n$$到$$R$$的映射。如果y=g(x)并且z=f(y)，那么

$$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_J}{\partial x_i}$$

使用向量记法，可以等价地写成

$$\bigtriangledown_x z = (\frac{\partial y}{\partial x})^T \bigtriangledown_y z$$

这里$$\frac{\partial y}{\partial x}$$是g的$$n \times m$$的Jacobian矩阵。

从这里我们看到，变量x的梯度可以通过Jacobian矩阵$$\frac{\partial y}{\partial x}$$和梯度$$\bigtriangledown_y z$$相乘来得到。反向传播算法由图中每一个这样的Jacobian梯度的乘积操作所组成。

通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。从概念上讲，这与使用向量的反向传播完全相同。唯一的区别是如何将数字排列成网络以形成张量。我们可以想象，在运行反向传播之前，将每个张量变平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。从这种重新排列的观点上看，反向传播仍然只是将Jacobian乘以梯度。

为了表示值z关于张量X的梯度，我们记为$$\bigtriangledown_X z$$，就像X是向量一样。X的索引现在有多个坐标——例如，一个3维的张量由3个坐标索引。我们可以通过使用单个变量i来表示完整的索引元组，从而完全抽象出来。对所有可能的元组i，$$(\bigtriangledown_X z)_i$$给出$$\frac{\partial z}{\partial X_i}$$。这与向量中索引的方式完全一致，$$(\bigtriangledown_x z)_i$$给出$$\frac{\partial z}{\partial x_i}$$。使用这种记法，我们可以写出适用于张量的链式法则。如果Y=g(X)并且z=f(Y)，那么

$$\bigtriangledown_X z = \sum_j (\bigtriangledown_X Y_j) \frac{\partial z}{\partial Y_j}$$

### 递归地使用链式法则来实现反向传播

使用链式规则，我们可以直接写出某个标量关于计算图中任何产生该标量的节点的梯度的代数表达式。然而，实际在计算机中计算该表达式时会引入一些额外的考虑。

具体来说，许多子表达式可能在梯度的整个表达式中重复若干次。任何计算梯度的程序都需要选择是存储这些子表达式还是重新计算它们几次。在某些情况下，计算两次相同的子表达式纯粹是浪费。在复杂图中，可能存在指数多的这种计算上的浪费，使得简单的链式法则不可实现。在其他情况下，计算两次相同的子表达式可能是以较高的运行时间为代价来减少内存开销的手段。

我们首先给出一个版本的反向传播算法，它指明了梯度的直接计算方式，按照它实际完成的顺序并且递归地使用链式法则。我们可以直接执行这些计算或者将算法的描述视为用于计算反向传播的计算图的符号表示。然而，这些公式并没有明确地操作和构造用于计算梯度的符号图。

## 基于梯度的优化

大多数深度学习算法都涉及某种形式的优化。优化指的是改变x以最小化或最大化某个函数f(x)的任务。我们通常以最小化f(x)指代大多数最优化问题。最大化可经由最小化算法最小化-f(x)来实现。

我们把要最小化或最大化的函数称为目标函数（objective function）或准则（criterion）。当我们对其进行最小化时，也把它称为代价函数（cost function）、损失函数（loss function）或误差函数（error function）。

我们通常使用个一个上标*表示最小化或最大化函数的x值，如记$$x^* = arg min f(x)$$。

这里简要回顾微积分概念如何与优化联系。

假设有一个函数$$y=f(x)$$，其中x和y是实数。这个函数的导数（derivative）记为$$f'(x)$$或$$\frac{dy}{dx}$$。导数$$f'(x)$$代表f(x)在点x处的斜率。换句话说，它表明如何缩放输入的小变化才能在输出获得相应的变化：$$f(x+\epsilon) \approx f(x) + \epsilon f'(x)$$。

因此导数对于最小化一个函数很有用，因为它告诉我们如何更改x来略微地改善y。例如，我们知道对于足够小的$$\epsilon$$来说，$$f(x-\epsilon (f'(x)))$$是比f(x)小的。因此我们可以将x往导数的反方向移动一小步来减小f(x)。这种技术称为梯度下降（gradient descent）。

当$$f'(x)=0$$时，导数无法提供往哪个方向移动的信息。$$f'(x)=0$$的点称为临界点(critical point)或驻点(stationary point)。一个局部极小点(local minimum)意味着这个点的f(x)小于所有邻近点，因此不可能通过移动无穷小的步长来减小f(x)。一个局部极大点（local maximum）意味着这个点的f(x)大于所有邻近点，因此不可能通过移动无穷小的步长来增大f(x)。有些临界点既不是最小点也不是最大点，这些点称为鞍点(saddle point)。

使f(x)取得绝对的最小值（相对所有其他值）的点是全局最小点（global minimum）。函数可能只有一个全局最小点或存在多个全局最小点，还可能存在不是全局最优的局部极小点。


## 随机梯度下降

几乎所有的深度学习算法都用到了一个非常重要的算法：随机梯度下降（stochastic gradient descent，SGD）。

## Reference

- 深度学习. 6.5 反向传播和其他微分算法
- [反向传播演算](https://www.youtube.com/watch?v=tIeHLnjs5U8)