---
layout: post
title: 【Method】深度学习（七）反向传播
categories: Analytics
---

## 原理

当我们使用前馈神经网络接收输入x并产生输出$$\hat{y}$$时，信息通过网络向前流动。输入x提供初始信息，然后传播到每一层的隐藏单元，最终产生输出$$\hat{y}$$。这称之为前向传播（forward propagation）。在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数$$J(\theta)$$。反向传播（back propagation）算法（Rumelhart et al., 1986），经常简称backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在计算上的代价可能很大。反向传播算法使用简单和廉价的程序来实现这个目标。

反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但原则上它可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。特别地，我们会描述如何计算一个任意函数f的梯度$$\Nabla_x f(x,y)$$，其中x是一组变量，我们需要它们的导数，而y是函数的另外一组输入变量，但我们并不需要它们的导数。在学习算法中，我们最常需要的梯度是代价函数关于参数的梯度，即$$\Nabla_\theta J(\theta)$$。许多机器学习任务需要计算其他导数，来作为学习过程的一部分，或者用来分析学得的模型。反向传播算法也适用于这些任务，不局限于计算代价函数关于参数的梯度。通过在网络中传播信息来计算导数的想法非常普遍，它还可以用于计算诸如多输入函数f的Jacobian的值。我们这里描述的是最常用的情况，其中f只有单个输出。

### 计算图

为了更精确地描述反向传播算法，使用更精确的计算图（computational graph）语言是很有帮助的。

将计算形式化为图形的方法有很多。

这里，我们使用图中的每一个节点来表示一个变量。变量可以是标量、向量、矩阵、张量、或者甚至是另一类型的变量。

为了形式化我们的图形，我们还需引入操作（operation）这一概念。操作是指一个或多个变量的简单函数。我们的图形语言伴随着一组被允许的操作。我们可以通过将多个操作复合在一起来描述更为复杂的函数。

不失一般性，我们定义一个操作仅返回单个输出变量。这并没有失去一般性，是因为输出变量可以有多个条目，例如向量。反向传播的软件实现通常支持具有多个输出的操作，但是我们在描述中避免这种情况，因为它引入了对概念理解不重要的许多额外细节。

如果变量y是变量x通过一个操作计算得到的，那么我们画一条从x到y的有向边。我们有时用操作的名称来注释输出的节点，当上下文很明确时，有时也会省略这个标注。

### 微积分中的链式法则

微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运算顺序。

设x是实数，f和g是从实数映射到实数的函数。假设$$y = g(x)$$并且$$z = f(g(x)) = f(y)$$。那么链式法则是说

$$\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$$

我们可以将这种标量进行扩展。假设$$x \in R^m, y \in R^n$$，g是从$$R^m$$到$$R^n$$的映射，f是从$$R^n$$到$$R$$的映射。如果y=g(x)并且z=f(y)，那么

$$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_J}{\partial x_i}$$

使用向量记法，可以等价地写成

$$\bigtriangledown_x z = (\frac{\partial y}{\partial x})^T \bigtriangledown_y z$$

这里$$\frac{\partial y}{\partial x}$$是g的$$n \times m$$的Jacobian矩阵。

从这里我们看到，变量x的梯度可以通过Jacobian矩阵$$\frac{\partial y}{\partial x}$$和梯度$$\bigtriangledown_y z$$