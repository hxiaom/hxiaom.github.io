---
layout: post
title: 【Method】深度学习正则化（一）参数范数惩罚
categories: Analytics
---

## 原理

机器学习中的一个核心问题是设计不仅在训练数据上表现好，而且能在新输入上泛化好的算法。在机器学习中，许多策略被显示地设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。

我们将正则化定义为“对学习算法的修改——旨在减少泛化误差而不是训练误差”。目前有许多正则化策略。有些策略向机器学习模型添加限制参数值的额外约束。有些策略向目标函数增加额外项来对参数值进行软约束。如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时候，这些约束和惩罚被设计为编码特定类型的先验知识；其他时候，这些约束和惩罚被设计为偏好简单模型，以便提高泛化能力。有时，惩罚和约束对于确定欠定的问题的必要的。其他形式的正则化，如被称为集成的方法，则结合多个假说来解释训练数据。

在深度学习的背景下，大多数正则化策略都会对估计进行正则化。估计的正则化以偏差的增加换取方差的减少。一个有效的正则化是有利的“交易”，也就是能显著减少方差而不过度增加偏差。我们在讨论泛化和过拟合时，主要侧重模型族训练的3种情况：（1）不包括真实的数据生成过程——对应欠拟合含有偏差的情况；（2）匹配真实数据生成过程；（3）除了包括真实的数据生成过程，还包括许多其他可能的生成过程——方差（而不是偏差）主导的过拟合。正则化的目标是使模型从第三种情况转化为第二种情况。

在实践中，过于复杂的模型族不一定包括目标函数或真实数据生成过程，甚至也不包括近似过程。我们几乎从未知晓真实数据的生成过程，所以我们永远不知道被估计的模型族是否包括生成过程。然而，深度学习算法的大多数应用都是针对这样的情况，其中真实数据的生成过程几乎肯定在模型族之外。深度学习算法通常应用于极为复杂的领域，如图像、音频序列和文本，本质上这些领域的真实生成过程涉及模拟整个宇宙。从某种程度上说，我们总是持方枘（数据生成过程）而欲内园凿（模型族）。

这意味着控制模型的复杂度不是找到合适规模的模型（带有正确的参数个数）这样一个简单的事情。相反，我们可能会发现，或者说在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。

### 参数范数惩罚

正则化在深度学习的出现前就已经被使用了数十年。线性模型，如线性回归和逻辑回归，可以使用简单、直接、有效的正则化策略。

许多正则化方法通过对目标函数J添加一个参数范数惩罚$$\Omega(\theta)$$，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化后的目标函数记为$$\tilde{J}$$:

$$\tilde{J}(\theta; X, y) = J(\theta; X,y) + \alpha \Omega(\theta)$$

其中$$\alpha \in [0, \infty]$$是权衡范数惩罚项$$\Omega$$和标准目标函数$$J(X;\theta)$$相对贡献的超参数。将$$\alpha$$设为0表示没有正则化。$$\alpha$$越大，对应正则化惩罚越大。

当我们的训练算法最小化正则化后的目标函数$$\tilde{J}$$时，它会降低原始目标J关于训练数据的误差并同时减小在某些衡量标准下参数$$\theta$$（或参数子集）的规模。选择不同的参数范数$$\Omega$$会偏好不同的解。我们将讨论各种范数惩罚对模型的影响。

在研究不同范数的正则化表现之前，需要说明一下，在神经网络中，参数包括每一层仿射变换的权值和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。精确拟合偏置所需的数据通常比拟合权重少得多。每个权重都会指定两个变量如何相互作用。我们需要在各种条件下观测这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量w表示所有应受范数惩罚影响的权重，而向量$$\theta$$表示所有参数（包括w和无须正则化的参数）。

在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的$$\alpha$$系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。

### $$L^2$$参数正则化

最简单也最常见的参数范数惩罚是通常被称为权重衰减（weight decay）的$$L^2$$参数范数惩罚。这个正则化策略通过向目标函数添加一个正则化项$$\Omega(\theta) = \frac{1}{2} \| w \|_2^2$$，使权重更加接近原点。在其他学术圈，$$L^2$$也被称为岭回归或Tikhonov正则。

