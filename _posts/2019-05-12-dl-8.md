---
layout: post
title: 【Method】深度学习正则化（一）参数范数惩罚
categories: Analytics
---

## 原理

机器学习中的一个核心问题是设计不仅在训练数据上表现好，而且能在新输入上泛化好的算法。在机器学习中，许多策略被显示地设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。

我们将正则化定义为“对学习算法的修改——旨在减少泛化误差而不是训练误差”。目前有许多正则化策略。有些策略向机器学习模型添加限制参数值的额外约束。有些策略向目标函数增加额外项来对参数值进行软约束。如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时候，这些约束和惩罚被设计为编码特定类型的先验知识；其他时候，这些约束和惩罚被设计为偏好简单模型，以便提高泛化能力。有时，惩罚和约束对于确定欠定的问题的必要的。其他形式的正则化，如被称为集成的方法，则结合多个假说来解释训练数据。

在深度学习的背景下，大多数正则化策略都会对估计进行正则化。估计的正则化以偏差的增加换取方差的减少。一个有效的正则化是有利的“交易”，也就是能显著减少方差而不过度增加偏差。我们在讨论泛化和过拟合时，主要侧重模型族训练的3种情况：（1）不包括真实的数据生成过程——对应欠拟合含有偏差的情况；（2）匹配真实数据生成过程；（3）除了包括真实的数据生成过程，还包括许多其他可能的生成过程——方差（而不是偏差）主导的过拟合。正则化的目标是使模型从第三种情况转化为第二种情况。

在实践中，过于复杂的模型族不一定包括目标函数或真实数据生成过程，甚至也不包括近似过程。我们几乎从未知晓真实数据的生成过程，所以我们永远不知道被估计的模型族是否包括生成过程。然而，深度学习算法的大多数应用都是针对这样的情况，其中真实数据的生成过程几乎肯定在模型族之外。深度学习算法通常应用于极为复杂的领域，如图像、音频序列和文本，本质上这些领域的真实生成过程涉及模拟整个宇宙。从某种程度上说，我们总是持方枘（数据生成过程）而欲内园凿（模型族）。

这意味着控制模型的复杂度不是找到合适规模的模型（带有正确的参数个数）这样一个简单的事情。相反，我们可能会发现，或者说在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。

### 参数范数惩罚

正则化在深度学习的出现前就已经被使用了数十年。线性模型，如线性回归和逻辑回归，可以使用简单、直接、有效的正则化策略。

许多正则化方法通过对目标函数J添加一个参数范数惩罚$$\Omega(\theta)$$，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化后的目标函数记为$$\tilde{J}$$:

$$\tilde{J}(\theta; X, y) = J(\theta; X,y) + \alpha \Omega(\theta)$$

其中$$\alpha \in [0, \infty]$$是权衡范数惩罚项$$\Omega$$和标准目标函数$$J(X;\theta)$$相对贡献的超参数。将$$\alpha$$设为0表示没有正则化。$$\alpha$$越大，对应正则化惩罚越大。

