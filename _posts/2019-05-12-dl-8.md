---
layout: post
title: 【Method】深度学习正则化（一）参数范数惩罚
categories: Analytics
---

## 原理

机器学习中的一个核心问题是设计不仅在训练数据上表现好，而且能在新输入上泛化好的算法。在机器学习中，许多策略被显示地设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。

我们将正则化定义为“对学习算法的修改——旨在减少泛化误差而不是训练误差”。目前有许多正则化策略。有些策略向机器学习模型添加限制参数值的额外约束。有些策略向目标函数增加额外项来对参数值进行软约束。如果我们细心选择，这些额外的约束和惩罚可以改善模型在测试集上的表现。有时候，这些约束和惩罚被设计为编码特定类型的先验知识；其他时候，这些约束和惩罚被设计为偏好简单模型，以便提高泛化能力。有时，惩罚和约束对于确定欠定的问题的必要的。其他形式的正则化，如被称为集成的方法，则结合多个假说来解释训练数据。

在深度学习的背景下，大多数正则化策略都会对估计进行正则化。估计的正则化以偏差的增加换取方差的减少。一个有效的正则化是有利的“交易”，也就是能显著减少方差而不过度增加偏差。我们在讨论泛化和过拟合时，主要侧重模型族训练的3种情况：（1）不包括真实的数据生成过程——对应欠拟合含有偏差的情况；（2）匹配真实数据生成过程；（3）除了包括真实的数据生成过程，还包括许多其他可能的生成过程——方差（而不是偏差）主导的过拟合。正则化的目标是使模型从第三种情况转化为第二种情况。

在实践中，过于复杂的模型族不一定包括目标函数或真实数据生成过程，甚至也不包括近似过程。我们几乎从未知晓真实数据的生成过程，所以我们永远不知道被估计的模型族是否包括生成过程。然而，深度学习算法的大多数应用都是针对这样的情况，其中真实数据的生成过程几乎肯定在模型族之外。深度学习算法通常应用于极为复杂的领域，如图像、音频序列和文本，本质上这些领域的真实生成过程涉及模拟整个宇宙。从某种程度上说，我们总是持方枘（数据生成过程）而欲内园凿（模型族）。

这意味着控制模型的复杂度不是找到合适规模的模型（带有正确的参数个数）这样一个简单的事情。相反，我们可能会发现，或者说在实际的深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化误差的意义上）是一个适当正则化的大型模型。

### 参数范数惩罚

正则化在深度学习的出现前就已经被使用了数十年。线性模型，如线性回归和逻辑回归，可以使用简单、直接、有效的正则化策略。

许多正则化方法通过对目标函数J添加一个参数范数惩罚$$\Omega(\theta)$$，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化后的目标函数记为$$\tilde{J}$$:

$$\tilde{J}(\theta; X, y) = J(\theta; X,y) + \alpha \Omega(\theta)$$

其中$$\alpha \in [0, \infty]$$是权衡范数惩罚项$$\Omega$$和标准目标函数$$J(X;\theta)$$相对贡献的超参数。将$$\alpha$$设为0表示没有正则化。$$\alpha$$越大，对应正则化惩罚越大。

当我们的训练算法最小化正则化后的目标函数$$\tilde{J}$$时，它会降低原始目标J关于训练数据的误差并同时减小在某些衡量标准下参数$$\theta$$（或参数子集）的规模。选择不同的参数范数$$\Omega$$会偏好不同的解。我们将讨论各种范数惩罚对模型的影响。

在研究不同范数的正则化表现之前，需要说明一下，在神经网络中，参数包括每一层仿射变换的权值和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。精确拟合偏置所需的数据通常比拟合权重少得多。每个权重都会指定两个变量如何相互作用。我们需要在各种条件下观测这两个变量才能良好地拟合权重。而每个偏置仅控制一个单变量。这意味着，我们不对其进行正则化也不会导致太大的方差。另外，正则化偏置参数可能会导致明显的欠拟合。因此，我们使用向量w表示所有应受范数惩罚影响的权重，而向量$$\theta$$表示所有参数（包括w和无须正则化的参数）。

在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的$$\alpha$$系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。

### $$L^2$$参数正则化

最简单也最常见的参数范数惩罚是通常被称为权重衰减（weight decay）的$$L^2$$参数范数惩罚。这个正则化策略通过向目标函数添加一个正则化项$$\Omega(\theta) = \frac{1}{2} \| w \|_2^2$$，使权重更加接近原点。在其他学术圈，$$L^2$$也被称为岭回归或Tikhonov正则。

我们可以通过研究正则化后目标函数的梯度，洞察一些权重衰减的正则化表现。为了简单起见，我们假设其中没有偏置参数，因此$$\theta$$就是w。这样一个模型具有以下总的目标函数：

$$\tilde{J}(w; X, y) = \frac{\alpha}{2} w^Tw + J(w; X, y)$$

与之对应的梯度为

$$\bigtriangledown_w \tilde{J}(w; X,y) = \alpha w + \bigtriangledown_w J(w; X,y)$$

使用单步梯度下降更新权重，即执行以下更新：

$$ w \gets w - \epsilon(\alpha w + \bigtriangledown_w J(w; X,y))$$

换种写法就是

$$ w \gets (1-\epsilon \alpha)w - \epsilon \bigtriangledown_w J(w; X,y)$$

我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因子）。这是单个步骤发生的变化。但是，在训练的整个过程会发生什么呢？

我们进一步简化分析，令$$w^*$$为未正则化的目标函数取得最小训练误差时的权重向量，即$$w^*=arg min_w J(w)$$，并在$$w^*$$的邻域对目标函数做二次近似。如果目标函数确实是二次的（如以均方误差拟合线性回归模型的情况），则该近似是完美的。近似的$$\hat{J}(\theta)$$如下：

$$\hat{J} (\theta) = J(w^*) + \frac{1}{2}(w - w^*)^T H(w - w^*)$$

其中H是J在$$w^*$$处计算的Hessian矩阵（关于w）。因为$$w^*$$被定义为最优，即梯度消失为0，所以该二次近似中没有一阶项。同样地，因为$$w^*$$是J的一个最优点，我们可以得出H是半正定的结论。

当$$\hat{J}$$取得最小时，其梯度

$$\bigtriangledown_w \hat{J} (w) = H(w - w^*)$$

为0.

为了研究权重衰减带来的影响，我们探讨最小化正则化后的$$\hat{J}$$。我们使用变量$$\tilde{w}$$表示此时的最优点：

$$\alpha \tilde{w} + H(\tilde{w} - w^*) = 0$$

$$(H + \alpha I) \tilde{w} = H w^*$$

$$\tilde{w} = (H + \alpha I)^{-1} H w^*$$

当$$\alpha$$趋向于0时，正则化的解$$\tilde{w}$$会趋向于$$w^*$$。那么当$$\alpha$$增加时会发生什么呢？因为H是实对称的，所以我们可以将其分解为一个对角矩阵$$\Lambda$$和一组特征向量的标准正交基Q，并且有$$H = Q\Lambda Q^T$$。可得，

$$\begin{align}
\tilde{w} &= (Q\Lambda Q^T + \alpha I)^{-1} Q\Lambda Q^T w^* \\
&= [Q(\Lambda + \alpha I) Q^T]^{-1} Q \Lambda Q^T w^* \\
&= Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^* \\
\end{align}$$

我们可以看到权重衰减的效果是沿着由H的特征向量所定义的轴缩放$$w^*$$。具体来说，我们会根据$$\frac{\lambda_i}{\lambda_i + \alpha}$$因子缩放与H第i个特征向量对齐的$$w^*$$的分量。

沿着H特征值较大的方向（如$$\lambda_i >> \alpha$$）正则化的影响较小。而$$\lambda_i << \alpha$$的分量将会收缩到几乎为零。

只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向（对应Hessian矩阵较小的特征值）上改变参数不会显著增加梯度。这种不重要方向对应的分量在训练过程中因正则化而衰减掉。

目前为止，我们讨论了权重衰减对优化一个抽象通用的二次代价函数的影响。这些影响具体是怎么和机器学习关联的呢？我们可以研究线性回归，它的真实代价函数是二次的，因此我们可以使用相同的方法分析。再次应用分析，我们会在这种情况下得到相同的结果，但这次我们使用训练数据的术语表述。线性回归的代价函数是平方误差之和：

$$(Xw - y)^T(Xw-y)$$

我们添加$$L^2$$正则项后，目标函数变为

$$(Xw -y)^T(Xw-y) + \frac{1}{2} \alpha w^T w$$

这将正规方程的解从

$$w = (X^T X)^{-1}X^T y$$

变为

$$w = (X^T X + \alpha I)^{-1}X^T y$$

未正则化时，矩阵$$X^T X$$与协方差$$\frac{1}{m}X^T X$$成正比。$$L^2$$正则项将这个矩阵替换为$$(X^T X + \alpha I)^{-1}$$，这个新矩阵与原来的是一样的，不同的仅仅是在对角加了$$\alpha$$。这个矩阵的对角项对应每个输出特征的方差。我们可以看到，$$L^2$$正则化能够让学习算法“感知”到具有较高方差的输入x，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。

### $$L^1$$正则化

对模型参数w的$$L^1$$正则化被定义为

$$\Omega(\theta) = \|w\|_1 + \sum_i \mid w_i \mid$$

即各个参数的绝对值之和。接着我们将讨论$$L^1$$正则化对简单线性回归模型的影响，与分析$$L^2$$正则化时一样不考虑偏置参数。我们尤其感兴趣的是找出$$L^1$$和$$L^2$$正则化之间的差异。与$$L^2$$权重衰减类似，我们也可以通过缩放惩罚项$$\Omega$$的正超参数$$\alpha$$来控制$$L^1$$权重衰减的强度。因此，正则化的目标函数$$\tilde{J}(w;X,y)$$如下所示

$$\tilde{J} (w;X,y) = \alpha \|w\|_1 +J(w;X,y)$$

对应的梯度（实际上是次梯度）：

$$\triangledown_w \tilde{J}(w;X,y) = \alpha sign(w) + \triangle_w J(w; X,y)$$

其中sign(w)只是简单地取w各个元素的正负号。

观察上式，我们立刻发现$$L^1$$的正则化效果与$$L^2$$大不一样。具体来说，我们可以看到正则化对梯度的影响不再是线性地缩放每个$$w_i$$；而是添加了一项与$$sign(w_i)$$同号的常数。使用这种形式的梯度之后，我们不一定能得到$$J(X,y;w)$$二次近似的直接算术解（$$L^2$$正则化时可以）。

