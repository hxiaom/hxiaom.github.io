---
layout: post
title: 【Method】深度学习（五）隐藏单元
categories: Analytics
---

神经网络的设计选择中，使用基于梯度的优化方法来训练的大多数参数化机器学习模型都是通用的。现在我们转向一个前馈神经网络独有的问题：该如何选择隐藏单元的类型，这些隐藏单元用在模型的隐藏层中。

隐藏单元的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理论原则。

整流线性单元是隐藏单元极好的默认选择。许多其他类型的隐藏单元也是可用的。决定何时使用哪种类型的隐藏单元是困难的事（尽管整流线性单元通常是一个可接受的选择）。我们这里描述对于每种隐藏单元的一些基本直觉。这些直觉可用用来建议我们何时来尝试一些单元。通常不可能预先预测出哪种隐藏单元工作得最好。设计过程充满试验和错误，先直觉认为某种隐藏单元可能表现良好，然后用它组成神经网络进行训练，最后用验证集来评估它的性能。

这里列出的一些隐藏单元可能并不是在所有的输入点上都是可微的。例如，整流线性单元$$g(z) = max{0,z}$$在z=0处不可微。这似乎使得g对于基于梯度的学习算法无效。在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值。因为我们不再期望训练能够实际达到梯度为0的点，所以代价函数的最小值对应于梯度未定义的点是可以接受的。不可微的隐藏单元通常只在少数点上不可微。

除非另有说明，大多数的隐藏单元都可以描述为接受输入向量x，计算放射变化$$z=W^T x+b$$，然后使用一个逐元素的非线性函数g(z)。大多数隐藏单元的区别仅仅在于激活函数g(z)的形式。

### 整流线性单元及其扩展

整流线性单元使用激活函数$$g(z) = max{0,z}$$

整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为0，并且在整流线性单元处于激活状态时，它的一阶导数处处为1.这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。

整流线性单元通常作用与仿射变换之上：

$$h=g(W^T x+b)$$

当初始化仿射变换的参数时，可以将b的所有元素设置成一个小的正值，例如0.1。这使得整流线性单元很可能初始时就对训练集中的大多数输入呈现激活状态，并且允许导数通过。

有很多整流线性单元的扩展存在。大多数这些扩展的表现比得上整流线性单元，并且偶尔表现得更好。

整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。整流线性单元的各种扩展保证了它们能在各个位置都接收到梯度。

整流线性单元的三个扩展基于当$$z_i < 0$$时使用一个非零的斜率$$\alpha_i : h_i = g(z, \alpha)_i = max(0, z_i) + \alpha_i min(0, z_i)$$。绝对值整流（absolute value rectification）固定$$\alpha_i = -1$$来得到$$g(z) = \mid z \mid$$。它用于图像中的对象识别，其中寻找在输入照明极性反转下不变的特征是有意义的。整流线性单元的其他扩展比这应用地更广泛。渗漏整流线性单元（Leaky ReLU）将$$\alpha_i$$固定成一个类似0.01的小值，参数化整流线性单元（parametric ReLU）或者PReLU将$$\alpha_i$$作为学习的参数。

### logistic sigmoid与双曲正切函数

在引入整流线性单元之前，大多数神经网络使用logistic sigmoid激活函数

$$g(z) = \sigma(z)$$

或者是双曲正切激活函数

$$g(z) = tanh(z)$$

这些激活函数紧密相关，因为$$tanh(z) = 2\sigma (2z)-1$$。

我们依据看过sigmoid单元作为输出单元用来预测二值型变量取值为1的概率。与分段线性单元不同，sigmoid单元在其大部分定义域被都饱和——当z取绝对值很大的正值时，它们饱和到一个高值，当z取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当z接近0时它们才对输入强烈敏感。sigmoid单元的广泛饱和性使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将它们用作前馈网络中的隐藏单元。当使用一个合适的代价函数来抵消sigmoid的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。


### 其他隐藏单元

也存在许多其他种类的隐藏单元，但它们并不常用。

一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数与流行的激活函数表现得一样好。