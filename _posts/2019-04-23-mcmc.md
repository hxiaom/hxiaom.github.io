---
layout: post
title: 【Method】MCMC
categories: Analytics
---

## 原理

随机模拟（或统计模拟）方法有一个很酷的别名是蒙特卡洛方法（Monte Carlo Simulation）。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯·诺依曼、费米、费曼、Nicholas Metropolis，在美国洛斯阿拉莫斯实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法，并在最早的计算机上进行编程实现。

现代的统计模拟方法最早由数学家乌拉姆提出，被Metropolis命名为蒙特卡罗方法，蒙特卡罗是著名的赌场，赌博总是和统计密切关联的，所以这个命名风趣而贴切，很快被大家广泛接受。不过据说费米之前就已经在实验中使用了，但是没有发表。说起蒙特卡罗方法的源头，可以追溯到18世纪，布丰当年用于计算π的著名的投针实验就是蒙特卡罗模拟实验。统计采用的方法其实数学家们很早就知道，但是在计算机出现以前，随机数生成的成本很高，所以该方法也没有实用价值。随着计算机技术在二十世纪后半叶的迅猛发展，随机模拟技术很快进入实用阶段。对那些用确定算法不可行或不可能解决的问题，蒙特卡罗方法常常给人们带来希望。

统计模拟中有一个重要的问题就是给定一个概率分布$$p(x)$$,我们如何在计算机中生成它的样本。一般而言均匀分布Uniform(0,1)的样本是相对容易生成的。通过线性同余发生器可以生成伪随机数，我们用确定性算法生成[0,1]之间的伪随机数序列后，这些序列的各种统计指标和均匀分布Uniform(0,1)的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。

而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于Uniform(0,1)的样本生成，例如正态分布可以通过著名的Box-Muller变换得到

Box-Muller变换：如果随机变量$$U_1, U_2$$独立且$$U_1, U_2 ~ Uniform(0,1)$$，则

$$Z_0 = \sqrt{-2 ln U_1} cos(2\pi U_2)$$

$$Z_1 = \sqrt{-2 ln U_1} sin(2\pi U_2)$$

则，$$Z_0, Z_1$$独立且服从标准正态分布。

其它几个著名的连续分布，包括指数分布、Gamma分布、t分布、F分布、Beta分布、Dirichlet分布等等，也都可以通过类似的数学变换得到；离散的分布通过均匀分布更加容易生成（更多可参看Sheldon M. Ross 《统计模拟》）。

不过我们并不是总是这么幸运的，当$$p(x)$$的形式很复杂，或者$$p(x)$$是个高维的分布的时候，样本的生成就可能很困难了。譬如有如下的情况

1. $$P(x) = \frac{\widetilde{p}(x)}{\int \widetilde{p}(x)dx}，而$$\widetilde{p}(x)$$我们是可以计算的，但是底下的积分式无法显式计算。

2. $$p(x,y)$$是一个二维的分布函数，这个函数本身计算很困难，但是条件分布$$p(x \mid y) p(y \mid x)$$的计算相对简单；如果p(x)是高维的，这种情形就更加明显。

此时就需要使用一些更加复杂的随机模拟的方法来生成样本。而本文介绍的MCMC和之后介绍的Gibbs Sampling算法就是最常用的两种。这两个方法在现代贝叶斯分析中被广泛使用。要了解这两个算法，我们首先要对马氏链的平稳分布的性质有基本的认识。

### 马氏链及其平稳分布

马氏链的数学定义很简单

$$P(X_{t+1}=x \mid X_t, X_{t-1}, ...) = P(X_{t+1}=x \mid X_t)$$

## 参考文献

- LDA数学八卦 Rickjin