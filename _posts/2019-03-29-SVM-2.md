---
layout: post
title: 【Daily Method】SVM (二) 线性支持向量机
categories: Analytics
---

## 目的

## 原理

线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，因为这时上述方法中的不等式约束并不能都成立（函数间隔大于等于1的约束条件）。这就需要修改硬间隔最大化，使其成为软间隔最大化。

对每个样本点$$(x_i,y_i)$$引进一个松弛变量$$\xi_i \geq 0$$，使函数间隔加上松弛变量大于等于1。这样，约束条件变为

$$ y_i(w \bullet x_i + b) \geq 1 - \xi_i $$

同样，对每个松弛变量$$\xi_i$$，支付一个代价$$\xi_i$$。目标函数由原来的$$\frac{1}{2} \|w\|^2$$变成

$$\frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i$$

这里，C>0称为惩罚参数，一般由应用问题决定，C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。

有了上面的思路，可以和训练数据集可分时一样来考虑训练数据集线性不可分时的线性支持向量学习问题。相应于硬间隔最大化，它成为软间隔最大化。

线性不可分的线性支持向量机的学习问题变成如下凸二次规划问题（原始问题）：

$$ \min_{w,b,\xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i $$

$$ s.t. y_i(w \bullet x_i + b) \geq 1 - \xi_i , i=1,2,...,N $$

$$ \xi_i \geq 0 , i=1,2,...,N $$

线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：

$$ \sum_{i=1}^N [1-y_i(w\bullet x_i+b)]_+ + \lambda \|w\|^2 $$

目标函数的第1项是经验损失或经验风险，函数

$$L(y(w\bullet x+b)) = [1-y(w\bullet x+b)]_+ $$

称为合页损失函数（hinge loss function），下表“+”表示以下取正值的函数。

这两种目标函数是等价的。