---
layout: post
title: 【Method】深度学习（四）输出层
categories: Analytics
---

## 原理

### 输出单元

代价函数的选择与输出单元的选择密切相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。

任何可用作输出的神经网络单元，也可以被用作隐藏单元。这里，我们着重讨论将这些单元用作模型输出时的情况，不过原则上它们也可以在内部使用。

我们假设前馈网络提供了一组定义为$$h=f(x;\theta)$$的隐藏特征。输出层的作用是随后对这些特征进行一些额外的变换来完成整个网络必须完成的任务。

#### 用于高斯输出分布的线性单元

一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性。这些单元往往被直接称为线性单元。

给定特征h，线性输出单元层产生一个向量$$\hat{y} = W^T h +b$$

线性输出层经常被用来产生条件高斯分布的均值：

$$p(y \mid x) = N(y; \hat{y}, I)$$

最大化其对数似然此时等价于最小化均方误差。

最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或更容易地使高斯分布的协方差矩阵作为输入的函数。然而，对于所有输入，协方差矩阵都必须被限定成一个正定矩阵。线性输出层很难满足这种限定，所以通常使用其他的输出单元来对协方差参数化。

因为线性模型不会饱和，所以它们易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法

#### 用于Bernoulli输出分布的sigmoid单元

许多任务需要预测二值型变量y的值。具有两个类的分类问题可以归结为这种形式。

此时最大似然的方法是定义y在x条件下的Bernoulli分布。

Bernoulli分布仅需单个参数来定义。神经网络只需预测$$P(y=1 \mid x)$$即可。为了使这个数是有效的概率，它必须处在区间[0,1]中。

为了满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单元，并且通过阈值来限制它成为一个有效的概率：

$$P(y=1 \mid x) = max{0, min{1, w^Th +b}}$$

这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。当$$w^Th +b$$处于单位区间外时，模型的输出对于其参数的梯度都将为0。梯度为0通常是有问题的，因为学习算法对于如何改善相应的参数不再具有指导意义。

相反，最好是使用一种新的方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用sigmoid输出单元结合最大似然来实现的。