---
layout: post
title: 【Method】循环网络（三）循环神经网络
categories: Analytics
---

## 原理

基于图展开和参数共享的思想，我们可以设计各种循环神经网络。

循环神经网络中一些重要的设计模式包括以下几种：

（1）每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络；

（2）每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络；

（3）隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络。

任何图灵可计算的函数都可以通过这样一个有限维的循环网络计算，在这个意义上，循环神经网络是万能的。RNN经过若干时间步后读取输出，这与由图灵机所用的时间步是渐进线性的，与输入长度也是渐进线性的（Siegelmann and Sontag, 1991）。由图灵机计算的函数是离散的，所以这些结果都是函数的具体实现，而不是近似。RNN作为图灵机使用时，需要一个二进制序列作为输入，其输出必须离散化以提供二进制输出。利用单个有限大小的特定RNN计算在此设置下的所有函数是可能的（Siegelmann and Sontag（1995）用了866个单元）。图灵机的”输入“是要计算函数的详细说明（specification），所以模拟此图灵机的相同网络足以应付所有问题。用于证明的理论RNN可以通过激活和权重（由无限精度的有理数表示）来模拟无限堆栈。

现在我们研究RNN的前向传播公式。这个图没有指定隐藏单元的激活函数。假设使用双曲正切激活函数。此外，图中没有明确指定何种形式的输出和损失函数。假定输出是离散的，如用于预测词或字符的RNN。表示离散变量的常规方式是把输出o作为每个离散变量可能值的非标准化对数概率。然后，我们可以应用softmax函数后续处理后，获得标准化后概率的输出向量$$\hat{y}$$。RNN从特定的初始状态$$h^{(0)}$$开始前向传播。从t=1到$$t=\tau$$的每个时间步，我们应用以下更新方程：

$$\begin{align}
a^{(t)} &= b+ Wh^{(t-1)} + Ux^{(t)} \\
h^{(t)} &= tanh(a^{(t)}) \\
o^{(t)} &= c + Vh^{(t)} \\
\hat{y}^{(t)} &= softmax(o^{(t)}) \\
\end{align}$$

其中的参数的偏置向量b和c连同权重矩阵U、V和W，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输入序列映射到相同长度的输出序列。与x序列配对的y的总损失就是所有时间步的损失之和。例如，$$L^{(t)}$$为给定的$$x^{(1)},...,x^{(t)}$$后$$y^{(t)}$$的负对数似然，则

