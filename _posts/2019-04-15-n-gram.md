---
layout: post
title: 【Method】N-gram Language Models
categories: Analytics
---

## 原理

语言模型（language model，LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的相关研究中得到了广泛的应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因数据缺乏而必须采取平滑（smoothing）算法。

### n元语法

一个语言模型通常构建为字符串s的概率分布p(s)，这里p(s)试图反映的是字符串作为一个句子出现的频率。

对于一个由l个基元（”基元“可以为字、词或短语等，为了表述方便，以后我们只用”词“来通指）构成的句子 $$s=\omega_1 \omega_2 ... \omega_l$$，其概率计算公式可以表示为

$$\begin{align}
p(s)&=p(\omega_1)p(\omega_2 \mid \omega_1)p(\omega_3 \mid \omega_1 \omega_2) ... p(\omega_l \mid \omega_1 ... \omega_{l-1}) \\
&= \prod_{i=1}^l p(\omega_i \mid \omega_1 ... \omega_{i-1}) \\
\end{align}$$

在上式总，产生第$$i(1 \leq i \leq l)$$个词的概率是由已经产生的i-1个词$$\omega_1 \omega_2 ... \omega_{i-1}$$。一般地，我们把前i-1个词$$\omega_1 \omega_2 ... \omega_{i-1}$$称为第i个词的”历史（history）“。在这种计算方法中，随着历史长度的增加，不同的历史数目按指数级增长。如果历史长度为i-1，那么，就有$$L^{i-1}$$种不同的历史（假设L为词汇集的大小），而我们必须考虑在所有$$L^{i-1}$$种不同的历史情况下，产生第i个词的概率。这样的话，模型中就有$$L^i$$个自由参数$$p(\omega_i \mid \omega_1, \omega_2 ... \omega_{i-1})$$。假设L=5000，i=3，那么自由参数的数目就是1250亿个。这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本不可能在训练数据中出现。因此，为了解决这个问题，可以将历史$$\omega_1, \omega_2 ... \omega_{i-1}$$按照某个法则映射到等价类$$E(\omega_1 \omega_2 ... \omega_{i-1})$$，而等价类的数目远远小于不同历史的数目。如果假定：

$$p(\omega_i \mid \omega_1, \omega_2 ... \omega_{i-1}) = p(\omega_i \mid E(\omega_1, \omega_2 ... \omega_{i-1}))$$

那么，自由参数的数目就会大大减少，有很多方法可以将历史划分成等价类，其中，一种比较实际的做法是，将两个历史$$\omega_{i-n+2} ... \omega_{i-1} \omega_i$$和$$v_{k-n+2}...v_{k-1}v_k$$映射到同一个等价类，当且仅当这两个历史最近的$$n-1(1 \leq n \leq l)$$个词相同，即如果$$E(\omega_1 \omega_2 ... \omega_{i-1} \omega_i) = E(v_1 v_2 ... v_{k-1} v_k)$$，当且仅当$$(\omega_{i-n+2} ... \omega_{i-1} \omega_i) = (v_{k-n+2}...v_{k-1}v_k)$$。

满足上述条件的语言模型称为n元语法或n元文法（n-gram）。通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。在实际应用中，取n=3的情况较多。当n=1时，即出现在第i位上的词$$\omega_i$$独立于历史，一元文法被记作unigram，或uni-gram，或monogram；当n=2时，即出现在第i位上的词$$\omega_i$$仅与它前面一个历史词$$\omega_{i-1}$$有关，二元文法模型被称为一阶马尔科夫链（Markov chain），记作bigram或bi-gram；当n=3时，即出现在第i位置上的词$$\omega_i$$仅与它前面的两个历史词$$\omega_{i-2} \omega_{i=1}$$有关，三元文法模型被称为二阶马尔科夫链，记作trigram或tri-gram。