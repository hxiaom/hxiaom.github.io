---
layout: post
title: 【Method】N-gram Language Models
categories: Analytics
---

## 原理

语言模型（language model，LM）在自然语言处理中占有重要的地位，尤其在基于统计模型的相关研究中得到了广泛的应用。目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因数据缺乏而必须采取平滑（smoothing）算法。

### n元语法

一个语言模型通常构建为字符串s的概率分布p(s)，这里p(s)试图反映的是字符串作为一个句子出现的频率。

对于一个由l个基元（”基元“可以为字、词或短语等，为了表述方便，以后我们只用”词“来通指）构成的句子 $$s=\w_1 \w_2 ... \w_l$$，其概率计算公式可以表示为

$$\begin{align}
p(s)&=p(\w_1)p(\w_2 \mid \w_1)p(\w_3 \mid \w_1 \w_2) ... p(\w_l \mid \w_1 ... \w_{l-1}) \\
&= \prod_{i=1}^l p(\w_i \mid \w_1 ... \w_{i-1}) \\
\end{align}$$

在上式总，产生第$$i(1 \leq i \leq l)$$个词的概率是由已经产生的i-1个词$$\w_1 \w_2 ... \w_{i-1}$$。一般地，我们把前i-1个词$$\w_1 \w_2 ... \w_{i-1}$$称为第i个词的”历史（history）“。在这种计算方法中，随着历史长度的增加，不同的历史数目按指数级增长。如果历史长度为i-1，那么，就有$$L^{i-1}$$种不同的历史（假设L为词汇集的大小），而我们必须考虑在所有$$L^{i-1}$$种不同的历史情况下，产生第i个词的概率。这样的话，模型中就有$$L^i$$个自由参数$$p(\w_i \mid \w_1, \w_2 ... \w_{i-1})$$。假设L=5000，i=3，那么自由参数的数目就是1250亿个。这使我们几乎不可能从训练数据中正确地估计出这些参数，实际上，绝大多数历史根本不可能在训练数据中出现。因此，为了解决这个问题，可以将历史$$\w_1, \w_2 ... \w_{i-1}$$按照某个法则映射到等价类$$E(\w_1 \w_2 ... \w_{i-1})$$，而等价类的数目远远小于不同历史的数目。如果假定：

$$p(\w_i \mid \w_1, \w_2 ... \w_{i-1}) = p(\w_i \mid E(\w_1, \w_2 ... \w_{i-1}))$$

那么，自由参数的数目就会大大减少，有很多方法可以将历史划分成等价类，其中，一种比较实际的做法是，将两个历史$$\w_{i-n+2} ... \w_{i-1} \w_i$$和$$v_{k-n+2}...v_{k-1}v_k$$映射到同一个等价类，当且仅当这两个历史最近的$$n-1(1 \leq n \leq l)$$个词相同，即如果$$E(\w_1 \w_2 ... \w_{i-1} \w_i) = E(v_1 v_2 ... v_{k-1} v_k)$$，当且仅当$$(\w_{i-n+2} ... \w_{i-1} \w_i) = (v_{k-n+2}...v_{k-1}v_k)$$。

满足上述条件的语言模型称为n元语法或n元文法（n-gram）。通常情况下，n的取值不能太大，否则，等价类太多，自由参数过多的问题仍然存在。在实际应用中，取n=3的情况较多。当n=1时，即出现在第i位上的词$$\w_i$$独立于历史，一元文法被记作unigram，或uni-gram，或monogram；当n=2时，即出现在第i位上的词$$\w_i$$仅与它前面一个历史词$$\w_{i-1}$$有关，二元文法模型被称为一阶马尔科夫链（Markov chain），记作bigram或bi-gram；当n=3时，即出现在第i位置上的词$$\w_i$$仅与它前面的两个历史词$$\w_{i-2} \w_{i=1}$$有关，三元文法模型被称为二阶马尔科夫链，记作trigram或tri-gram。

以二元语法模型为例，根据前面的解释，我们可以近似地认为，一个词的概率只依赖于它前面的一个词，那么

$$p(s) = \prod_{i=1}^l p(\w_i \mid \w_1 ... \w_{i-1}) \approx \prod_{i=1}^l p(\w_i \mid \w_{i-1})$$

为了使得$$p(\w_i \mid \w_{i-1})$$对于i=1有意义，我们在句子开头加上一个句首标记<BOS>，即假设$$\w_0$$就是<BOS>。另外，为了使得所有的字符串的概率之和$$\w_s p(s)=1$$，需要在句子结尾再放一个句尾标记<EOS>，并且使之包含在上述乘积中（如果不做这样的处理，所有给定长度的字符串的概率和为1，而所有字符串的概率和为无穷大）。

为了估计$$p(\w_i \mid \w_{i-1})$$条件概率，可以简单地计算二元语法$$w_{i-1}w_i$$在某一文本中出行的频率，然后归一化。如果用$$c(w_{i-1}w_i)表示二元语法$$w_{i-1}w_i$$在给定文本中的出现次数，我们可以采用下面的计算公式：

$$p(\w_i \mid \w_{i-1}) = \frac{c(w_{i-1} w_i)}{\sum_{w_i}c(w_{i-1}w_i)}$$

