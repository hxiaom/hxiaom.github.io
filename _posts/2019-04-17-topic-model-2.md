---
layout: post
title: 【Method】Topic Model（二）pLSA
categories: Analytics
---

## 原理

概率隐性语义分析(Probabilistic latent semantic analysis, pLSA)采取概率方法替代LSA中的SVD以解决问题。

其核心思想是找到一个潜在主题的概率模型，该模型可以生成我们在“文档-术语”矩阵中观察到的数据。特别是，我们需要个模型P(D,W)，使得对于任何文档d和单词w，P(d,w)能对应于“文档-术语”矩阵中的那个条目。

让我们回想主题模型的基本假设：每个文档由多个主题组成，每个主题由多个单词组成。则在pLSA中

- 给定文档d，主题z以$$P(z \mid d)$$的概率出行在该文档中；
- 给定主题d，单词w以$$P(w \mid z)$$的概率从主题z中提取出来。20

![](/img/2019-04-17-topic-model-2-1.jpeg)

从形式上看，一个给定的文档和单词同时出行的联合概率是：

直观来说，等式右边告诉我们理解某个文档的可能性有多大；然后，根据该文档主题的分布情况，在该文档中找到某个单词的可能性有多大。

在这种情况下，P(D)、$$P(Z \mid D)$$、和$$P(W \mid Z)$$是我们模型的参数。P(D)可以直接由我们的语料库确定。$$P(Z \mid D)$$和$$P(W \mid Z)$$利用了多项式分布建模，并且可以使用期望最大化算法（EM）进行训练。EM无需进行算法的完整数学处理，而是一种基于未观测潜变量（此处指主题）的模型找到最可能的参数估计的方法。

有趣的是，P(D,W)可以利用不同的3个参数等效地参数化：

可以通过将模型看作一个生成过程来理解这种等价性。在第一个参数化过程中，我们从概率为P(d)的文档开始，然后用$$P(z \mid d)$$生成主题，最后用$$P(w \mid z)$$生成单词。而在上述这个参数化过程中，我们从P(z)开始，再用$$P(d \mid z)$$和$$P(w \mid z)$$单独生成文档。



## 参考文献

- [一文读懂如何用LSA、PSLA、LDA和lda2vec进行主题建模](https://www.sohu.com/a/234584362_129720)
- [NLP —— 图模型（三）pLSA（Probabilistic latent semantic analysis，概率隐性语义分析）模型](https://www.cnblogs.com/Determined22/p/7237111.html)