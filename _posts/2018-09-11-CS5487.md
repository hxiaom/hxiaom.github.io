---
layout: post
title: "CS5487 Machine Learning (Antoni Chan)"
categories: DataScience
---

## 1. Introduction

The field of pattern recognition is concerned with the **automatic disvery of regularities in data** through the use of computer algorithms and with the sue of these regularities to take actions such as classifying the data into different categories.

**training set** is used to tune the parameters of an adaptive model.

**target vector t**, which represents the identity of the corresponding digit.

The result of running the machine learning algorithm **can be expressed as a function y(x)** which takes a new digit image x as input and that generates an output vector y, encoded in the same way as target vectors.

The precise form of the function y(x) is determined during the training phase, also known as the **learning phase**, on the basis of the training data.

Once the model is trained it can then determine the identity of new digit images, which are said to comprise a **test set**.

The ability to categorize correctly new examples that differ from those used for training is known as **generalization**. generalization is a **central goal** in pattern recognition.

For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve. This **pre-processing stage is sometimes also called feature extraction**. Note that new test data must be pre-proessed using the same steps as the training data.

Pre-processing might also be performed in order to **speed up computation**. The aim is to find useful features that are fast to compute, and yet that also preserve useful discriminatory information. Because the number of such features is smaller than the number of pixels, this kind of pre-processing represents a form of **dimensionality reduction**. Care must be taken during pre-processing because often information is discarded, and if this information is important to the solution of the problem then the overall accuracy of the system can suffer.

Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as **supervised learning** problems. Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called **classification** problems. If the desired output consists of one or more continuous variables, then the task is called **regression**.

In other pattern recognition problems, the training data consists of a set of input vector x without any corresponding target values. The goal in such **unsupervised learning** problems may be to discover groups of similar examples within the data, where it is called **clustering**, or to determine the distribution of data within the input space, known as **density estimation**, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of **visualization**.

Finally, the technique of **reinforcement learning** (Sutton and Barto, 1998) is concerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward. Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them by **a process of trial and error**. A major challenge of reinforcement learning is **credit assignment problem**. A general feature of reinforcement learning is the **trade-off between exploration and exploitation**.

three important tools that will be used throughout the book, namely **probability theory, decision theory, and information theory**.

### 1.1 Example: Polynomial Curve Fitting
 
 individual observations are corrupted by random noise. This **noise** might arise from **intrinsically stochastic (i.e. random)** processes such as radioactive decay but more typically is due to **there being source of variability that are themselves unobserved**.

 **Probability theory**, discussed in Section 1.2, probides a framework for expressing such uncertainty in a precise and quantitative manner, and **decision theory**, discussed in Section 1.5, allows us to expoit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria.

 **Model comparison** or **model selection**

 the reason of over-fitting is becoming **increasingly tuned to the random noise on the target values**.

 the over-fitting problem become **less severe as the data set increases**. Another way to say this is that **the larger the data set, the more complex (in other words more flexible) the model that we can afford to fit to the data.**

 One rough heuristic that is sometimes advocated is that **the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model**. However,as we shall see in Chapter 3, the number of parameters is not necessarily the **most appropriate measure of model complexity**.

 Also, there is something rather unsatisfying about having to limit the number of parameters in a model according to **the size of the available training set**. It would seem more reasonable to choose the complexity of the model according to **the complexity of the problem being solved**.

 We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood (discussed in Section 1.2.5), and that the over-fitting problem can be understood as a general property of maximum likelihood. **By adopting a Bayesian approach, the over-fitting problem can be avoided**. We shall see that there is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points. Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.
 