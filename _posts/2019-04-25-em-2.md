---
layout: post
title: 【Method】EM算法（二）EM算法推导及收敛性
categories: Analytics
---

## 原理

我们面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）Y关于参数$$\theta$$的对数似然函数，即极大化

$$\begin{align}
L(\theta) &= log P(Y \mid \theta) = log \sum_Z P(Y,Z \mid \theta)\\
&= log(\sum_Z P(Y \mid Z, \theta) P(Z \mid \theta)) \\
\end{align}$$

注意到这一极大化的主要困难是上式中有未观测数据并有包含和（或积分）的对数。

事实上，EM算法是通过迭代逐步近似极大化$$L(\theta)$$的。假设在第i次迭代后$$\theta$$的估计值是$$\theta^{(i)}$$。我们希望新估计值$$\theta$$能使$$L(\theta)$$增加，即$$L(\theta) > L(\theta^{(i)})$$，并逐步达到极大值。为此，考虑两者的差值：

$$L(\theta) - L(\theta^{(i)}) = log(\sum_Z P(Y \mid Z, \theta) P(Z \mid \theta)) - log P(Y \mid \theta^{(i)})$$

利用Jensen不等式（$$log \sum_j \lamda_j y_j \geq \lamda_j log y_j,其中\lamda_j \geq 0, \sum_j \lamda_j =1$$）得到其下界：



## 参考文献

- 统计学习方法 李航 第9章