---
layout: post
title: 【Method】回归分析专题（二）二元被解释变量回归
categories: Analytics
---

我们经常使用二元变量做解释变量，没有造成任何问题。但当被解释变量是二元变量时，情况就变得复杂了：用一条直线去拟合取值只有0和1两个值的被解释变量，这是什么意思呢？

本章研究的二元被解释变量是一种取值有限的被解释变量，即受限被解释变量（limited dependent variable）。

## 二元被解释变量与线性概率模型

房屋抵押贷款申请能否被批准是二元被解释变量的一个例子，许多其他重要问题也常常涉及二元被解释变量。例如，学费补助对个人上大学有什么影响？青少年是否吸烟取决于什么因素？一国是否接受外国援助取决于什么因素？求职者是否成功取决于什么因素？在这些例子中，我们感兴趣的结果都是二元的：学生上大学或不上大学、青少年吸烟或不吸烟、一国接受或不接受外国援助、求职者得到或没有得到工作。

本节首先讨论二元被解释变量回归与连续被解释变量回归的区别，然后介绍一种最简单的二元被解释变量模型，即线性概率模型。

### 二元被解释变量

本章所用的实例是，种族是否是导致房屋抵押贷款申请被拒绝的因素。其中，二元被解释变量是房屋抵押贷款申请是否被拒绝。

我们首先考虑下面两组变量之间的关系：一个变量为二元被解释变量deny，当房屋抵押贷款申请被拒绝时取1，而被批准时取0。另一个变量为P/I ratio，该变量为连续变量，表示申请者预期月还款额与其收入之比。

应用于二元被解释变量的多元线性回归模型被称为线性概率模型；“线性”是指它是一条直线，而“概率模型”是指这一模型刻画了被解释变量等于1的概率。

### 线性概率模型

线性概率模型（linear probability model）是多元回归模型在被解释变量为二元变量时的称谓。由于被解释变量是二元变量，故总体回归函数表示给定X时被解释变量等于1的概率。解释变量X的稀疏$$\beta_1$$表示X变化1个单位所引起的Y=1的概率变化。同理，由回归方程估计得到的OLS预测值$$\hat{Y}_i$$表示被解释变量等于1的概率预测值，OLS估计值$$\hat{\beta}_1$$表示X变化1个单位所引起的Y=1的概率变化。

之前介绍的多元回归模型的工具几乎都可以应用到线性概率模型中。系数可以通过OLS进行估计。95%置信区间为系数估计值$$\pm 1.96$$标准误差，多个系数的假设检验可以使用F统计量，变量之间的交互作用可以使用解释变量的交互项方法进行建模。由于线性概率模型中的误差项通常都是异方差的，故有必要使用异方差-稳健标准误差进行推断。

一个无法照搬硬套的工具是$$R^2$$。当被解释变量为连续变量时，可以想象$$R^2=1$$的情形：所有的数据恰好都落在回归线上。但当被解释变量为二元变量时，这是不可能达到的，除非解释变量也是二元变量。因此，$$R^2$$在这里并不是一个特别有用的统计量。我们将在下节中讨论拟合优度。

线性概率模型

线性概率模型是多元线性回归模型在被解释变量$$Y_i$$为二元变量时的应用

$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + ... + \beta_k X_{ki} + u_i$$

由于Y是二元变量，则$$E(Y \mid X_1, X_2, ..., X_k) = P(Y=1 \mid X_1, X_2, ..., X_k)$$，因此对于线性概率模型，有

$$P(Y \mid X_1, X_2, ..., X_k) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + ... + \beta_k X_{ki}$$

回归系数$$\beta_1$$表示在保持其他解释变量不变的情况下，$$X_1$$变化1个单位所引起的Y=1的概率变化，关于$$\beta_2, ..., \beta_m$$的理解以此类推。回归系数可以通过OLS估计得到，且通常的（异方差-稳健）OLS标准误差可以用于置信区间的构造和假设检验。

线性概率模型的缺陷。线性特征使得线性概率模型易于使用，但同时也是该模型的主要缺陷。因为概率不可能大于1，因此给定X的变化对于Y=1概率的影响一定是非线性的。尽管P/I ratio从0.3到0.4的变化可能对被拒概率由较大的影响，但当P/I ratio很大以至于贷款申请非常容易被拒绝时，继续增加P/I ratio对被拒概率几乎没什么影响。但是在线性概率模型中，P/I ratio的变化对被拒概率的影响是固定的，这会导致概率预测值在P/I ratio非常低的时候小于0而在P/I ratio非常高的时候大于1！这是毫无意义的：概率不可能小于0或者大于1.这种荒谬的结论是线性回归不可避免的结果。为了解决这个问题，我们引入专门为二元被解释变量设计的非线性模型，即probit和logit模型。

## probit回归和logit回归

probit回归和logit回归是专门为二元被解释变量设计的非线性回归模型。由于二元被解释变量Y的回归是对Y=1的概率进行建模，因此采用能使预测值落在0~1的非线性模型才有意义。由于累积分布函数(c.d.f.)产生的概率介于0和1之间，因此可以将其应用到probit回归和logit回归中。其中，probit回归使用的是标准正态累积分布函数，logit回归，也称逻辑斯蒂回归（logistic regression），使用的是“逻辑斯蒂”累积分布函数。

### probit回归

一元probit回归。只包含一个解释变量X的probit回归模型为

P(Y=1 \mid X) = \Phi (\beta_0 + \beta_1 X)$$

式中，$$\Phi$$为标准正态累积分布函数。

在probit模型中，$$\beta_0 + \beta_1 X$$项扮演了标准正态累积分布表中的“z”的角色。

上式中的probit系数$$\beta_1$$表示X变化1个单位所引起的z值的变化。如果$$\beta_1$$为正，则X的增加会引起z值增加，从而使得Y=1的概率增加；如果$$\beta_1$$为负，则X的增加会引起Y=1概率的下降。尽管X对z值的影响是线性的，但它对概率的影响是非线性的。因此在实际应用中，对probit模型最简单的解释是使用一个或多个解释变量的值来计算概率预测值或概率预测值的变化。当只有一个解释变量时，概率预测值可以作为X的函数来画图。

多元probit回归。到目前为止，我们已经学过的所有回归中，如果遗漏与已有解释变量相关的Y的决定因素将会导致遗漏变量偏差。probit回归也不例外。在线性回归中解决这问题的方法是把这些因素作为解释变量加入回归模型中，这同样也是probit模型解决遗漏变量偏差的方法。

多元probit模型是一元probit模型的拓展，它加入了其他用于计算z值的解释变量。相应地，包含两个解释变量$$X_1$$和$$X_2$$的probit总体回归模型为

$$P(Y=1 \mid X_1, X_2) = \Phi (\beta_0 + \beta_1 X_1 + \beta_2 X_2)$$

X变化的效应。一般情况下，回归模型可以用来计算由X的变化所引起的Y的期望变化。当Y为二元变量时，它的条件期望即为取值等于1时的条件概率，故由X变化引起的Y的期望变化是指Y=1的概率变化。

当总体回归函数为X的非线性函数时，通过以下三步可估计得到Y的期望变化：首先，用估计的回归函数计算X取初始值的预测值；接下来，计算当X变化为$$X+\delta X$$时的预测值；最后，计算两个预测值之间的差值。无论非线性模型多么复杂，这种方法总能计算出X变化的效应预测值。当把这种方法应用到probit模型中时，可以得到X的变化对Y=1的概率的效应估计。

probit模型、概率估计和效应估计

多元总体probit模型为

$$P(Y=1 \mid X_1, X_2, ..., X_k) = \Phi (\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k)$$

其中，被解释变量Y为二元变量，$$\Phi$$为标准正态累积分布函数，$$X_1, X_2$$等为解释变量。该模型最好通过计算概率预测值和解释变量的效应来进行解释。

给定$$X_1, X_2, ..., X_k$$时Y=1的概率预测值可以通过计算z值得到，其中$$z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k$$，然后再正态分布表中查找该z值所对应的值即可。

系数$$\beta_1$$表示在保持$$X_2, ..., X_k$$不变的情况下，$$X_1$$变化1个单位所引起的z值的变化情况。

解释变量的变化对概率预测值产生的影响可通过下述方式计算得到：(1)计算解释变量取初始值时的概率预测值；(2) 计算解释变量取新值或变化后所对应的概率预测值；(3) 计算两个概率预测值之差。

probit系数的估计。上述probit系数是通过最大似然法估计得到的，这种方法在很多实证应用中都可以得到有效估计量（最小方差），包括被解释变量为二元变量的回归。在大样本下，最大似然估计量是一致的且服从正态分布，故可以使用常用的方法来构造系数的t统计量和置信区间。

估计probit模型的统计软件通常使用最大似然估计法，因此这在实际应用中是一种很简便的方法。与回归系数的标准误差一样，我们可以同样的方式来使用软件给出标准误差。例如，probit系数真值的95%置信区间可以用系数估计值$$\pm 1.96$$标准误差来进行构造。同理，使用最大似然估计量计算得到的F统计量也可用于检验联合假设。

### logit回归

logit回归模型。logit回归与probit回归非常相似，区别在于logit回归将标准正态累积分布函数$$\Phi$$替换为标准逻辑斯蒂累积分布函数，我们用F表示该函数。逻辑斯蒂累积分布函数的具体函数形式被定义为指数函数形式。

logit回归

二元被解释变量Y的多元总体logit模型为

$$P(Y=1 \mid X_1, X_2,...,X_k) = F(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k) = \frac{1}{1+e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k)}}$$

除了累积分布函数不同外，logit回归于probit回归非常相似。

与probit一样，最好使用概率预测值和概率预测值之间的差异来解释logit回归系数。

logit模型的系数也可以通过最大似然法进行估计。在大样本下，最大似然估计量是一致的且服从正态分布的，因此可以使用常规方法构造系数的t统计量和置信区间。

logit回归函数与probit回归函数非常相似。根据历史经验，使用logit回归的主要原因是计算逻辑斯蒂累积分布函数的速度要快于正态累积分布函数。但随着计算机计算效率的提高，这种区别就不再重要了。

### 线性概率模型、probit模型和logit模型的比较

线性概率模型、probit模型和logit模型这三种模型都只是对未知总体回归函数$$E(Y \mid X)=P(Y=1 \mid X)$$的近似。其中，线性概率模型最容易使用也最容易理解，但它无法反映真实总体回归函数的非线性特征。probit和logit回归模型都能模拟概率中的这种非线性关系，但它们的回归系数难以解释。那么，在事件中我们应该选择哪一种模型呢？

不存在唯一的正确答案，不同的研究人员会使用不同的模型。probit和logit模型常常能得到相似的结果。就实践目的而言，这两个估计值非常接近。选择logit还是probit模型的一种方法是，选择你使用的统计软件最容易实现的那个模型。

线性概率模型对非线性总体回归函数的近似最不合理。尽管如此，当某些数据集中的解释变量很少出现的极端值时，线性概率模型依然能够提供一个充分的近似。

## logit模型和probit模型的估计与推断

probit和logit回归函数是关于系数的非线性函数。也就是说，probit系数$$\beta_0, \beta_1, ..., \beta_k$$出现在标准正态累积分布函数$$\Phi$$中，logit系数出现在标准逻辑斯蒂累积分布函数F中。由于总体回归函数是系数$$\beta_0, \beta_1, ..., \beta_k$$的非线性函数，故这些系数无法通过OLS进行估计。

本节介绍probit和logit模型系数估计的标准方法，即最大似然估计方法。由于现代统计软件都已嵌入这种方法，故实践中probit和logit系数的最大似然估计很容易得到。然而，最大似然估计的理论要比最小二乘理论更加复杂。因此在介绍最大似然方法之前，我们先讨论另一种估计方法，即非线性最小二乘法。

### 非线性最小二乘估计

当总体回归函数是关于参数的非线性函数，如probit回归系数，非线性最小二乘法是估计这类未知参数的一般方法。与OLS一样，非线性最小二乘法也是要寻找使得模型预测误差平方和最小的参数值。

具体地，考虑probit模型系数的非线性最小二乘估计量。给定X时Y的条件期望为$$E(Y \mid X_1, ..., X_k) = P(Y=1 \mid X_1, ..., X_k) = \Phi (\beta_0 + \beta_1 X_1 + ... + \beta_k X_k)$$。probit系数的非线性最小二乘估计量为使下式所示的预测误差平方和达到最小时$$b_0, ...,b_k$$的取值：

$$\sum_{i=1}^n [Y_i - \Phi(b_0 + b_1 X_{1i} + ... + b_k X_{ik})]^2$$

非线性最小二乘估计量和线性回归中的OLS估计量具有两点相同的重要性质：一致性（当样本容量增大时，参数估计量趋于真值的概率接近于1）和大样本下服从正态分布。虽然非线性最小二乘估计量具有这样优良性质，但仍存在比非线性最小二乘估计量方差更小的其他估计量，即非线性最小二乘估计量是非有效的。出于这个原因，实践中很少采用probit系数的非线性最小二乘估计量，而是使用最大似然估计量。

### 最大似然估计

似然函数（likelihood function）是抽样的联合概率分布，是未知系数的函数。未知系数的最大似然估计量（maximum likelihood estimator，MLE）是使似然函数达到最大时所计算得到的系数值。由于最大似然估计量选择了使似然函数即联合概率分布达到最大的未知系数，故实际上最大似然估计量选择的是使n个样本数据被抽中的概率达到最大的系数值。在这个意义上，MLE是那些“最可能”生成这些数据的系数值。

与非线性最小二乘估计量一样，MLE估计量是一致的且在大样本下服从正态分布。由于回归软件通常计算的是probit系数的MLE估计量，所以在实践中使用这种估计量是非常便捷的。

基于MLE的统计推断。由于MLE估计量在大样本下服从正态分布，故probit和logit模型基于MLE估计量的统计推断与线性回归模型中基于OLS估计量的推断是一样的，即利用t统计量进行统计推断，且95%置信区间为系数的$$\pm 1.96$$标准误差。而多个系数的联合假设检验则使用F统计量，这与线性回归模型所使用的方法非常相似。由此可见，所有这些统计推断都与线性回归模型中的统计推断类似。

在实际应用中需要注意的一点是，对于联合假设检验看，某些统计软件使用的是F统计量，而其他软件使用的是$$\chi^2$$统计量。$$\chi^2$$统计量为$$q \times F$$，其中q为待检验的约束个数。在原假设下，由于F统计量在大样本下服从$$\chi^2/q$$分布，因此$$q \times F$$在大样本下服从$$\chi_q^2$$分布。由于这两种方法唯一的区别在于是否除以q，故它们提供的统计推断其实是相同的，但是你必须了解你的统计软件使用的是哪一种方法以便查找正确的临界值。

### 拟合优度

我们曾提到对于线性概率模型，$$R^2$$是一个非常糟糕的拟合优度。对于probit和logit模型而言，同样如此。适用于二元被解释变量模型的两种拟合优度是“正确预测的比例”和“伪$$R^2$$”。其中，正确预测的比例（fraction correctly predicted）遵循如下规则：如果$$Y_i =1$$且概率预测值高于50%，或$$Y_i=0$$且概率预测值低于50%，那么称$$Y_i$$是正确预测的，否则称$$Y_i$$是错误预测的。“正确预测的比例”是指n个观测值$$Y_1,...,Y_n$$中预测正确的比例。

这种拟合优度的优点在于容易理解，缺点在于它没有反映出预测的质量：如果$$Y_i=1$$，则概率预测值无论是51%还是90%，都被认为是正确预测的。

伪$$R^2$$(pseudo-$$R^2$$)是使用似然函数来度量模型的拟合程度。由于MLE使似然函数值达到最大，故在probit和logit模型中加入其它解释变量会使似然函数值变大，这与增加解释变量会使线性回归的残差平方和减小一样。这表明，可以通过比较包含全部解释变量的最大似然函数值与未包含任何解释变量的最大似然函数值来度量probit模型的拟合效果。实际上，这就是所谓的伪$$R^2$$。