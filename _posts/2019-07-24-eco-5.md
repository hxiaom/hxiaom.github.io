---
layout: post
title: 【Method】回归分析基础（二）一元线性回归：假设检验和置信区间
categories: Analytics
---

## 关于某个回归系数的假设检验

一般来说，t统计量具有如下形式

$$t = \frac{估计量-假设值}{估计量的标准误差}$$

### $$\beta_1$$的双边假设

对系数$$\beta_1$$进行假设检验的一般方法与对总体均值进行假设检验的方法相同。因此我们首先回归一下关于总体均值的检验

Y的均值等于某一特定值$$\mu_{Y,0}$$的原假设可以表示为$$H_0: E(Y) = \mu_{Y,0}$$，而双边备择假设为$$H_1: E(Y) \neq \mu_{Y,0}$$。

对原假设$$H_0$$的检验可以按照三个步骤进行。第一步是计算$$\bar{Y}$的标准误差$$SE(\bar{Y})$$，它是$$\bar{Y}$$抽样分布标准差的估计量；第二步是计算t统计量；第三步是计算p值，它是基于实际观测到的检验统计量得到的能够拒绝原假设的最低显著性水平。

斜率$$\beta_1$$的假设检验。从理论上讲，上述关于总体均值的检验过程准确有效的关键之处在于：在大样本下，$$\bar{Y}$$的抽样分布是渐近正态的。因为在大样本中，$$\hat{\beta}_1$$也服从正态分布，所以我们也可以利用上述方法来检验关于斜率$$\beta_1$$真值的假设。

在进行检验之前，我们需要明确列示出原假设和备择假设，如原假设为解释变量对被解释变量无影响$$\beta_1=0$$。更一般地，原假设是总体斜率$$\beta_1$$的真值等于某个特定值$$\beta_{1,0}$$，双边备择假设为$$\beta_1$$不等于$$\beta_{1,0}$$。即原假设和双边备择假设分别为：

$$H_0: \beta_1 = \beta_{1,0}  and H_1: \beta_1 \neq \beta_{1,0}$$

为了检验原假设$$H_0$$，我们采用与总体均值检验相同的三个步骤。

第一步，计算$$\hat{\beta}_1$$的标准误差$$SE(\hat{\beta}_1)$$。它是$$\hat{\beta}_1$$的抽样分布的标准差$$\sigma_{\hat{\beta}_1}$$的估计量。具体地

$$SE(\hat{\beta}_1) = \sqrt{\sigma_{\hat{\beta}_1}^2}$$

其中

$$\sigma_{\hat{\beta}_1}^2 = \frac{1}{n} \times \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}_i^2}{[\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2]}$$

尽管$$\sigma_{\hat{\beta}_1}^2$$的公式很复杂，但在实际应用中我们通常利用回归软件来计算，所以使用起来比较容易。

第二步，计算t统计量

$$t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}$$

第三步，计算p值，即在原假设成立的条件下，得到与$$\beta_{1,0}$$距离至少和实际计算的估计值$$\hat{\beta}_1^{act}$$与$$\beta_{1,0}$$的距离相同的$$\hat{\beta}_1$$的概率。其数学表达式为

$$\begin{align}
p-value &= P_{H_0} [\mid \hat{\beta}_1 - \beta_{1,0} \mid > \mid \hat{\beta}_1^{act} - \beta_{1,0} \mid] \
&= P_{H_0} [\mid \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \mid > \mid \frac{\hat{\beta}_1^{act} - \beta_{1,0}}{SE(\hat{\beta}_1)} \mid] \
&= P_{H_0} (\mid t \mid > \mid t^{act} \mid) \
\end{align}$$

其中，$$P_{H_0}$$表示在原假设成立的条件下计算得到的概率，第二个等式是由上式同除以$$SE(\hat{\beta}_1)$$得到的。因为在大样本下$$\hat{\beta}_1$$服从渐近正态分布，因此在原假设下，t统计量近似服从标准正态分布，故在大样本下有

$$p-value = P(\mid Z \mid > \mid t^{act} \mid) = 2 \phi (- \mid t^{act} \mid)$$

若p值小于5%，则意味着在原假设下，得到至少和实际观测值与原假设距离相同的$$\hat{\beta}_1$$值的概率小于5%，从而提供了反对原假设的证据，即在5%的显著性水平下拒绝原假设。

另一种方法是在5%的显著性水平下，只需比较t统计量的绝对值和双边检验的临界值1.96就可以检验原假设，若$$\mid t^{act} \mid >1.96$$，则在5%的显著性水平下拒绝原假设。

### $$\beta_1$$的单边假设

有时候使用单边假设检验更合适也更有意义。

对于单边检验，其原假设和单边备择假设分别为：

$$H_0: \beta_1 = \beta{1.0} and H_1: \beta_1 < \beta_{1,0}$$

因为单边和双边假设检验的原假设均相同，故t统计量的构造也一样。两者之间的唯一区别在于如何解释t统计量。

单边检验的p值可以通过标准正态分布的累积分布函数得到，即

$$p-value = P(Z < t^{act}) = \phi (t^{act}) (p值，单边左尾检验）$$

如果备择假设为$$\beta_1$$大于$$\beta_{1,0}$$，则上式不等号方向相反，故p值为右尾概率$$P(Z > t^{act})$$。

什么时候使用单边检验？在实际应用中，只有存在明确的理由时，我们才使用单边备择假设。这个理由可能来自经济理论、先验证据或者二者皆有。然而，即使一开始认为相关备择假设可能是单边的，但仔细考虑后却未必如此。

### 截距$$\beta_0$$的假设检验

我们偶尔也会关心截距$$\beta_0$$的假设检验。关于截距的原假设和双边备择假设分别为

$$H_0: \beta_0 = \beta_{0,0}; H_1: \beta_0 \neq \beta_{0,0}$$

检验这一原假设的一般方法是将上节中的三个步骤应用于$$\beta_0$$。

如果你脑海中有一个原假设，则假设检验是很有用的。基于统计证据接受或拒绝原假设的方法，为我们提供了利用样本了解总体时处理内在不确定性的强大工具。然而，很多时候我们并不知道回归系数对于的具体假设是什么，于是我们需要了解回归系数的取值区间，即需要构造置信区间。


## 回归系数的置信区间

由于斜率$$\beta_1$$的任何估计结果都存在不确定性，因此我们不能仅通过一组样本数据来准确推测$$\beta_1$$的真值。然而，我们可以利用OLS估计量及其标准误差来构造斜率$$\beta_1$$或截距$$\beta_0$$的置信区间。

$$\beta_1$$的置信区间。$$\beta_1$$的95%置信区间（confidence interval）有两个等价的定义。其一是在5%的显著性水平下利用双边假设检验不能拒绝的$$\beta_1$$的取值集合。其二是以95%的概率包含$$\beta_1$$真值的区间，即抽取的可能样本中有95%的样本构造的置信区间中包含了$$\beta_1$$的真值。因为在95%的所有样本的置信区间中都包含了真值，故称其具有95%的置信水平（confidence level）。

这两个定义等价的原因是：根据定义，5%的显著性水平下的假设检验只能在5%的所有可能样本中拒绝$$\beta_1$$的真值，即在95%的所有可能样本中不会拒绝$$\beta_1$$的真值。又因为95%置信区间（根据第一种定义）是在5%显著性水平下不能拒绝的所有$$\beta_1$$的取值集合，故说明95%的所有可能样本的置信区间都包含了$$\beta_1$$的真值。

当假设值$$\beta_{1,0}$$落在$$\hat{\beta}_1 \pm 1.96 WE(\hat{\beta}_1)$$范围之外时，t统计量将拒绝假设值$$\beta_{1,0}$$，故我们得到一种构造置信区间的简易方法，即$$\beta_1$$的95%置信区间为$$[\hat{\beta}_1 - 1.96 SE(\hat{\beta}_1), \hat{\beta}_1 + 1.96 SE(\hat{\beta}_1)]$$。这一方法与构造总体均值置信区间的方法类似。

$$\beta_0$$的置信区间。$$\beta_0$$的95%置信区间构造方法类似于$$\beta_1$$的置信区间构造方法，只需用$$\hat{\beta}_0$$和$$SE(\hat{\beta}_0)$$分别代替$$\hat{\beta}_1$$和$$SE(\hat{\beta}_1)$$

X变化的预期效应的置信区间。$$\beta_1$$的95%置信区间可以用于构造X的变化所引起的预期效应的95%置信区间。

考虑X一定量的变化$$\delta x$$，则对应的Y的预期变化为$$\beta_1 \delta x$$。尽管斜率$$\beta_1$$是未知的，但因为我们可以构造$$\beta_1$$的置信区间，所以我们也能构造预期效应$$\beta_1 \delta x$$的置信区间。

$$\beta_1 \delta x 的95%置信区间 = [(\hat{\beta}_1 - 1.96SE(\hat{\beta}_1)) \delta x, (\hat{\beta}_1 + 1.96SE(\hat{\beta}_1)) \delta x]$$

## X为二元变量时的回归

到目前为止的讨论均集中在解释变量为连续的情形。回归分析也可以用于解释变量是二元变量的情形，即解释变量只能取0或1两个值。例如，X可能表示工人的性别（女性取1，男性取0）。二元变量有时也称作指示变量（indicator variable）或虚拟变量（dummy variable）。

回归系数的解释。二元解释变量的回归模型结构同连续型解释变量的结构一样，但对$$\beta_1$$的解释却有所不同。

为了理解这一点，设有一变量$$D_i$$，其值等于0还是1取决于学生-教师比是否低于20，即$$D_i=1$$，若第i个学区的学生-教师比<20；$$D_i=0$$，若第i个学区的学生-教师比>=20；

以$$D_i$$为解释变量的总体回归模型为

$$Y_i = \beta_0 + \beta_1 D_i + u_i, i=1,...,n$$

除了解释变量是二元变量$$D_i$$之外，该模型与连续型解释变量$$X_i$$的回归模型相同。因为$$D_i$$是不连续的，故将$$\beta_1$$看作斜率没什么用处，因为$$D_i$$只能取两个值，所以不存在“直线”，从而讨论斜率也就没有任何意义。所以我们简单地将$$\beta_1$$看作是回归中乘以$$D_i$$的系数(coefficient multiplying $$D_i$$)，或简称为$$D_i$$的系数（coefficient on $$D_i$$)。

在含有二元解释变量的回归中，解释$$\beta_0$$和$$\beta_1$$的最好方法是分别讨论$$D_i=0$$和$$D_i=1$$的情形。

在测试成绩的例子中，$$\beta_1$$表示学生-教师比较低的学区的平均测试成绩与学术-教师比较高学区的平均测试成绩之差。

由于$$\beta_1$$为总体均值之差，所以可将OLS估计量$$\hat{\beta}_1$$看作是两组抽样的$$Y_i$$的样本均值之差。

假设检验和置信区间。如果两总体均值相同，则$$\beta_1$$为零。则两总体均值相同的原假设可以通过检验原假设$$\beta_1=0$$和备择假设$$\beta_1 \neq 0$$来进行。我们可以利用t统计量来检验这一假设。具体而言，当OLS的t统计量绝对值大于1.96时，在5%的显著性水平下，我们拒绝原假设。类似地，$$\hat{\beta}_1$$的95%置信区间$$\hat{\beta}_1 \pm 1.96 SE(\hat{\beta}_1)$$给出了两总体均值之差的95%置信区间。

## 异方差和同方差

上文关于误差项$$u_i$$分布的唯一假设是：给定$$X_i$$时，其均值为零（第一个最小二乘假设）。进一步讲，如果$$u_i$$的方差不依赖于$$X_i$$，则称误差项是同方差的。本节将讨论同方差的含义，同方差条件下OLS估计量标准误差的简化公式，以及在实践中应用这些简化公式时存在的风险。

### 什么是异方差和同方差

异方差和同方差的定义。若对任意的i=1,...,n，给定$$X_i$$时$$u_i$$的方差$$Var(u_i \mid X_i = x)$$为常数且不依赖于$$X_i$$，则称误差项$$u_i$$是同方差的（homoskedastic）。否则，误差项便是异方差的（heteroskedastic）。

### 同方差条件下的数学性质

OLS估计量仍然是无偏的且渐近正态的。由于最小二乘假设对条件方差没有限制，故它们适用于异方差的情形，也适用于同方差的情形。因此，即使误差是同方差的，OLS估计量仍然是无偏且一致的。另外，即使误差是同方差的，OLS估计量在大样本下仍然服从正态分布。不论误差项是同方差还是异方差，OLS估计量都是无偏、一致和渐近正态的。

同方差条件下OLS估计量的有效性。如果最小二乘假设成立且误差是同方差的，则OLS估计量$$\hat{\beta}_0$$和$$\hat{\beta}_1$$在所有线性估计量中是有效且无偏的。这一结论称作高斯-马尔可夫定理。

同方差适用的方差公式。如果误差项是同方差的，则我们可以简化$$\hat{\beta}_0$$和$$\hat{\beta}_1$$的方差公式。因此，当误差项为同方差时，$$\hat{\beta}_0$$和$$\hat{\beta}_1$$的标准误差具有专门的公式。$$\hat{\beta}_1$$的同方差适用的标准误差（homoskedasticity-only standard error）为$$SE(\hat{\beta}_1) = \sqrt{\tilde{\sigma}_{\hat{\beta}_1}^2}$$，其中$$\tilde{\sigma}_{\hat{\beta}_1}^2$$为$$\hat{\beta}_1$$的方差的同方差适用估计量。

$$\tilde{\sigma}_{\hat{\beta}_1}^2 = \frac{s_{\hat{u}}^2}{\sum_{i=1}^n (X_i - \bar{X})^2}$$

因为这些公式都是在误差项为同方差的特殊条件下推导出来的，且不适用于误差项为异方差的情形，所以称它们是OLS估计量的方差和标准误差的“同方差适用”公式。正如其名称所示，若误差项为异方差，则同方差适用的标准误差便不适用。当误差项为异方差时，利用同方差适用的标准误差计算的t统计量即使在大样本下也不服从标准正态分布。事实上，用同方差适用的标准误差计算的t统计量需要用到的确切的临界值取决于异方差的精确性质，因此无法给出这些临界值。类似地，如果误差项是异方差，一般情况下，即使在大样本情况下，我们利用$$\pm 1.96$$倍的同方差使用的标准误差来构造的置信区间包含参数真值的概率也不再是95%。

相反地，因为同方差是异方差的一种特殊情形，所以不论误差项是同方差还是异方差，关于$$\hat{\beta}_1$$和$$\hat{\beta}_0$$的方差估计量$$\hat{\sigma}_{\hat{\beta}_1}^2$$和$$\hat{\sigma}_{\hat{\beta}_0}^2$$的统计推断都是有效的。所以不论误差项是否异方差，基于这些标准误差的假设检验和置信区间都是有效的。这些公式被称为Eicker-Huber-White标准误差。

### 实际应用中的具体含义

异方差和同方差，哪一个更常见？对于这一问题的回答取决于实际应用。

异方差存在于很多计量经济学应用中。一般情况下，经济学理论很少给出误差项是同方差的理由。因此，除非你有充分的理由说明误差项是同方差的，否则应该谨慎一点，即假设它是异方差的。

经验做法。在实际问题分析中，我们应该使用异方差-稳健标准误差还是同方差适用的标准误差呢？在此给出一个经验做法。让我们设想一下：我们可以先计算二者，然后再在它们之间做选择。如果同方差适用的标准误差和异方差-稳健标准误差相同，则我们可以使用异方差-稳健标准误差，这不会影响结果；然而，如果它们不同，则我们更应该使用允许存在异方差的更为可靠的标准误差。所以，现实中最简单的方法就是一直使用异方差-稳健标准误差。

由于历史原因，很多软件程序将同方差使用的标准误差作为其默认设置，所以需要使用者自己主动选择异方差-稳健标准误差的选项。

## 普通最小二乘的理论基础

OLS估计量是无偏的且一致的，其方差与n成反比，且当样本容量较大时服从正态分布。另外，在一些假设条件下，OLS估计量比某些其他估计量更有效。特别是当最小二乘假设成立且误差项为同方差时，OLS估计量在所有线性无偏估计量中具有最小方差。这是高斯-马尔可夫定理的基本内容，本节对其进行解释和讨论。本节最后讨论了当高斯-马尔可夫条件不成立的情况下比OLS更有效的其他估计量。

### 线性无偏估计量和高斯-马尔可夫定理

如果三个最小二乘假设成立，且误差项为同方差，则OLS估计量在所有线性无偏估计量中具有最小方差。换句话说，OLS估计量是最佳线性无偏估计量（best linear conditionally unbiased estimator，BLUE）。该结论是关于“样本均值$$\bar{Y}$$是总体均值的所有无偏估计量中最有效的估计量”这一结论的推广。

线性无偏估计量。$$\beta_1$$的线性无偏估计量是由所有满足以下条件的估计量组成的：它们是$$Y_1, ..., Y_n$$的线性函数，且在给定$$X_1,...,X_n$$时均具有无偏性。即如果$$\tilde{\beta}_1$$是一个线性估计量，则它可以表示为

$$\tilde{\beta}_1 = \sum_{i=1}^n a_i Y_i$$

其中，权重$$a_1, ..., a_n$$依赖于$$X_1, ..., X_n$$，但不依赖于$$Y_1,...,Y_n$$。若给定$$X_1, ..., X_n$$时，$$\tilde{\beta}_1$$的期望值为$$\beta_1$$，则称估计量$$\tilde{\beta}_1$$是条件无偏的。即若

$$E(\tilde{\beta}_1 \mid X_1, ..., X_n) = \beta_1$$

则估计量$$\tilde{\beta}_1$$是条件无偏的。

高斯-马尔可夫定理(Gauss-Markov theorem)指出，在满足一系列高斯-马尔可夫条件下，给定$$X_1, ..., X_n$$时，OLS估计量$$\hat{\beta}_1$$在$$\beta_1$$的所有线性无偏估计量中具有最小方差，即OLS估计量为BLUE。高斯-马尔可夫条件可由三个最小二乘假设及误差项为同方差假设推得。因此，若三个最小二乘假设成立且误差项为同方差，则OLS估计量为BLUE。

高斯-马尔可夫定理的局限性。高斯-马尔可夫定理为使用OLS方法提供了理论依据。然而这一定理具有两个主要的局限性：第一，其条件在实际应用中可能不成立，特别是实际经济应用中的模型误差项通常是异方差，此时OLS估计量不再是BLUE。虽然异方差的存在并不影响基于异方差-稳健标准误差的推断，但它却意味着OLS估计量不再是有效线性无偏估计量。如果我们已知异方差的具体形式，便可以利用加权最小二乘估计量来代替OLS估计量。

高斯-马尔可夫定理的第二个局限性是，即使定理的条件成立，但还可能存在其他的非线性无偏估计量，并且在某些条件下，这些估计量比OLS估计量更有效。

### 不同于OLS的回归估计量

在一定条件下，某些回归估计量比OLS更有效。

加权最小二乘估计量。如果误差项是异方差，则OLS估计量不再是BLUE。如果已知异方差的具体形式，则可以构造一个比OLS估计量方差更小的估计量。