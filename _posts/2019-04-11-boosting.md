---
layout: post
title: 【Method】Boosting（一）AdaBoost
categories: Analytics
---

## 原理

提升（boosting）方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

### Boosting的基本思路

提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠订个诸葛亮”的道理。

历史上，Kearns和Valiant首先提出了“强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确（probably approximately correct，PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

这样一来，问题便成为，在学习中，如果已经发行了“弱学习算法”，那么能否将它提升（boosting）为“强学习算法”。大家知道，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具代表性的是AdaBoost算法。

对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易得多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。

这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。

AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里。

### AdaBoost

现在叙述AdaBoost算法。假设给定一个二分类的训练数据集

$$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$

其中，每个样本点由实例与标记组成。实例$$x_i \in X \in R^N$$，标记$$y_i \in Y ={-1, +1}$$，X是实例空间，Y是标记集合。AdaBoost利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。

输入：训练数据集$$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$，其中$$x_i \in X \in R^N, y_i \in Y ={-1, +1}$$；弱学习算法；
输出：最终分类器G(x).

(1) 初始化训练数据的权值分布

$$D_1=(w_11,...,w_1i,...,w_1N), w_1i=\frac{1}{N}, i=1,2,...,N$$

(2) 对m=1,2,...,M
    
(a) 使用具有权值分布$$D_m$$的训练数据集学习，得到基本分类器

$$G_m(x): X \to {-1,+1}$$

(b) 计算$$G_m(x)$$在训练数据集上的分类误差率

$$e_m=P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i)\neq y_i)$$

(c) 计算$$G_m(x)$$的系数

$$\alpha_m = \frac{1}{2} ln \frac{1-e_m}{e_m}$$

(d) 更新训练数据集的权值分布

$$ D_{m+1}=(w_{m+1,1}, ..., w_{m+1,i},..., w_{m+1,N})$$

$$ w_{m+1,i} = \frac{w_{mi}}{Z_m} exp(-\alpha_m y_i G_m(x_i)), i=1,2,...,N$$

这里，$$Z_m$$是规范化因子

$$Z_m = \sum_{i=1}{N} w_{mi} exp(-\alpha_m y_i G_m(x_i)) $$

它使$$D_{m+1}$$成为一个概率分布。

(3) 构建分布分类器的线性组合

$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)$$

得到最终分类器

$$G(x)=sign(f(x)) = sign(\sum_{m=1}^M \alpha_m G_m(x))$$

对AdaBoost算法作如下说明：

步骤（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$$G_1(x)$$

步骤（2）AdaBoost反复学习基本分类器，在每一轮m=1,2,...,M顺次执行下列操作：

（a）使用当前分布$$D_m$$加权的训练数据集，学习基本分类器$$G_m(x)$$

（b）计算基本分类器$$G_m(x)$$在加权训练数据集上的分类误差率：



## 参考材料

- 统计学习方法 李航 第8章 提升方法