---
layout: post
title: 【Method】决策树（二）特征选择
categories: Analytics
---

## 原理

特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。

### 信息增益

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为

$$P(X=x_1)=p_i, i=1,2,...,n$$

则随机变量X的熵定义为

$$H(X)=-\sum_{i=1}^N p_i log p_i$$

若$$p_i=0$$，则定义$$0log0=0$$。通常