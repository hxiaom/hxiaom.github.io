---
layout: post
title: 【Paper】The social dilemma of autonomous vehicles
categories: Mobility
---

Bonnefon J F, Shariff A, Rahwan I. The social dilemma of autonomous vehicles[J]. Science, 2016, 352(6293): 1573-1576.

## Motivation

AVs will sometimes have to choose between two evils, such as running over pedestrians or sacrificing themselves and their passenger to save the pedestrians. Defining the algorithms that will help AVs make these moral decisions is a formidable challenge.

Manufacturers and regulators will need to accomplish three potentially incompatible objectives: being consistent, not causing public outrage, and not discouraging buyers.

## Research Question

What is the moral algorithms that we are willing to accept as citizens and to be subjected to as car owners ?

## Model

Survey: six Amazon Mechanical Turk studies (n = 1928 total participants， U.S. residents only) between June and November 2015.

Studies described in the experimental ethics literature largely rely on MTurk respondents, with robust results, even though MTurk respondents are not necessarily representative of the U.S. population (13, 14).

In study one (n = 182 participants), 76% of participants thought that it would be more moral for AVs to sacrifice one passenger rather than kill 10 pedestrians [with a 95% confidence interval (CI) of 69 to 82]. 

In study two (n = 451 participants), participants were presented with dilemmas that varied the number of pedestrians’ lives that could be saved, from 1 to 100. Participants did not think that AVs should sacrifice their passenger when only one pedestrian could be saved (with an average approval rate of 23%), but their moral approval increased with the number of lives that could be saved (P < 0.001), up to approval rates consistent with the 76% observed in study one (Fig. 2B)

In study three presentsthe first hintofasocial dilemma. On a scale of 1 to 100, respondents were asked to indicate how likely they would be to buy an AVprogrammed tominimize casualties (which would, in these circumstances, sacrifice them and their co-rider family member), as well as how likely they would be to buy an AV programmed to prioritize protecting its passengers, even if it meant killing 10 or 20 pedestrians. Although the reported likelihood of buying an AV was low even for the self-protective option (median = 50), respondents indicated a significantly lower likelihood (P <0.001) of buying the AV when they imagined the situation in which they and their family member would be sacrificed for the greater good (median = 19). In other words, even though participants still agreed that utilitarian AVs were themostmoral, theypreferred the self-protective model for themselves.

In study four (n = 267 participants) offers another demonstration of this phenomenon. Participants were given 100 points to allocate between different types ofalgorithms, to indicate (i) howmoral the algorithms were, (ii) how comfortable par- ticipants were for other AVs to be programmed in a given manner, and (iii) how likely participants would be to buy an AV programmed in a given manner.

In study five(n = 376 participants), we asked participants about their attitudes toward legally enforcing utilitarian sacrifices. Participants con- sidered scenarios in which either a human driver or a control algorithm had anopportunity to self- sacrificetosave1or 10 pedestrians(Fig. 3C). As usual, the perceived morality of the sacrifice was high and about the same whether the sacrifice was performed by a human or by an algorithm (median = 70). When we inquired whether partic- ipants would agree to see such moral sacrifices legally enforced, their agreement was higher for algorithms than for human drivers (P<0.002), but the average agreement still remained below the midpoint of the 0 to 100 scale in each scenario. Agreement was highest in the scenario in which algorithms saved 10 lives,witha 95%CI of 33 to 46.

In study six (n = 393 participants), we asked participants specifically about their likeli- hood ofpurchasing theAVswhose algorithmshad been regulated by the government. people were reluctant to accept governmental regulation of utilitarian AVs. Even in the most favorable condition, when participants imagined only them- selves being sacrificed to save 10 pedestrians, the 95% CI for whether people thought it was appro- priate for the government to regulate this sacri- fice was only 36 to 48. Finally,

## Result

Participants in six Amazon Mechanical Turk studies approved of utilitarian AVs (that is, AVs that sacrifice their passengers for the greater good) and would like others to buy them, but they would themselves prefer to ride in AVs that protect their passengers at all costs.

The study participants disapprove of enforcing utilitarian regulations for AVs and would be less willing to buy such an AV.

## Contribution

Three groups may be able to decide how AVs handle ethical dilemmas: the consumers who buy the AVs; the manufacturers that program the AVs; and the government, which may regulate the kind ofprogrammingmanufacturers canoffer and consumers can select. 

Our findings suggest that regulation for AVs maybe necessarybut also counterproductive.Mor- al algorithms for AVs create a social dilemma (18, 19). Although people tend to agree that every- one would be better off ifAVs were utilitarian (in the sense ofminimizing the number ofcasualties on the road), these same people have a personal incentive to ride in AVs that will protect them at allcosts.Accordingly,if bothself-protective and utilitarian AVs were allowed on the market, few people would bewilling to ride in utilitarian AVs, even though they would prefer others to do so. Regulation may provide a solution to this problem, but regulators will be faced with two diffi- culties: First, most people seem to disapprove of a regulation that would enforce utilitarian AVs. Second—and amore serious problem—our results suggest that such regulation could substantially delay the adoption of AVs, which means that the lives saved by making AVs utilitarian may be outnumbered by the deaths caused by delaying the adoption ofAVs altogether. Thus, car-makers and regulators alike should be considering solu- tions to these obstacles.

Figuring out how to build ethical autonomous machines is one of the thorniest challenges in ar- tificial intelligence today (22).