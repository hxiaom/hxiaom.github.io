---
layout: post
title: 【Method】聚类算法
categories: Analytics
---

聚类是针对给定的样本，依据它们特征的相似度或距离，将其归并到若干个“类”或“簇”的数据分析问题。一个类是样本的一个子集。直观上，相似的样本聚集在相同的类，不相似的样本分散在不同的类。这里，样本之间的相似度或距离起着重要作用。

聚类的目的是通过得到的类或簇来发现数据的特点或对数据进行处理，在数据挖掘、模式识别等领域有着广泛的应用。聚类属于无监督学习，因为只是根据样本的相似度或距离将其进行归类，而类或簇事先并不知道。

聚类算法很多，本章介绍两种最常用的聚类算法：层次聚类（hierarchical clustering）和k均值聚类（k-means clustering）。层次聚类又有聚合（自下而上）和分裂（自上而下）两种方法。聚合法开始将每个样本各自分到一个类；之后将相距最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化的类别。分裂法开始将所有样本分到一个类；之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件；得到层次化的类别。k均值聚类是基于中心的聚类方法，通过迭代，将样本分到k个类中，使得每个样本与其所属类的中心或均值最近；得到k个“平坦的”、非层次化的类别，构成对空间的划分。k均值聚类的算法1967年由MacQueen提出。

## 聚类的基本概念

### 相似度或距离

聚类的对象是观测数据，或样本集合。假设有n个样本，每个样本由m个属性的特征向量组成。样本集合可以用矩阵X表示。

聚类的核心概念是相似度（similarity）或距离（distance），有多种相似度或距离的定义。因为相似度直接影响聚类的结果，所以其选择是聚类的根本问题。具体哪种相似度更合适取决于应用问题的特性。

1. 闵可夫斯基距离（Minkowski distance）

在聚类中，可以将样本集合看作是向量空间中点的集合，以该空间的距离表示样本之间的相似度。常用的距离有闵可夫斯基距离，特别是欧式距离。闵可夫斯基距离越大相似度越小，距离越小相似度越大。

给定样本集合X，X是m维实数向量空间$$R^m$$中点的集合，其中$$x_i,x_j \in X， x_i = (x_{1i}, x_{2i}, ..., x_{mi})^T, x_j=(x_{1j},x_{2j},...,x_{mj}^T$$，样本$$x_i$$与样本$$x_j$$的闵可夫斯基距离定义为

$$d_{ij} = (\sum_{k=1}^m \mid x_{ki} - x_{kj} \mid^p)^{\frac{1}{p}}$$

这里$$P \geq 1$$。当$$p=2$$时称为欧式距离（Euclidean distance），即

$$d_{ij} =  (\sum_{k=1}^m \mid x_{ki} - x_{kj} \mid^2)^{\frac{1}{2}}$$

当$$p=1$$时称为曼哈顿距离（Manhattan distance），即

$$d_{ij} = \sum_{k=1}^m \mid x_{ki} - x_{kj} \mid$$

当$$p=\infty$$时称为切比雪夫距离（Chebyshev distance），取各个坐标数值差的绝对值的最大值，即

$$d_{ij} = \max_k \mid x_{ki} - x_{kj} \mid$$

2. 马哈拉诺比斯距离

马哈拉诺比斯距离（Mahalanobis distance），简称马氏距离，也是另一种常用的相似度，考虑各个分量（特征）之间的相关性与各个分量的尺度无关

基于共现的相似度

基于网络的相似度

基于词向量的相似度

基于知识图谱的相似度