---
layout: post
title: 【Paper】Reasoning with neural tensor networks for knowledge base completion
categories: Mobility
---

Socher R, Chen D, Manning C D, et al. Reasoning with neural tensor networks for knowledge base completion[C]//Advances in neural information processing systems. 2013: 926-934.

## Motivation

Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships.

not all common knowledge that is obvious to people is expressed in text [5, 6, 2, 7]. We adopt here the complementary goal of predicting the likely truth of additional facts based on existing facts in the knowledge base. Such factual, common sense reasoning is available and useful to people. For instance, when told that a new species of monkeys has been discovered, a person does not need to find textual evidence to know that this new monkey, too, will have legs (a meronymic relationship inferred due to a hyponymic relation to monkeys in general).

## Proposed method

In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. 

Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the “Sumatran tiger” and “Bengal tiger.”

 Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.

## Contribution

The first contribution of this paper is the new neural tensor network (NTN), which generalizes several previous neural network models and provides a more powerful way to model relational information than a standard neural network layer.

The second contribution is to introduce a newway to represent entities in knowledge bases. Previous work [8, 9, 10] represents each entity with one vector. However, does not allow the sharing of statistical strength if entity names share similar substrings. Instead, we represent each entity as the average of its word vectors, allowing the sharing of statistical strength between the words describing each entity e.g., Bank of China and China.

The third contribution is the incorporation of word vectors which are trained on large unlabeled text. This readily available resource enables all models to more accurately predict relationships.
