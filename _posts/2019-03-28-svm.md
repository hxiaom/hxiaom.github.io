---
layout: post
title: 【Daily Method】SVM (一) 线性可分支持向量机
categories: Analytics
---

## 目的

- 二分类模型

## 原理

- 像什么就是什么

- 支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；

- 支持向量机还包括核技巧，这使它成为实质上的非线性分类器。

- 支持向量机的学习策略就是间隔最大化，可以形式化为一个求解凸二次规划（convex quadratic programming）的问题，支持向量机的学习算法是求解凸二次规划的最优化算法。

- 支持向量机的分类
    - 当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；
    - 当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；
    - 当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。

- 考虑一个二分类问题，假设输入空间与特征空间为两个不同的空间。输入空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。所以，输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。

- 一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。

- 给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为

    $$ w^{*} * x + b^{*} =0 $$

    以及相应的分类决策函数

    $$ f(x) = sign(w^{*} * x + b^{*}) $$

    称为线性可分支持向量机。

- 一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$$w*x+b=0$$确定的情况下，$$ \mid w*x+b \mid$$能够相对地表示点x距离超平面的远近。而$$w*x+b$$的符号与类标记y的符号是否一致能够表示分类是否哦正确。所以可用量$$y(w*x+b)$$来表示分类的正确性及确信度，这就是函数间隔（functional margin）的概念。

- 选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量w加某些约束，如规范化，$$ \|w\|=1 $$，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。此时，

    $$ r_i = y_i(\frac{w}{\|w\|}*x_i+\frac{b}{\|w\|}) $$

- 定义超平面(w,b)关于训练集数据T的几何间隔为超平面(w,b)关于T中所有样本点$$(x_i, y_i)$$的几何间隔之最小值，即

    $$ r = \min_{i=1,...,N}r_i $$

- 间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类，也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。

- 最大间隔分离超平面，可以表示为下面的约束最优化问题：

    $$ \max_{w,b} r $$

    $$ s.t. y_i(\frac{w}{\|w\|}*x_i+\frac{b}{\|w\|}) >= r, i=1,2,...,N $$

    考虑几何间隔和函数间隔的关系，可将这个问题改写为

    $$ \max_{w,b} \hat{r} $$

    $$ s.t. y_i(w*x_i+b) >= \hat{r}, i=1,2,...,N $$ 

    函数间隔$$\hat{r}$$的取值并不影响最优化问题的解。事实上，假设将w和b按比例改变为λw和λb,这是函数间隔成为λ$$\hat{r}$$。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，就可以取$$\hat{r}=1$$，将$$\hat{r}=1$$代入上面的最优化问题，得到

    $$ \min_{w,b} \frac{1}{2}\|w||^2 $$

    $$ s.t. y_i(w*x_i+b)-1 >= 0, i=1,2,...,N $$ 

    这是一个凸二次规划问题（convex quadratic programming）问题。

- 定理：最大间隔分离超平面的存在唯一性。若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。

- 支持向量与间隔边界

    在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector），支持向量是使约束条件式等号成立的点，即

    $$ y_i(w*x_i+b)-1 = 0 $$

    对



## 实现



## 相关算法

最近邻算法

感知机


## 参考文献

1. 终极算法：机器学习和人工智能如何重塑世界 佩德罗·多明戈斯 第七章 类推学派：像什么就是什么
2. 统计学习方法 李航 第7章 支持向量机
3. Pattern Recognition And Machine Learning Bishop Section 7 Sparse Kernel Machines