---
layout: post
title: 【Method】深度学习正则化（七）Dropout
categories: Analytics
---

## 原理

Dropout(Srivastava et al., 2014)提供了正则化一大类模型的方法，计算方便但功能强大。在第一种近似下，Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。Bagging涉及训练多个模型，并在每个测试样本上评估多个模型。当每个模型都是一个很大的神经网络时，这似乎是不切实际的，因为训练和评估这样的网络需要花费很大运行时间和内存。通常我们只能集成5-10个神经网络，超过这个数量就会迅速变得难以处理。Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。

具体而言，Dropout训练的集成包括所有从基础网络除去非输出单元后形成的子网络。最先进的神经网络基于一系列仿射变换和非线性变换，我们只需将一些单元的输出乘零就能有效地删除一个单元。这个过程需要对模型（如径向基函数网络，单元的状态和参考值之间存在一定区别）进行一些修改。为了简单起见，我们在这里提出乘零的简单Dropout算法。但是它被简单修改后，可以与从网络中移除单元的其他操作结合使用。

回想一下Bagging学习，我们定义k个不同的模型，从训练集有放回采样构造k个不同的数据集，然后在训练集i上训练模型i。Dropout的目标是在指数级数量的神经网络上近似这个过程。具体来说，在训练中使用Dropout时，我们会使用基于小批量产生较小步长的学习算法，如随机梯度下降等。我们每次在小批量中加载一个样本，然后随机抽样应用于网络中所有输入和隐藏单元的不同二值掩码。对于每个单元，掩码是独立采用的。掩码值为1的采用概率（导致包含一个单元）是训练开始前一个固定的超参数。它不是模型当前参数值或输入样本的函数。通过在每一个小批量训练的神经网络中，一个输入单元被包括的概率为0.8，一个隐藏单元被包括的概率为0.5。然后，我们运行和之前一样的前向传播、反向传播以及学习更新。

更正式地说，假设一个掩码向量$$\mu$$指定被包括的单元，$$J(\theta, \mu)$$是由参赛$$\theta$$和掩码$$\mu$$定义的模型代价。那么Dropout训练的目标是最小化$$E_{\mu} J(\theta, \mu)$$。这个期望包含多达指数级的项，但我们可以通过抽样$$\mu$$获得梯度的无偏估计。

Dropout训练和Bagging训练不太一样。在Bagging的情况下，所有模型都是独立的。在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。在Dropout的情况下，通常大部分模型都没有显示地被训练，因为通常父神经网络会很大，以至于到宇宙毁灭都不可能采样完所有的子网络。取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。这些是仅有的区别。除了这些，Dropout与Bagging算法一样。例如，每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集。

Bagging集成必须根据所有成员的累积投票做一个预测。在这种背景下，我们将这个过程称为推断（inference）。目前为止，我们在介绍Bagging和Dropout时没有要求模型具有明确的概率。现在，我们假定该模型的作用是输出一个概率分布。在Bagging的情况下，每个模型i产生一个概率分布$$p^{(i)}(y \mid x)$$。集成的预测由这些分布的算术平均值给出：

$$\frac{1}{k} \sum_{i=1}^k p^{(i)} (y \mid x)$$

在Dropout的情况下，通过掩码$$\mu$$定义每个子模型的概率分布$$p(y \mid x, \mu)$$。所有掩码的算术平均值由下式给出：

$$\sum_\mu p(\mu)p(y \mid x,\mu)$$

其中$$p(\mu)$$是训练时采用$$\mu$$的概率分布。

因为这个求和包含多达指数级的项，除非该模型的结构允许某种形式的简化，否则是不可能计算的。目前为止，无法得知深度神经网络是否允许某种可行的简化。相反，我们可以通过采样近似推断，即平均许多掩码的输出。即使是10~20个掩码就足以获得不错的表现。

然而，一个更好的方法能不错地近似整个集成的预测，且只需一个前向传播的代价。要做到这一点，我们改用集成成员预测分布的几何平均而不是算术平均。Warde-Farley et al. (2014) 提出的论点和经验证据表明，在这个情况下几何平均与算术平均表现得差不多。

多个概率分布的几何平均不能保证是一个概率分布。为了保证结果是一个概率分布，我们要求没有子模型给某一事件分配概率0，并重新标准化所得分布。通过几何平均直接定义的非标准化概率分布由下式给出：

$$\tilde{p}_{ensemble} (y \mid x) = \sqrt[2^d]{\prod_{\mu} p(y \mid x,\mu)}$$

其中d是可被丢弃的单元数。这里为简化介绍，我们使用均匀分布的$$\mu$$，但非均匀分布也是可以的。为了作出预测，我们必须重新标准化集成

$$p_{ensemble} (y \mid x)  = \frac{\tilde{p}_{ensemble} (y \mid x)}{\sum_{y'} \tilde{p}_{ensemble} (y' \mid x)}$$

涉及Dropout的一个重要观点(Hinton et al., 2012c)是，我们可以通过评估模型中$$p(y \mid x)$$来近似$$p_{ensemble}$$：该模型具有所有单元，但我们将单元i的输出的权重乘以单元i的被包含概率。这个修改的动机是得到从该单元输出的正确期望值。我们把这种方法称为权重比例推断规则（weight scaling inference rule）。目前还没有在深度非线性网络上对这种近似推断规则的准确性作任何理论分析，但经验上表现得很好。

因为我们通常使用$$\frac{1}{2}$$的包含概率，权重比例规则一般相当于在训练结束后将权重除以2，然后像平常一样使用模型。