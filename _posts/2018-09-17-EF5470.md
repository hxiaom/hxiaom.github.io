---
layout: post
title: EF5470 Econometrics (Fred Y. K. Kwan)
categories: DataScience
---

Textbook: Stock, J. and Watson, M. (2012) Introduction to Econometrics. 3rd edition. Pearson.

## Part I Introduction to Econometrics

### Chapter 1 Economic Questions and Data

At a broad level, econometrics is the science and art of using **economic theory** and **statistical techniques** to analyze **economic data**.

#### 1.1 Economic Questions We Examine

Four questions as example

- Question #1: Does Reducing Class Size Improve Elementray School Education?
- Question #2: Is There Racial Discrimination in the Market for Home Loans?
- Question #3: How Much Do Cigarette Taxes Reduce Smoking?
- Question #4: By How Much Will U.S. GDP Grow Next Year?

Each of these questions requires a **numerical answer**. **Economic theory** provides clues about that answer but the actual value of the number must be learned **empirically**, that is, by analyzing data. Because we use data to answer quantitative questions, our answers always have some **uncertainty**: A different set of data would produce a different numerical answer. Therefore, the conceptual framework for the analysis needs to provide **both** a **numerical answer** to the question and a **measure of how precise** the answer is.

The conceptual framework used in this book is the **multiple regression model**, the mainstay of econometrics.

#### 1.2 Causal Effects and Idealized Experiments

Questions in econometrics concern **causal relationships** among variables. **Causality** means that a specific action leads to a specific, measureable consequence.

**Estimation of Causal Effects**

One way to measure causal effect is to conduct an **experiment**: **randomized controlled experment**.

**Forecasting and Causality**

Another kind of questions econometrics concerned about are **forecasting**. Even though forecasting need not involve causal realtionships, economic theory suggests **patterns and relationships** that might be useful for forecasting.

#### 1.3 Data: Sources and Types

In econometrics, data come from one of two sources: **experiments** or **nonexperimental observations** of the world.

Three main types of data: **cross-sectional data, time series data, and panel data**.

**Experimental Versus Observational Data**

Expriments: **expensive, difficult to administer, sometimes unethical**.

Observational data: **difficult to sort out the effect of the "treatment" from other relecant factors**.

**Cross-Sectional Data**

Data on different entities for a single time period are called cross-sectional data.

**Time Series Data**

Time series data are for a single entity collected at multiple time periods.

**Panel Data**

Panel data, also called longitudinal data, are data for multiple entities in which each entity is observed at two or more time periods.

### Chapter 2 Review of Probability

Most aspects of the world around us have an element of **randomness**. The theory of probability provides mathematical tools for **quantifying and describing this randomness**.

#### 2.1 Random Variables and Probability Distributions

The **mutually exclusive** potential results of a random process are called the **outcomes**.

The **probability** of an outcome is the proportion of the time that the outcome occurs in the long run.

The set of all possible outcomes is called the **sample space**.

An **event** is a subset of the sample space, that is, an event is a set of one or more outcomes.

A **random variable** is a **numerical summary** of a random outcome.

**Probability Distribution of a Discrete Random Variable**

The **probability distribution** of a discrete random variables is the list of all possible values of the variable and the probability that each value will occur.

cumulative probability distribution is also referred to as a cumulative distribution function (c.d.f.).

the outcomes are 0 or 1. A binary random variable is called a **Bernoulli random variable**, and its probability distribution is called the **The Bernoulli distribution**

**Probability Distribution of a Continuous Random Variable**

for coninuous variables, the probability is summarized by the **probability density function**. A probability density function is also called a p.d.f.

#### 2.2 Expected Values, Mean, and Variance

**The Expected Value of a Random Variable**

The **expected value** of a random variable Y, denoted E(Y), is the **long-run average value** of the random variable over **many repeated trials or occurrences**.

The expected value of Y is also called the **expectation** of Y or the **mean** of Y.

**The Standard Deviation and Variance**

The variance and standard deviation measure the **dispersion** or the **"spread"** of a probability distribution.

var(Y) = E[(Y-u_Y)^2]

**Mean and Variance of a Linear Function of a Random Variable**

**Other Measures of the Shape of a Distribution**

the **skewness** measures the lack of symmetry of a distribution.

the **kurtosis** measures how thick, or "heavy" are its tails. An extreme value of Y is called an **outlier**. The greater the kurtosis of a distribution, the more likely are outliers. A distribution with kurtosis **exceeding 3** is called **leptokurtic** or, more simply, **heavy-tailed**.

The **mean, variance, skewness, and kurtosis** are all based on what are called the **moments of a distribution**.

**Moments**. The mean of Y, E(Y), is also called the first moment of Y, and the expected value of the square of Y, E(Y^2), is called the second moment of Y. In general, the expected value of Y^T is called the T th moment of the random variable Y.

#### 2.3 Two Random Variables

The **joint probability distribution** of two discrete random variables, say X and Y, is the probability that the random variables **simultaneously** take on certain values, say x and y.

The **marginal probability distribution** of a random variable Y is **just another name** for its probability distribution.

The distribution of a random variable Y conditional on another random variable X taking on a specific value is called the **conditional distribution of Y given X**.

The **conditional expectation of Y given X**, also called the **conditonal mean of Y given X**.

**The law of iterated expectations**: E(Y)=E[E(Y|X)]

**Conditional variance**. The variance of Y conditional on X is the variance of the conditional distribution of Y given X.

Two random variables X and Y are **independently distributed**, or **independent**, if knowing the value of one of the variables provides no information about the other. P(Y=y|X=x) = P(Y=y)

On measure of the **extent to which two random variables move together** is their **covariance**. The covariance between X and Y is the expected value cov(X, Y) = E[(X-u_X)(Y-u_Y)].

The **correlation** is an alternative measure of dependence between X and Y that solves the **"units" problem** of the covariance. corr(X, Y) = cov(X, Y) / (var(X)var(Y))^(1/2)

#### 2.4 The Normal, Chi-Squared, Student t, and F Distributions

