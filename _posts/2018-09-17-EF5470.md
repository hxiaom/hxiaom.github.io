---
layout: post
title: EF5470 Econometrics (Fred Y. K. Kwan)
categories: DataScience
---

Textbook: Stock, J. and Watson, M. (2012) Introduction to Econometrics. 3rd edition. Pearson.

## Part I Introduction to Econometrics

### Chapter 1 Economic Questions and Data

At a broad level, econometrics is the science and art of using **economic theory** and **statistical techniques** to analyze **economic data**.

#### 1.1 Economic Questions We Examine

Four questions as example

- Question #1: Does Reducing Class Size Improve Elementray School Education?
- Question #2: Is There Racial Discrimination in the Market for Home Loans?
- Question #3: How Much Do Cigarette Taxes Reduce Smoking?
- Question #4: By How Much Will U.S. GDP Grow Next Year?

Each of these questions requires a **numerical answer**. **Economic theory** provides clues about that answer but the actual value of the number must be learned **empirically**, that is, by analyzing data. Because we use data to answer quantitative questions, our answers always have some **uncertainty**: A different set of data would produce a different numerical answer. Therefore, the conceptual framework for the analysis needs to provide **both** a **numerical answer** to the question and a **measure of how precise** the answer is.

The conceptual framework used in this book is the **multiple regression model**, the mainstay of econometrics.

#### 1.2 Causal Effects and Idealized Experiments

Questions in econometrics concern **causal relationships** among variables. **Causality** means that a specific action leads to a specific, measureable consequence.

**Estimation of Causal Effects**

One way to measure causal effect is to conduct an **experiment**: **randomized controlled experment**.

**Forecasting and Causality**

Another kind of questions econometrics concerned about are **forecasting**. Even though forecasting need not involve causal realtionships, economic theory suggests **patterns and relationships** that might be useful for forecasting.

#### 1.3 Data: Sources and Types

In econometrics, data come from one of two sources: **experiments** or **nonexperimental observations** of the world.

Three main types of data: **cross-sectional data, time series data, and panel data**.

**Experimental Versus Observational Data**

Expriments: **expensive, difficult to administer, sometimes unethical**.

Observational data: **difficult to sort out the effect of the "treatment" from other relecant factors**.

**Cross-Sectional Data**

Data on different entities for a single time period are called cross-sectional data.

**Time Series Data**

Time series data are for a single entity collected at multiple time periods.

**Panel Data**

Panel data, also called longitudinal data, are data for multiple entities in which each entity is observed at two or more time periods.

### Chapter 2 Review of Probability

Most aspects of the world around us have an element of **randomness**. The theory of probability provides mathematical tools for **quantifying and describing this randomness**.

#### 2.1 Random Variables and Probability Distributions

The **mutually exclusive** potential results of a random process are called the **outcomes**.

The **probability** of an outcome is the proportion of the time that the outcome occurs in the long run.

The set of all possible outcomes is called the **sample space**.

An **event** is a subset of the sample space, that is, an event is a set of one or more outcomes.

A **random variable** is a **numerical summary** of a random outcome.

**Probability Distribution of a Discrete Random Variable**

The **probability distribution** of a discrete random variables is the list of all possible values of the variable and the probability that each value will occur.

cumulative probability distribution is also referred to as a cumulative distribution function (c.d.f.).

the outcomes are 0 or 1. A binary random variable is called a **Bernoulli random variable**, and its probability distribution is called the **The Bernoulli distribution**

**Probability Distribution of a Continuous Random Variable**

for coninuous variables, the probability is summarized by the **probability density function**. A probability density function is also called a p.d.f.

#### 2.2 Expected Values, Mean, and Variance

**The Expected Value of a Random Variable**

The **expected value** of a random variable Y, denoted E(Y), is the **long-run average value** of the random variable over **many repeated trials or occurrences**.

The expected value of Y is also called the **expectation** of Y or the **mean** of Y.

**The Standard Deviation and Variance**

The variance and standard deviation measure the **dispersion** or the **"spread"** of a probability distribution.

var(Y) = E[(Y-u_Y)^2]

**Mean and Variance of a Linear Function of a Random Variable**

**Other Measures of the Shape of a Distribution**

the **skewness** measures the lack of symmetry of a distribution.

the **kurtosis** measures how thick, or "heavy" are its tails. An extreme value of Y is called an **outlier**. The greater the kurtosis of a distribution, the more likely are outliers. A distribution with kurtosis **exceeding 3** is called **leptokurtic** or, more simply, **heavy-tailed**.

The **mean, variance, skewness, and kurtosis** are all based on what are called the **moments of a distribution**.

**Moments**. The mean of Y, E(Y), is also called the first moment of Y, and the expected value of the square of Y, E(Y^2), is called the second moment of Y. In general, the expected value of Y^T is called the T th moment of the random variable Y.

#### 2.3 Two Random Variables

The **joint probability distribution** of two discrete random variables, say X and Y, is the probability that the random variables **simultaneously** take on certain values, say x and y.

The **marginal probability distribution** of a random variable Y is **just another name** for its probability distribution.

The distribution of a random variable Y conditional on another random variable X taking on a specific value is called the **conditional distribution of Y given X**.

The **conditional expectation of Y given X**, also called the **conditonal mean of Y given X**.

**The law of iterated expectations**: E(Y)=E[E(Y\|X)]

**Conditional variance**. The variance of Y conditional on X is the variance of the conditional distribution of Y given X.

Two random variables X and Y are **independently distributed**, or **independent**, if knowing the value of one of the variables provides no information about the other. P(Y=y\|X=x) = P(Y=y)

On measure of the **extent to which two random variables move together** is their **covariance**. The covariance between X and Y is the expected value cov(X, Y) = E[(X-u_X)(Y-u_Y)].

The **correlation** is an alternative measure of dependence between X and Y that solves the **"units" problem** of the covariance. corr(X, Y) = cov(X, Y) / (var(X)var(Y))^(1/2)

#### 2.4 The Normal, Chi-Squared, Student t, and F Distributions

**The Normal Distribution**

the normal density with mean μ and variance δ^2 is symmetric around its mean and has **95%** of its probability between μ-1.96σ and μ+1.96σ.

**The standard normal distribution** is the normal distribution with mean μ=0 and variance σ^2=1 and is denoted N(0,1). Random variables that have a N(0,1) distribution are often deoted **Z**, and the **standard normal cumulative distribution function** is denoted by the **Greek letter φ**; accordingly, P(Z <= c) = φ(c), where c is a constant.

To look up probabilities for a normal variable with a general mean and variance, we must **standardize the variable** by first subtracting the mean, then by dividing the result by the standard deviation, by computing **Z = (Y - μ)/σ**.

The normal distribution is symmetric, so its **skewness is zero**, The **kurtosis** of the normal distribution is **3**.

**The multivariate normal distribution**

The normal distribution can be generalized to describe the joint distribution of a set of random variables. In this cae, the distribution is called the** multivariate normal distribution**, or, if only two variables are being considered, the **bivariate normal distribution**.

The multivariate normal distribution has **four important properties.**

1. If X and Y have a bivariate normal distribution with covariance σ_XY and if a and b are two constants, then aX + bY has the normal distribution: aX + bY ~ N(aμ_X+bμ_Y, a^2σ_X^2+b^2σ_Y^2+2abσ_XY). More generally, if n random variables have a multivariate normal distribution, then any linear combination of these variables (such as their sum) is normally distributed.

2. If a set of variables has a multivariate normal distribution, then the marginal distribution of each of the variables is normal.

3. If variables with a multivariate normal distribution have covariances that equal zero. then the variables are **independent**. Thus, if X and Y have a bivariate normal distribution and σ_XY=0, then X and Y are independent. Zero covariance implies independence is a **special property** of the multivariate normal distribution that is not true in general.

4. If X and Y have a bivariate normal distribution, then the conditional expectation of Y given X is linear in X; that is, E(Y\|X=x)= a + bx, where a and b are constants. Joint normality implies **linearity of conditional expectations**, but linearity of conditional expectations does not imply joint normality.

**The Chi-Squared Distribution**

The chi-squared distribution is used when testing certain types of hypotheses in statistics and econometrics.

The chi-squared distribution is the distribution of **the sum of m squared independent standard normal random variables**.

This distribution depends on m, which is called **the degrees of freedom of the chi-squared distribution**. For example, let Z1, Z2, and Z3 be independent standard normal variables. Then Z1^2 + Z2^2 + Z3^2 has a chi-squared distribution with 3 degrees of freedom.

A chi-squared distribution with m degrees of freedom is denoted X_m^2

**The Student t Distribution**

The Student t distribution with m degrees of freedom is defined to be the distribution of **the ratio of a standard normal random variable, divided by the square root of an independently distributed chi-squared random variable with m degrees of freedom divided by m.** That is, let Z be a standard normal random variable, let W be a random variable with a chi-squared distribution with m degrees of freedom, and let Z and W be **independently distributed**. Then the random variable **Z/(W/m)^1/2** has a Student t distribution (also called the t distribution) with m degrees of freedom. This distribution is denoted **t_m**.

The Student t distribution depends on **the degrees of freedom m**. Thus the 95th percentile of the t_m distribution depends on the degrees of freedom m. The Student t distribution has a **bell shape** similar to that of the normal distribution, but **when m is small (20 or less)**, it has more mass in the tails - that is, it is a "fatter" bell shape than the normal. **When m is 30 or more**, the Student t distribution is well approximated by the standard normal distribution and the t_∞ distribution equals the standard normal distribution.

**The F Distribution**

The F distribution with m and n degrees of freedom, denoted **F_m,n**, is defined to be the distribution of **the ratio of a chi-squared random variable with degrees of freedom m, to an independently distributed chi-squared random variable with degrees of freedom n, divided by n.**

To state this mathematically, let W be a chi-suqared random variable with m degrees of freedom and let V be a chi-squared random variable with n degrees of freedom , where W and V are **independently distributed**. Then **(W/m)/(V/n)** has an F_m,n distribution.

In statistics and econometics, an important **special case** of the F distribution arises when the denominator degrees of freedom is large enough that the F_m,n distribution can be approximated by the F_m,∞ distribution. In this limiting case, the denominator random variable V/n is **the mean of infinitely many squared standard normal random variable**s, and that mean is 1 because the mean of a squared standard normal random varables is 1. Thus the F_m,∞ distribution is the distribution of a **chi-squared random variable with m degrees of freedom, divided by m;** W/m is distributed F_m,∞.

#### 2.5 Random Sampling and the Distribution of the Sample Average

**Characterizing the distributions of sample averages** therefore is an essential step toward understanding the performance of econometric procedures.

The act of random sampling - that is, randomly drawing a sample from a larger population - has the effect of making the sample average itself a **random variable**. 

Because the sample average is a random variable, it has a probability distribution, which is called its **sampling distribution**.

**Random Sampling**

**Simple random sampling**: n objects are selected at random from a population and each member of the population is equally likely to be included in the sample.

The n observations in the sample are denoted Y1, ..., Yn.

The act of random sampling means that Y1, ..., Yn can be treated as **random variables**. Before they are sampled, Y1, ..., Yn can take on many possible values; after they are sampled, a specific value is recorded for each observation.

When Y1, ..., Yn are drawn from the same distribution and are independently distributed, they are said to be **independently and identically distributed** (or **i.i.d.**).

**The Sampling Distribution of the Sample Average**

The sample average or sample mean Y_bar of the n observations Y1, ..., Yn is Y-bar=1/n(Y1+Y2+...+Yn)

An essential concept is that **the act of drawing a random sample has the effect of making the sample average Y-bar a random variable**. Because the sample was drawn at random, the value of each Yi is random. Because Y1, ..., Yn are random, their average is random. Had a different sample been drawn, then the observations and their sample average would have been different; The value of Y-bar differs from one randomly drawn sample to the next.

Because Y-bar is random, it has a **probability distribution**. The distribution of Y-bar is called the **sampling distribution** of Y-bar because it is the probability distribution associated with possible values of Y-bar that could be computed for different possible samples Y1, ..., Yn.

The sampling distribution of averages and weighted averages plays a **central role** in statistics and econometrics.

**Mean and variance of Y-bar**: Suppose that the observations Y1, ..., Yn are i.i.d., and let μY and σY^2 denote the mean and variance of Yi. E(Y-bar)=μY. var(Y-bar) = (σY^2)/n.




