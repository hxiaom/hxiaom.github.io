---
layout: post
title: TF-IDF
categories: Analytics
---

## 目的

词语相对于文章的重要性指标。

常用于：
- 文章中关键词获取；
- 文章中词语重要性加权


## 原理

前提：
1. 有个语料库，语料库里有多个文件；
2. 每个文件包含多个词语。

原理：
1. 字词的重要性随着它在文件中出现的次数成正比增加；（TF计算）
2. 但同时会随着它在语料库中出现的频率成反比下降。（IDF计算）
3. TF-IDF = TF * IDF （同时考虑TF与IDF）

## 计算

### Term Frequency(TF)
TF有很多不同的算法，从简单到复杂。但目的都是反映该词语在文章中的出现频次。

| Weighting Scheme | TF Weight |
| ---------------- | --------- |
| binary           | 0,1       |
| raw count        | $$f_{t,d}$$ | 
| term frequency   | $$\frac {f_{t,d}}{\sum_{t' \in d} f_{t',d}}$$  |
| log normalization | $$ log(1+ f_{t,d}) $$ |
| double normalization 0.5 | $$ 0.5 + 0.5 * \frac {f_{t,d}}{\max_{t' \in d} f_{t',d}}$$ |
| double normalization K | $$ K + (1-K)\frac {f_{t,d}}{\max_{t' \in d} f_{t',d}} $$ |


### Inverse document frequency(IDF)
IDF也有很多不同的算法，但目的都是反映该词语在语料库中出行的频次的**倒数**。这个指数衡量的是这个单词包含了多少信息。

| Weighting Scheme | IDF Weight $$ (n_t = \abs{d \in D : t \in d}) $$ |
| ---------------- | ---------------- |
| unary            | 1                |
| inverse document frequency | $$ log\frac {N}{n_t} = -log \frac{n_t}{N} $$ |
| inverse document frequency smooth | $$ log(\frac {N}{1+n_t}) $$ |
| inverse document frequency max    | $$ log(\frac {\max_{t' \in d} n_{t'}}{1+n_t}) $$ |
| probabilistic inverse document frequency | $$ log \frac {N-n_t}{n_t} $$ |

### TF-IDF

Recommended tf–idf weighting schemes

| weighting sheme | document term weight | query term weight |
| --------------- | -------------------- | ----------------- |
| 1               | $$ f_{t,d}*log \frac {N}{n_t} $$ |   |


## 现成的包



## 参考文献
1. [wikipedia: tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)

